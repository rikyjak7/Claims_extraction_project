<!DOCTYPE html>
<html lang="en">
<head>
<meta content="text/html; charset=utf-8" http-equiv="content-type"/>
<title>One missing piece in Vision and Language: A Survey on Comics Understanding</title>
<!--Generated on Sat Sep 14 18:13:36 2024 by LaTeXML (version 0.8.8) http://dlmf.nist.gov/LaTeXML/.-->
<meta content="width=device-width, initial-scale=1, shrink-to-fit=no" name="viewport"/>
<link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/css/bootstrap.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/ar5iv-fonts.0.7.9.min.css" rel="stylesheet" type="text/css"/>
<link href="/static/browse/0.3.4/css/latexml_styles.css" rel="stylesheet" type="text/css"/>
<script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.0/dist/js/bootstrap.bundle.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/html2canvas/1.3.3/html2canvas.min.js"></script>
<script src="/static/browse/0.3.4/js/addons_new.js"></script>
<script src="/static/browse/0.3.4/js/feedbackOverlay.js"></script>
<meta content="
Survey,  Comics,  Manga,  Layers of Understanding,  Comics Understanding,  Vision-Language tasks
" lang="en" name="keywords"/>
<base href="/html/2409.09502v1/"/></head>
<body>
<nav class="ltx_page_navbar">
<nav class="ltx_TOC">
<ol class="ltx_toclist">
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S1" title="In One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">1 </span><span class="ltx_text ltx_font_smallcaps">Introduction</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S2" title="In One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2 </span><span class="ltx_text ltx_font_smallcaps">Background</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S2.SS1" title="In 2 Background ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1 </span><span class="ltx_text ltx_font_italic">The Comics Research Epochs</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S2.SS1.SSS1" title="In 2.1 The Comics Research Epochs ‣ 2 Background ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.1 </span>Traditional Machine Learning</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S2.SS1.SSS2" title="In 2.1 The Comics Research Epochs ‣ 2 Background ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.2 </span>Deep Learning approaches</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S2.SS1.SSS3" title="In 2.1 The Comics Research Epochs ‣ 2 Background ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.1.3 </span>Modern Foundational models</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S2.SS2" title="In 2 Background ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">2.2 </span><span class="ltx_text ltx_font_italic">Relevant Surveys</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S3" title="In One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">3 </span><span class="ltx_text ltx_font_smallcaps">Comics Foundations</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4" title="In One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4 </span><span class="ltx_text ltx_font_smallcaps">Tasks and Datasets</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4.SS1" title="In 4 Tasks and Datasets ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.1 </span><span class="ltx_text ltx_font_italic">Annotations overview</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4.SS2" title="In 4 Tasks and Datasets ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.2 </span><span class="ltx_text ltx_font_italic">Datasets overview</span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4.SS3" title="In 4 Tasks and Datasets ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">4.3 </span><span class="ltx_text ltx_font_italic">Datasets for Evaluation</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S5" title="In One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">5 </span><span class="ltx_text ltx_font_smallcaps">Taxonomy: <span class="ltx_text ltx_font_italic">LoCU</span></span></span></a></li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6" title="In One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6 </span><span class="ltx_text ltx_font_smallcaps">Layer 1: Tagging and Augmentation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS1" title="In 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1 </span><span class="ltx_text ltx_font_italic">Tagging</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS1.SSS1" title="In 6.1 Tagging ‣ 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.1 </span>Image Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS1.SSS2" title="In 6.1 Tagging ‣ 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.2 </span>Emotion Classification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS1.SSS3" title="In 6.1 Tagging ‣ 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.3 </span>Action Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS1.SSS4" title="In 6.1 Tagging ‣ 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.1.4 </span>Page Stream Segmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS2" title="In 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2 </span><span class="ltx_text ltx_font_italic">Augmentation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS2.SSS1" title="In 6.2 Augmentation ‣ 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.1 </span>Super-Resolution</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS2.SSS2" title="In 6.2 Augmentation ‣ 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.2 </span>Style-Transfer</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS2.SSS3" title="In 6.2 Augmentation ‣ 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.3 </span>Vectorization</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS2.SSS4" title="In 6.2 Augmentation ‣ 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.2.4 </span>Depth Estimation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS3" title="In 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">6.3 </span><span class="ltx_text ltx_font_italic">Satellite Tasks</span></span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7" title="In One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7 </span><span class="ltx_text ltx_font_smallcaps">Layer 2: Grounding, Analysis, and Segmentation</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS1" title="In 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1 </span><span class="ltx_text ltx_font_italic">Grounding</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS1.SSS1" title="In 7.1 Grounding ‣ 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.1 </span>Detection</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS1.SSS2" title="In 7.1 Grounding ‣ 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.2 </span>Character Re-Identification</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS1.SSS3" title="In 7.1 Grounding ‣ 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.1.3 </span>Sentence-based Grounding</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS2" title="In 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2 </span><span class="ltx_text ltx_font_italic">Analysis</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS2.SSS1" title="In 7.2 Analysis ‣ 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.1 </span>Text-character association</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS2.SSS2" title="In 7.2 Analysis ‣ 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.2 </span>Panel Sorting</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS2.SSS3" title="In 7.2 Analysis ‣ 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.2.3 </span>Dialog transcription</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS3" title="In 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3 </span><span class="ltx_text ltx_font_italic">Segmentation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS3.SSS1" title="In 7.3 Segmentation ‣ 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.3.1 </span>Instance segmentation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS4" title="In 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4 </span><span class="ltx_text ltx_font_italic">Future Tasks</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7.SS4.SSS1" title="In 7.4 Future Tasks ‣ 7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">7.4.1 </span>Translation</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S8" title="In One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8 </span><span class="ltx_text ltx_font_smallcaps">Layer 3: Retrieval and Modification</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S8.SS1" title="In 8 Layer 3: Retrieval and Modification ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1 </span><span class="ltx_text ltx_font_italic">Retrieval</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S8.SS1.SSS1" title="In 8.1 Retrieval ‣ 8 Layer 3: Retrieval and Modification ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1.1 </span>Unimodal Retrieval</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S8.SS1.SSS2" title="In 8.1 Retrieval ‣ 8 Layer 3: Retrieval and Modification ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.1.2 </span>Cross-modal retrieval</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S8.SS2" title="In 8 Layer 3: Retrieval and Modification ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2 </span><span class="ltx_text ltx_font_italic">Modification</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S8.SS2.SSS1" title="In 8.2 Modification ‣ 8 Layer 3: Retrieval and Modification ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.2.1 </span>Image Inpainting and Editing</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S8.SS3" title="In 8 Layer 3: Retrieval and Modification ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3 </span><span class="ltx_text ltx_font_italic">Future Tasks</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S8.SS3.SSS1" title="In 8.3 Future Tasks ‣ 8 Layer 3: Retrieval and Modification ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">8.3.1 </span>Personalized Image retrieval</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S9" title="In One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9 </span><span class="ltx_text ltx_font_smallcaps">Layer 4: Advanced Visual-Language Understanding</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S9.SS1" title="In 9 Layer 4: Advanced Visual-Language Understanding ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1 </span><span class="ltx_text ltx_font_italic">Understanding</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S9.SS1.SSS1" title="In 9.1 Understanding ‣ 9 Layer 4: Advanced Visual-Language Understanding ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.1.1 </span>Visual Question Answering</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S9.SS2" title="In 9 Layer 4: Advanced Visual-Language Understanding ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2 </span><span class="ltx_text ltx_font_italic">Future Tasks</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S9.SS2.SSS1" title="In 9.2 Future Tasks ‣ 9 Layer 4: Advanced Visual-Language Understanding ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2.1 </span>Visual Entailment</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S9.SS2.SSS2" title="In 9.2 Future Tasks ‣ 9 Layer 4: Advanced Visual-Language Understanding ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2.2 </span>Visual Dialog</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S9.SS2.SSS3" title="In 9.2 Future Tasks ‣ 9 Layer 4: Advanced Visual-Language Understanding ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">9.2.3 </span>Visual Reasoning</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10" title="In One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10 </span><span class="ltx_text ltx_font_smallcaps">Layer 5: Generation and Synthesis</span></span></a>
<ol class="ltx_toclist ltx_toclist_section">
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10.SS1" title="In 10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.1 </span><span class="ltx_text ltx_font_italic">Generation</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10.SS1.SSS1" title="In 10.1 Generation ‣ 10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.1.1 </span>Comics generation from other media.</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10.SS1.SSS2" title="In 10.1 Generation ‣ 10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.1.2 </span>Image-to-Text generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10.SS1.SSS3" title="In 10.1 Generation ‣ 10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.1.3 </span>Text-to-Image generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10.SS1.SSS4" title="In 10.1 Generation ‣ 10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.1.4 </span>Sound Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10.SS2" title="In 10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.2 </span><span class="ltx_text ltx_font_italic">Synthesis</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10.SS2.SSS1" title="In 10.2 Synthesis ‣ 10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.2.1 </span>3D generation from images</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10.SS2.SSS2" title="In 10.2 Synthesis ‣ 10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.2.2 </span>Video generation</span></a></li>
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10.SS2.SSS3" title="In 10.2 Synthesis ‣ 10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.2.3 </span>Narrative-based Comic Generation</span></a></li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_subsection">
<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10.SS3" title="In 10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.3 </span><span class="ltx_text ltx_font_italic">Future Tasks</span></span></a>
<ol class="ltx_toclist ltx_toclist_subsection">
<li class="ltx_tocentry ltx_tocentry_subsubsection"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10.SS3.SSS1" title="In 10.3 Future Tasks ‣ 10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">10.3.1 </span>Comics to Scene graph</span></a></li>
</ol>
</li>
</ol>
</li>
<li class="ltx_tocentry ltx_tocentry_section"><a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S11" title="In One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title"><span class="ltx_tag ltx_tag_ref">11 </span><span class="ltx_text ltx_font_smallcaps">CONCLUSION</span></span></a></li>
</ol></nav>
</nav>
<div class="ltx_page_main">
<div class="ltx_page_content">
<article class="ltx_document ltx_authors_1line">
<h1 class="ltx_title ltx_title_document">
One missing piece in Vision and Language: 
<br class="ltx_break"/>A Survey on Comics Understanding
</h1>
<div class="ltx_authors">
<span class="ltx_creator ltx_role_author">
<span class="ltx_personname">Emanuele Vivoli <span class="ltx_ERROR undefined" id="id7.7.id1">\orcidlink</span>0000-0002-9971-8738<sup class="ltx_sup" id="id8.8.id2"><span class="ltx_text ltx_font_italic" id="id8.8.id2.1">∗,†,‡</span></sup>,
Andrey Barsky <span class="ltx_ERROR undefined" id="id9.9.id3">\orcidlink</span>0000-0002-6993-5969<sup class="ltx_sup" id="id10.10.id4"><span class="ltx_text ltx_font_italic" id="id10.10.id4.1">†</span></sup>,
Mohamed Ali Souibgui <span class="ltx_ERROR undefined" id="id11.11.id5">\orcidlink</span>0000-0000-0000-0000<sup class="ltx_sup" id="id12.12.id6"><span class="ltx_text ltx_font_italic" id="id12.12.id6.1">†</span></sup>,
Artemis LLabrés <span class="ltx_ERROR undefined" id="id13.13.id7">\orcidlink</span>0000-0000-0000-0000<sup class="ltx_sup" id="id14.14.id8"><span class="ltx_text ltx_font_italic" id="id14.14.id8.1">†</span></sup>,
Marco Bertini <span class="ltx_ERROR undefined" id="id15.15.id9">\orcidlink</span>0000-0002-1364-218X<sup class="ltx_sup" id="id16.16.id10"><span class="ltx_text ltx_font_italic" id="id16.16.id10.1">‡</span></sup>
and Dimosthenis Karatzas <span class="ltx_ERROR undefined" id="id17.17.id11">\orcidlink</span>0000-0001-8762-4454<sup class="ltx_sup" id="id18.18.id12"><span class="ltx_text ltx_font_italic" id="id18.18.id12.1">†</span></sup>
</span><span class="ltx_author_notes"> CVC, Autonomous University of Barcelona, Spain. MICC, University of Florence, Italy. corresponding author: <a class="ltx_ref ltx_href" href="mailto:evivoli@cvc.uab.cat" title="">evivoli@cvc.uab.cat</a></span></span>
</div>
<div class="ltx_abstract">
<h6 class="ltx_title ltx_title_abstract">Abstract</h6>
<p class="ltx_p" id="id19.id1">Vision-language models have recently evolved into versatile systems capable of high performance across a range of tasks, such as document understanding, visual question answering, and grounding, often in zero-shot settings. <em class="ltx_emph ltx_font_italic" id="id19.id1.1">Comics Understanding</em>, a complex and multifaceted field, stands to greatly benefit from these advances. Comics, as a medium, combine rich visual and textual narratives, challenging AI models with tasks that span image classification, object detection, instance segmentation, and deeper narrative comprehension through sequential panels. However, the unique structure of comics—characterized by creative variations in style, reading order, and non-linear storytelling—presents a set of challenges distinct from those in other visual-language domains.
In this survey, we present a comprehensive review of Comics Understanding  from both dataset and task perspectives. Our contributions are fivefold:
(1) We analyze the structure of the comics medium, detailing its distinctive compositional elements;
(2) We survey the widely used datasets and tasks in comics research, emphasizing their role in advancing the field;
(3) We introduce the Layer of Comics Understanding  (LoCU) framework, a novel taxonomy that redefines vision-language tasks within comics and lays the foundation for future work;
(4) We provide a detailed review and categorization of existing methods following the LoCU framework;
(5) Finally, we highlight current research challenges and propose directions for future exploration, particularly in the context of vision-language models applied to comics.
This survey is the first to propose a task-oriented framework for comics intelligence and aims to guide future research by addressing critical gaps in data availability and task definition.
A project associated with this survey is available at <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/emanuelevivoli/awesome-comics-understanding" title="">https://github.com/emanuelevivoli/awesome-comics-understanding</a>.</p>
</div>
<div class="ltx_keywords">
<h6 class="ltx_title ltx_title_keywords">Index Terms: </h6>
Survey, Comics, Manga, Layers of Understanding, Comics Understanding, Vision-Language tasks

</div>
<section class="ltx_section" id="S1">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">1 </span><span class="ltx_text ltx_font_smallcaps" id="S1.1.1">Introduction</span>
</h2>
<div class="ltx_para" id="S1.p1">
<p class="ltx_p" id="S1.p1.1">Comics serve as a sophisticated medium that combines visual and textual elements to tell stories <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib1" title="">1</a>]</cite>. Applying recent machine learning tools to understand comics is complicated. This complexity comes from its unique domain that, driven by the author’s creativity, exhibits unique variations in both style and content. For example, modeling the drawing style is complicated, the reading order changes from one book to another, and, unlike natural images, comics frequently depict transformations that defy physical laws, adding another layer of complexity.</p>
</div>
<div class="ltx_para" id="S1.p2">
<p class="ltx_p" id="S1.p2.1">Motivated by these challenges, advanced artificial intelligence (AI) approaches have been increasingly applied to Comics Understanding. Given that AI thrives on tackling complex and diverse tasks, many researchers are now focusing on problems like
object detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib2" title="">2</a>]</cite>, semantic segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib3" title="">3</a>]</cite>, optical character recognition (OCR) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib4" title="">4</a>]</cite>, recurrence of characters and objects in varying contexts <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib5" title="">5</a>]</cite>, pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib6" title="">6</a>]</cite>, depth estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib7" title="">7</a>]</cite>, and integration of visual and textual narratives <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib8" title="">8</a>]</cite>. Comics also include complex narrative elements such as satire and irony and spatial and temporal dynamics within the story that have not been deeply investigated. Many of these tasks, individually or in combination, represent the cutting edge of advanced AI, particularly in the area of Multimodal Vision-Language understanding <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib9" title="">9</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib10" title="">10</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib11" title="">11</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib12" title="">12</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib13" title="">13</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S1.F1"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="176" id="S1.F1.g1" src="extracted/5855233/figures/first-figure.png" width="299"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F1.2.1.1" style="font-size:90%;">Figure 1</span>: </span><span class="ltx_text" id="S1.F1.3.2" style="font-size:90%;">Number of publications on Comics and Manga (from Google Scholar). The publications have been filtered by keywords (manga, comics, graphic novels), topics and journals (computer science).</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p3">
<p class="ltx_p" id="S1.p3.1">Historically, in machine learning (and deep learning), innovation has been driven by the available data, both in terms of tasks and more capable models.
As new types of annotated data and more complex tasks emerged, the need for innovative approaches grew, particularly in domains that combine multiple modalities.
The domain of multimodal learning, especially the convergence of vision and language, has witnessed significant advancements. These advancements include the shift away from independent uni-modal representations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib15" title="">15</a>]</cite> toward the alignment and integration of different modalities <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib16" title="">16</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib17" title="">17</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib18" title="">18</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib19" title="">19</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib21" title="">21</a>]</cite> and the creation of multimodal embedding space combining aligned uni-modal inputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib23" title="">23</a>]</cite>, with early and late fusion. Trained on extensive multimodal datasets, these models have shown remarkable abilities in tasks like multimodal retrieval <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib22" title="">22</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib23" title="">23</a>]</cite>, visual question answering, image captioning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib17" title="">17</a>]</cite>, reasoning <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib20" title="">20</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib21" title="">21</a>]</cite>, and retrieval-augmented multimodal generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib24" title="">24</a>]</cite> as well as image generation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib25" title="">25</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib26" title="">26</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p4">
<p class="ltx_p" id="S1.p4.1">The comics domain is uniquely well-suited to driving advancements in these types of multimodal reasoning models. Research in comics has extensively explored a range of questions, from the human ability to derive meaning from sequential images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib27" title="">27</a>]</cite> to machine interpretation of comic strips, particularly through closure tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib8" title="">8</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S1.p5">
<p class="ltx_p" id="S1.p5.1">With the continuous advancement in AI models and the intensive interest in harvesting vast knowledge from Comics, a lot of work is being published constantly. This is evidenced by a great number of recent papers as shown in Fig. <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S1.F1" title="Figure 1 ‣ 1 Introduction ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">1</span></a>.
These contributions shifted the landscape around Comics Understanding, leaving previous surveys outdated <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib6" title="">6</a>]</cite>.
Nowadays, the Comics Understanding  research community lacks a comprehensive survey that catalogs these papers and the various challenges in the Vision-Language domain.</p>
</div>
<div class="ltx_para" id="S1.p6">
<p class="ltx_p" id="S1.p6.1">To address this need, we introduce in this paper the novel <span class="ltx_text ltx_font_italic" id="S1.p6.1.1">Layer of Comics Understanding</span> (LoCU) framework.
This taxonomy, illustrated in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S2.T1" title="TABLE I ‣ 2.1.3 Modern Foundational models ‣ 2.1 The Comics Research Epochs ‣ 2 Background ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">I</span></a>, delineates tasks by their input/output modalities and the spatio-temporal dimensions necessary for reasoning over comic data, inviting a structured and progressive approach to comics analysis.
Motivated by the uniquely broad task distribution of the comics medium, this survey aggregates and examines all extant research on machine learning in comics, aimed at highlighting critical unresolved issues in the field. It is the first to review the breadth of comics research in a hierarchical structure, and to our knowledge, the first to propose a big-picture taxonomy of comics intelligence from a task-oriented perspective.</p>
</div>
<figure class="ltx_figure" id="S1.F2"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_square" height="857" id="S1.F2.g1" src="x1.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S1.F2.2.1.1" style="font-size:90%;">Figure 2</span>: </span><span class="ltx_text" id="S1.F2.3.2" style="font-size:90%;"> Visualization of the Layers of Comics Understanding tasks. From panel level to multipage, from unimodal to multimodal, from simplest to more complex.</span></figcaption>
</figure>
<div class="ltx_para" id="S1.p7">
<p class="ltx_p" id="S1.p7.1">Within this framework, the survey enumerates the state of the art in Comics Understanding tasks, including image classification, object detection, semantic segmentation, style transfer, multimodal retrieval, synthesis, and generation. Our comprehensive analysis uncovers fundamental gaps—specifically in data availability and task definition. </p>
</div>
<div class="ltx_para" id="S1.p8">
<p class="ltx_p" id="S1.p8.1">In summary, the main contributions of this work are threefold.
<span class="ltx_text ltx_font_italic" id="S1.p8.1.1">First</span>, it presents a systematic review of
Comics Understanding tasks. This is the <span class="ltx_text ltx_font_italic" id="S1.p8.1.2">first</span> survey of comics tasks for the Vision-Language domain, which provides a big picture of this promising research field with a comprehensive summary and categorization of existing studies.
<span class="ltx_text ltx_font_italic" id="S1.p8.1.3">Second</span>, it studies the up-to-date progress of Comics Understanding, including a comprehensive benchmarking and discussion of existing work over multiple public datasets.
<span class="ltx_text ltx_font_italic" id="S1.p8.1.4">Third</span>, it shares several research challenges and potential research directions that could be pursued in VLMs
for Comics Understanding.</p>
</div>
<div class="ltx_para" id="S1.p9">
<p class="ltx_p" id="S1.p9.1">The rest of this survey is organized as follows.
Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S2" title="2 Background ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">2</span></a> introduces the Comics medium as a composition of Visual-Language tasks, and several related surveys.
Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S3" title="3 Comics Foundations ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">3</span></a> describes the Foundations of Comics, including their structure, similarities, and differences among various types.
Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4" title="4 Tasks and Datasets ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">4</span></a> introduces existing tasks and the commonly used datasets in Comics Understanding, also focusing on evaluation procedures.
In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S5" title="5 Taxonomy: LoCU ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">5</span></a> we propose the Layer of Comics Understanding taxonomy (LoCU), and for each layer a collection of tasks grouped by difficulty (see Annex <span class="ltx_ref ltx_missing_label ltx_ref_self">LABEL:annex:values</span>). In Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6" title="6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">6</span></a> we start with Layer 1: Tagging and Augmentation, Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S7" title="7 Layer 2: Grounding, Analysis, and Segmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">7</span></a> with Layer 2: Grounding, Analysis, and Segmentation, in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S8" title="8 Layer 3: Retrieval and Modification ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">8</span></a> with Layer 3: Retrieval and Modification, Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S9" title="9 Layer 4: Advanced Visual-Language Understanding ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">9</span></a> with Layer 4: Understanding, and in Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S10" title="10 Layer 5: Generation and Synthesis ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">10</span></a> with Layer 5: Generation and Synthesis.
Finally, we conclude with Section <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S11" title="11 CONCLUSION ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">11</span></a>.</p>
</div>
</section>
<section class="ltx_section" id="S2">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">2 </span><span class="ltx_text ltx_font_smallcaps" id="S2.1.1">Background</span>
</h2>
<div class="ltx_para" id="S2.p1">
<p class="ltx_p" id="S2.p1.1">This section first presents the development of the comics research field based on the learning paradigm and how it evolves towards <span class="ltx_text ltx_font_italic" id="S2.p1.1.1">more complex tasks</span>.
Then, we introduce the main limitations encountered by the research community, as well as discuss several related surveys to highlight the scope and contributions of this work.</p>
</div>
<section class="ltx_subsection" id="S2.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.1 </span><span class="ltx_text ltx_font_italic" id="S2.SS1.1.1">The Comics Research Epochs</span>
</h3>
<div class="ltx_para" id="S2.SS1.p1">
<p class="ltx_p" id="S2.SS1.p1.1">The development of the comics field can be broadly divided into three phases, including (1) <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.1">Traditional Machine Learning</span>, (2) <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.2">Deep Learning</span>, and (3) <span class="ltx_text ltx_font_italic" id="S2.SS1.p1.1.3">Modern Foundational Models</span>.
In what follows, we introduce, compare, and analyze the three paradigms and how they shaped Comics advancements in detail.</p>
</div>
<section class="ltx_subsubsection" id="S2.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.1 </span>Traditional Machine Learning</h4>
<div class="ltx_para" id="S2.SS1.SSS1.p1">
<p class="ltx_p" id="S2.SS1.SSS1.p1.1">Early machine learning approaches heavily depended on <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p1.1.1">feature engineering</span>, utilizing hand-crafted features <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib28" title="">28</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib29" title="">29</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib30" title="">30</a>]</cite>, which were capable of, <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS1.p1.1.2">e.g.</span>, classify the style of a manga using SVMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib31" title="">31</a>]</cite>, retrieve a manga character from a sketch with edge orientation histograms <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib32" title="">32</a>]</cite> or segment comic balloons with energy functions and active contour models <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib33" title="">33</a>]</cite>. However, these paradigms require domain experts to craft effective features for specific tasks, which do not cope well with complex tasks and have poor generalizability. Moreover, the feature engineering obstacles the extension of the Comics Understanding to more tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.2 </span>Deep Learning approaches</h4>
<div class="ltx_para" id="S2.SS1.SSS2.p1">
<p class="ltx_p" id="S2.SS1.SSS2.p1.1">With the development of deep learning, researchers have achieved major improvements by using end-to-end trainable deep neural networks (DNNs). These models eliminate the need for complex <span class="ltx_text ltx_font_italic" id="S2.SS1.SSS2.p1.1.1">feature engineering</span> and shift the focus to designing network architectures that learn features automatically. Applying these paradigms to comics came as a natural step. Models became better at detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib34" title="">34</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib35" title="">35</a>]</cite> and segmentation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib36" title="">36</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib37" title="">37</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib38" title="">38</a>]</cite>, and more complex tasks started being investigated and solved (e.g. visual and textual closure <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib8" title="">8</a>]</cite>).
However, the turn from traditional machine learning toward deep learning raises a new grand challenge: the laborious collection of large-scale, task-specific, and crowd-labeled data in DNN training. Moreover, while deep learning methods excel in specific tasks, they are often difficult to adapt to multi-task. In the domain of comics, where data is scarce, protected by copyright, and challenging to annotate, these constraints have significantly hindered the development of more generalized, multi-task models for Comics Understanding.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S2.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">2.1.3 </span>Modern Foundational models</h4>
<div class="ltx_para" id="S2.SS1.SSS3.p1">
<p class="ltx_p" id="S2.SS1.SSS3.p1.1">The era of foundational models marks a significant evolution in the application of AI to comics, with models that can learn from vast amounts of unstructured data to perform a variety of complex tasks <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib14" title="">14</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib39" title="">39</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib40" title="">40</a>]</cite>.
These models relax previous limitations by “solving” simple yet important tasks as a zero-shot counter effect, e.g. panel, character, and text-box detection <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite>. By leveraging these foundational models, more advanced tasks can be proposed and takled <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib42" title="">42</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib43" title="">43</a>]</cite>, opening the doors to yet unexplored more complex ones.</p>
</div>
<figure class="ltx_table" id="S2.T1">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S2.T1.3.1.1" style="font-size:90%;">TABLE I</span>: </span><span class="ltx_text" id="S2.T1.4.2" style="font-size:90%;">Overview of Vision-Language Tasks of the <span class="ltx_text ltx_font_italic" id="S2.T1.4.2.1">Layers of Comics Understanding</span>. The ranking is based on input and output modalities and dimensions, as illustrated in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#Ax2" title="B. Taxonomy ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title">B. Taxonomy</span></a>.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S2.T1.5">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S2.T1.5.1.1" style="background-color:#D9D9D9;">
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.1.1" style="background-color:#D9D9D9;">Layer</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.2.1" style="background-color:#D9D9D9;">Category</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.3.1" style="background-color:#D9D9D9;">Task</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.4.1" style="background-color:#D9D9D9;">Input</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S2.T1.5.1.1.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.1.1.5.1" style="background-color:#D9D9D9;">Output</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.2.2">
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.2.2.1" style="background-color:#6AA84F;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.2.2.1.1" style="color:#FFFFFF;background-color:#6AA84F;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.2.2.1.1.1">
<span class="ltx_tr" id="S2.T1.5.2.2.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.2.2.1.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">0</span></span>
<span class="ltx_tr" id="S2.T1.5.2.2.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.2.2.1.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">(Sec.5)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.2.2.2" style="background-color:#6AA84F;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.2.2.2.1" style="color:#FFFFFF;background-color:#6AA84F;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.2.2.2.1.1">
<span class="ltx_tr" id="S2.T1.5.2.2.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.2.2.2.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.2.2.2.1.1.1.1.1">View</span></span></span>
<span class="ltx_tr" id="S2.T1.5.2.2.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.2.2.2.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.2.2.2.1.1.2.1.1">(Sec. 5.1)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.2.2.3" style="background-color:#B6D7A8;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.2.2.3.1" style="background-color:#B6D7A8;">Basic Image Viewing (BIV)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.2.2.4" style="background-color:#FFFFFF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.2.2.4.1" style="background-color:#FFFFFF;">Text Command</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.2.2.5" style="background-color:#FFFFFF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.2.2.5.1" style="background-color:#FFFFFF;">Image Display</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.3.3">
<td class="ltx_td ltx_border_t" id="S2.T1.5.3.3.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.5.3.3.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.3.3.3" style="background-color:#D9EAD3;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.3.3.3.1" style="background-color:#D9EAD3;">Image Classification (I-CLS)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.3.3.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.3.3.4.1" style="background-color:#EFEFEF;">Image</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.3.3.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.3.3.5.1" style="background-color:#EFEFEF;">Tag</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.4.4">
<td class="ltx_td" id="S2.T1.5.4.4.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.4.4.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.4.4.3" style="background-color:#D9EAD3;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.4.4.3.1" style="background-color:#D9EAD3;">Emotion Classification</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.4.4.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Comic Panels/Images</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.4.4.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Emotion Labels</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.5.5">
<td class="ltx_td" id="S2.T1.5.5.5.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.5.5.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.5.3" style="background-color:#D9EAD3;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.5.5.3.1" style="background-color:#D9EAD3;">Action Detection</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.5.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.5.5.4.1" style="background-color:#EFEFEF;">Multiple Panels</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.5.5.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.5.5.5.1" style="background-color:#EFEFEF;">Tag</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.6.6">
<td class="ltx_td" id="S2.T1.5.6.6.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.6.6.2" style="background-color:#B6D7A8;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.6.6.2.1" style="color:#FFFFFF;background-color:#B6D7A8;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.6.6.2.1.1">
<span class="ltx_tr" id="S2.T1.5.6.6.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.6.6.2.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.6.6.2.1.1.1.1.1">Tagging</span></span></span>
<span class="ltx_tr" id="S2.T1.5.6.6.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.6.6.2.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.6.6.2.1.1.2.1.1">(Sec. 6.1)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.6.6.3" style="background-color:#D9EAD3;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.6.6.3.1" style="background-color:#D9EAD3;">Page Stream Segmentation (PSS)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.6.6.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Images</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.6.6.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Tags sequence</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.7.7">
<td class="ltx_td" id="S2.T1.5.7.7.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.7.7.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.7.7.3" style="background-color:#FFF2CC;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.7.7.3.1" style="background-color:#FFF2CC;">Image Super-Resolution (ISR)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.7.7.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.7.7.4.1" style="background-color:#EFEFEF;">Image</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.7.7.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.7.7.5.1" style="background-color:#EFEFEF;">Image</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.8.8">
<td class="ltx_td" id="S2.T1.5.8.8.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.8.8.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.8.8.3" style="background-color:#FFF2CC;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.8.8.3.1" style="background-color:#FFF2CC;">Style Transfer (ST)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.8.8.4" style="background-color:#FFFFFF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.8.8.4.1" style="background-color:#FFFFFF;">Image</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.8.8.5" style="background-color:#FFFFFF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.8.8.5.1" style="background-color:#FFFFFF;">Image</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.9.9">
<td class="ltx_td" id="S2.T1.5.9.9.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.9.9.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.9.9.3" style="background-color:#FFF2CC;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.9.9.3.1" style="background-color:#FFF2CC;">Vectorization</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.9.9.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.9.9.4.1" style="background-color:#EFEFEF;">Comic Panels/Images</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.9.9.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.9.9.5.1" style="background-color:#EFEFEF;">Vector Image</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.10.10">
<td class="ltx_td ltx_align_center" id="S2.T1.5.10.10.1" style="background-color:#DED06E;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.10.10.1.1" style="color:#FFFFFF;background-color:#DED06E;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.10.10.1.1.1">
<span class="ltx_tr" id="S2.T1.5.10.10.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.10.10.1.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1</span></span>
<span class="ltx_tr" id="S2.T1.5.10.10.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.10.10.1.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">(Sec.6)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.10.10.2" style="background-color:#DDD06E;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.10.10.2.1" style="color:#FFFFFF;background-color:#DDD06E;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.10.10.2.1.1">
<span class="ltx_tr" id="S2.T1.5.10.10.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.10.10.2.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.10.10.2.1.1.1.1.1">Augmentation</span></span></span>
<span class="ltx_tr" id="S2.T1.5.10.10.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.10.10.2.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.10.10.2.1.1.2.1.1">(Sec. 6.2)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.10.10.3" style="background-color:#FFF2CC;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.10.10.3.1" style="background-color:#FFF2CC;">Depth Estimation</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.10.10.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Comic Panels/Images</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.10.10.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Depth Map</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.11.11">
<td class="ltx_td ltx_border_t" id="S2.T1.5.11.11.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.5.11.11.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.11.11.3" style="background-color:#FFE599;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.11.11.3.1" style="background-color:#FFE599;">Object Detection</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.11.11.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.11.11.4.1" style="background-color:#EFEFEF;">Tag/s + Image</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.11.11.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.11.11.5.1" style="background-color:#EFEFEF;">Bounding Boxes</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.12.12">
<td class="ltx_td" id="S2.T1.5.12.12.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.12.12.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.12.12.3" style="background-color:#FFE599;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.12.12.3.1" style="background-color:#FFE599;">Character Re-identification (Character ID)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.12.12.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Multiple Panels</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.12.12.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Tag</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.13.13">
<td class="ltx_td" id="S2.T1.5.13.13.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.13.13.2" style="background-color:#FFD966;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.13.13.2.1" style="color:#FFFFFF;background-color:#FFD966;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.13.13.2.1.1">
<span class="ltx_tr" id="S2.T1.5.13.13.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.13.13.2.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.13.13.2.1.1.1.1.1">Grounding</span></span></span>
<span class="ltx_tr" id="S2.T1.5.13.13.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.13.13.2.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.13.13.2.1.1.2.1.1">(Sec. 7.1)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.13.13.3" style="background-color:#FFE599;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.13.13.3.1" style="background-color:#FFE599;">Grounding (IG)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.13.13.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.13.13.4.1" style="background-color:#EFEFEF;">[Prompt] + Image</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.13.13.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.13.13.5.1" style="background-color:#EFEFEF;">Bounding Boxes</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.14.14">
<td class="ltx_td" id="S2.T1.5.14.14.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.14.14.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.14.14.3" style="background-color:#F9CB9C;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.14.14.3.1" style="background-color:#F9CB9C;">Character-Balloon Association (Speaker ID)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.14.14.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Character + Balloons</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.14.14.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Tag</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.15.15">
<td class="ltx_td" id="S2.T1.5.15.15.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.15.15.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.15.15.3" style="background-color:#F9CB9C;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.15.15.3.1" style="background-color:#F9CB9C;">Dialog transcription</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.15.15.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.15.15.4.1" style="background-color:#EFEFEF;">Image</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.15.15.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.15.15.5.1" style="background-color:#EFEFEF;">Text</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.16.16">
<td class="ltx_td" id="S2.T1.5.16.16.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.16.16.2" style="background-color:#F6B26B;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.16.16.2.1" style="color:#FFFFFF;background-color:#F6B26B;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.16.16.2.1.1">
<span class="ltx_tr" id="S2.T1.5.16.16.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.16.16.2.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.16.16.2.1.1.1.1.1">Analysis</span></span></span>
<span class="ltx_tr" id="S2.T1.5.16.16.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.16.16.2.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.16.16.2.1.1.2.1.1">(Sec. 7.2)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.16.16.3" style="background-color:#F9CB9C;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.16.16.3.1" style="background-color:#F9CB9C;">Translation</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.16.16.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Image</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.16.16.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Text</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.17.17">
<td class="ltx_td ltx_align_center" id="S2.T1.5.17.17.1" style="background-color:#E8A84D;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.17.17.1.1" style="color:#FFFFFF;background-color:#E8A84D;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.17.17.1.1.1">
<span class="ltx_tr" id="S2.T1.5.17.17.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.17.17.1.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2</span></span>
<span class="ltx_tr" id="S2.T1.5.17.17.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.17.17.1.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">(Sec.7)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.17.17.2" style="background-color:#E69138;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.17.17.2.1" style="color:#FFFFFF;background-color:#E69138;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.17.17.2.1.1">
<span class="ltx_tr" id="S2.T1.5.17.17.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.17.17.2.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.17.17.2.1.1.1.1.1">Segmentation</span></span></span>
<span class="ltx_tr" id="S2.T1.5.17.17.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.17.17.2.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.17.17.2.1.1.2.1.1">(Sec. 7.3)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.17.17.3" style="background-color:#F6B26B;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.17.17.3.1" style="background-color:#F6B26B;">Instance Segmentation (IS)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.17.17.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.17.17.4.1" style="background-color:#EFEFEF;">[Prompt] + Image</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.17.17.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.17.17.5.1" style="background-color:#EFEFEF;">Segments</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.18.18">
<td class="ltx_td ltx_border_t" id="S2.T1.5.18.18.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.5.18.18.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.18.18.3" style="background-color:#EAD1DC;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.18.18.3.1" style="background-color:#EAD1DC;">Image-Text Retrieval (IR)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.18.18.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Text</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.18.18.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Image</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.19.19">
<td class="ltx_td" id="S2.T1.5.19.19.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.19.19.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.19.19.3" style="background-color:#EAD1DC;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.19.19.3.1" style="background-color:#EAD1DC;">Text-Image Retrieval (TR)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.19.19.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.19.19.4.1" style="background-color:#EFEFEF;">Image</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.19.19.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.19.19.5.1" style="background-color:#EFEFEF;">Text</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.20.20">
<td class="ltx_td" id="S2.T1.5.20.20.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.20.20.2" style="background-color:#E06666;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.20.20.2.1" style="color:#FFFFFF;background-color:#E06666;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.20.20.2.1.1" style="background-color:#E06666;">
<span class="ltx_tr" id="S2.T1.5.20.20.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.20.20.2.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.20.20.2.1.1.1.1.1" style="color:#FFFFFF;">Retrieval</span></span></span>
<span class="ltx_tr" id="S2.T1.5.20.20.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.20.20.2.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.20.20.2.1.1.2.1.1" style="color:#FFFFFF;">(Sec. 8.1)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.20.20.3" style="background-color:#EAD1DC;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.20.20.3.1" style="background-color:#EAD1DC;">Composed Image Retrieval (CIR)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.20.20.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Text + Image</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.20.20.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Image</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.21.21">
<td class="ltx_td" id="S2.T1.5.21.21.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.21.21.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.21.21.3" style="background-color:#EA9999;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.21.21.3.1" style="background-color:#EA9999;">Image Inpainting (II)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.21.21.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.21.21.4.1" style="background-color:#EFEFEF;">Text + [prompt] + Image</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.21.21.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.21.21.5.1" style="background-color:#EFEFEF;">Image</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.22.22">
<td class="ltx_td ltx_align_center" id="S2.T1.5.22.22.1" style="background-color:#E06666;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.22.22.1.1" style="color:#FFFFFF;background-color:#E06666;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.22.22.1.1.1">
<span class="ltx_tr" id="S2.T1.5.22.22.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.22.22.1.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3</span></span>
<span class="ltx_tr" id="S2.T1.5.22.22.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.22.22.1.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">(Sec.8)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.22.22.2" style="background-color:#CC4125;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.22.22.2.1" style="color:#FFFFFF;background-color:#CC4125;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.22.22.2.1.1" style="background-color:#CC4125;">
<span class="ltx_tr" id="S2.T1.5.22.22.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.22.22.2.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.22.22.2.1.1.1.1.1" style="color:#FFFFFF;">Modification</span></span></span>
<span class="ltx_tr" id="S2.T1.5.22.22.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.22.22.2.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.22.22.2.1.1.2.1.1" style="color:#FFFFFF;">(Sec. 8.2)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.22.22.3" style="background-color:#EA9999;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.22.22.3.1" style="background-color:#EA9999;">Image Editing via Text (IET)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.22.22.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Text + Image</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.22.22.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Image</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.23.23">
<td class="ltx_td ltx_border_t" id="S2.T1.5.23.23.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.5.23.23.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.23.23.3" style="background-color:#E06666;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.23.23.3.1" style="background-color:#E06666;">Visual Question Answering (VQA)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.23.23.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Image + Text</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.23.23.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Text</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.24.24">
<td class="ltx_td" id="S2.T1.5.24.24.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.24.24.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.24.24.3" style="background-color:#E06666;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.24.24.3.1" style="background-color:#E06666;">Visual Dialog (VisDial)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.24.24.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.24.24.4.1" style="background-color:#EFEFEF;">Image + Dialog + Text</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.24.24.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.24.24.5.1" style="background-color:#EFEFEF;">Text</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.25.25">
<td class="ltx_td ltx_align_center" id="S2.T1.5.25.25.1" style="background-color:#CC4125;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.25.25.1.1" style="color:#FFFFFF;background-color:#CC4125;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.25.25.1.1.1">
<span class="ltx_tr" id="S2.T1.5.25.25.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.25.25.1.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">4</span></span>
<span class="ltx_tr" id="S2.T1.5.25.25.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.25.25.1.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">(Sec.9)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.25.25.2" style="background-color:#990000;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.25.25.2.1" style="color:#FFFFFF;background-color:#990000;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.25.25.2.1.1">
<span class="ltx_tr" id="S2.T1.5.25.25.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.25.25.2.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.25.25.2.1.1.1.1.1">Understanding</span></span></span>
<span class="ltx_tr" id="S2.T1.5.25.25.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.25.25.2.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.25.25.2.1.1.2.1.1">(Sec. 9.1)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.25.25.3" style="background-color:#E06666;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.25.25.3.1" style="background-color:#E06666;">Visual Reasoning (VR)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.25.25.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Image + Text</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.25.25.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Text</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.26.26">
<td class="ltx_td ltx_border_t" id="S2.T1.5.26.26.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_t" id="S2.T1.5.26.26.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.26.26.3" style="background-color:#DD7E6B;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.26.26.3.1" style="background-color:#DD7E6B;">Image-2-Text Generation (I2T)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.26.26.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.26.26.4.1" style="background-color:#EFEFEF;">Image</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S2.T1.5.26.26.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.26.26.5.1" style="background-color:#EFEFEF;">Text</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.27.27">
<td class="ltx_td" id="S2.T1.5.27.27.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.27.27.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.27.27.3" style="background-color:#DD7E6B;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.27.27.3.1" style="background-color:#DD7E6B;">Grounded Image Captioning (GIC)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.27.27.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Image</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.27.27.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Text + Bbox</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.28.28">
<td class="ltx_td" id="S2.T1.5.28.28.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.28.28.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.28.28.3" style="background-color:#DD7E6B;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.28.28.3.1" style="background-color:#DD7E6B;">Text-2-Image Generation (T2I)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.28.28.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.28.28.4.1" style="background-color:#EFEFEF;">Text</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.28.28.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.28.28.5.1" style="background-color:#EFEFEF;">Image</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.29.29">
<td class="ltx_td" id="S2.T1.5.29.29.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.29.29.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.29.29.3" style="background-color:#DD7E6B;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.29.29.3.1" style="background-color:#DD7E6B;">Scene Graph Generation for Captioning</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.29.29.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Comic Panel</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.29.29.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Scene Graph</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.30.30">
<td class="ltx_td" id="S2.T1.5.30.30.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.30.30.2" style="background-color:#A61C00;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.30.30.2.1" style="color:#FFFFFF;background-color:#A61C00;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.30.30.2.1.1" style="background-color:#A61C00;">
<span class="ltx_tr" id="S2.T1.5.30.30.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.30.30.2.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.30.30.2.1.1.1.1.1" style="color:#FFFFFF;">Generation</span></span></span>
<span class="ltx_tr" id="S2.T1.5.30.30.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.30.30.2.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.30.30.2.1.1.2.1.1" style="color:#FFFFFF;">(Sec. 10.1)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.30.30.3" style="background-color:#DD7E6B;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.30.30.3.1" style="background-color:#DD7E6B;">Sound Generation from Single Panel</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.30.30.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.30.30.4.1" style="background-color:#EFEFEF;">Single Comic Panel</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.30.30.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.30.30.5.1" style="background-color:#EFEFEF;">Sound/Audio</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.31.31">
<td class="ltx_td" id="S2.T1.5.31.31.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.31.31.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.31.31.3" style="background-color:#C27BA0;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.31.31.3.1" style="background-color:#C27BA0;">3D Model Generation from Images (3DGI)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.31.31.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Collection of Images</td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.31.31.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3D Model</td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.32.32">
<td class="ltx_td" id="S2.T1.5.32.32.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td" id="S2.T1.5.32.32.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.32.32.3" style="background-color:#C27BA0;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.32.32.3.1" style="background-color:#C27BA0;">Video Generation from Text (VGT)</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.32.32.4" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.32.32.4.1" style="background-color:#EFEFEF;">Complex Long Text</span></td>
<td class="ltx_td ltx_align_center" id="S2.T1.5.32.32.5" style="background-color:#EFEFEF;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.32.32.5.1" style="background-color:#EFEFEF;">Video</span></td>
</tr>
<tr class="ltx_tr" id="S2.T1.5.33.33">
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.33.33.1" style="background-color:#A61C00;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.33.33.1.1" style="color:#FFFFFF;background-color:#A61C00;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.33.33.1.1.1">
<span class="ltx_tr" id="S2.T1.5.33.33.1.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.33.33.1.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">5</span></span>
<span class="ltx_tr" id="S2.T1.5.33.33.1.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.33.33.1.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">(Sec.10)</span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.33.33.2" style="background-color:#741B47;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.33.33.2.1" style="color:#FFFFFF;background-color:#741B47;">
<span class="ltx_tabular ltx_align_middle" id="S2.T1.5.33.33.2.1.1">
<span class="ltx_tr" id="S2.T1.5.33.33.2.1.1.1">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.33.33.2.1.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.33.33.2.1.1.1.1.1">Synthesis</span></span></span>
<span class="ltx_tr" id="S2.T1.5.33.33.2.1.1.2">
<span class="ltx_td ltx_nopad_r ltx_align_center" id="S2.T1.5.33.33.2.1.1.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S2.T1.5.33.33.2.1.1.2.1.1">(Sec. 10.2)</span></span></span>
</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.33.33.3" style="background-color:#C27BA0;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S2.T1.5.33.33.3.1" style="background-color:#C27BA0;">Narrative-Based Complex Scene Generation (NCSG)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.33.33.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Detailed Narrative Text</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S2.T1.5.33.33.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Series of Images</td>
</tr>
</tbody>
</table>
</figure>
</section>
</section>
<section class="ltx_subsection" id="S2.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">2.2 </span><span class="ltx_text ltx_font_italic" id="S2.SS2.1.1">Relevant Surveys</span>
</h3>
<div class="ltx_para" id="S2.SS2.p1">
<p class="ltx_p" id="S2.SS2.p1.1">To the best of our knowledge, this is the <span class="ltx_text ltx_font_italic" id="S2.SS2.p1.1.1">first</span> survey that reviews comics papers with a task-oriented approach for Vision-Language. Several relevant surveys have been conducted, examining broader aspects of visual art <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib44" title="">44</a>]</cite> and, more specifically, comics <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib45" title="">45</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib6" title="">6</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib46" title="">46</a>]</cite>. Notably, Augereau et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib6" title="">6</a>]</cite> categorized comic research into three primary areas: (i) content analysis, (ii) content generation and adaptation, and (iii) user interaction, which remain relevant entry points for understanding this field’s research and applications.
Since those surveys were published, foundational models have emerged, enabling significant advancements, particularly due to their ability to generalize across tasks without task-specific training. These models have unlocked new possibilities in comics research, allowing automatic annotation and analysis, among other tasks.</p>
</div>
<div class="ltx_para" id="S2.SS2.p2">
<p class="ltx_p" id="S2.SS2.p2.1">Building on these advancements, our survey focuses on two key aspects: 1) the recent progress in Comics Understanding tasks and datasets, and 2) the application of Vision-Language tasks to the unique characteristics of comics.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S3">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">3 </span><span class="ltx_text ltx_font_smallcaps" id="S3.1.1">Comics Foundations</span>
</h2>
<div class="ltx_para" id="S3.p1">
<p class="ltx_p" id="S3.p1.1">Our preliminary analysis prioritizes understanding the structural intricacies of comics.
The term “comics” broadly denotes the entire medium in its uncountable form, while “a comic” refers to a singular entity within this medium, such as a comic book or strip. The structure of comic books has been a subject of extensive study, examining aspects like their commonalities, layout structures <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib47" title="">47</a>]</cite>, content arrangement, narrative transitions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib48" title="">48</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib27" title="">27</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib49" title="">49</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib50" title="">50</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib51" title="">51</a>]</cite>, and stylistic narrative variations <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib52" title="">52</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib53" title="">53</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib54" title="">54</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S3.F3"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="486" id="S3.F3.g1" src="x2.png" width="830"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S3.F3.2.1.1" style="font-size:90%;">Figure 3</span>: </span><span class="ltx_text" id="S3.F3.3.2" style="font-size:90%;">Anatomy of Comic Page Elements: This illustration delineates the various components found on comic pages including object detection, linkages between elements, the designated reading order of texts and panels, as well as key textual features like character names—all arranged in a logical sequence throughout the panels and pages.</span></figcaption>
</figure>
<div class="ltx_para" id="S3.p2">
<p class="ltx_p" id="S3.p2.1">Typically, comics are presented in single or double-page formats, influenced by the artist’s style. They narrate stories across various genres or series, such as superheroes and science fiction tales, sometimes interlinking through crossovers. Notably, artistic consistency within a series or genre, often achieved through collaborative efforts, is a key feature. Each comic page (as illustrated in Figure <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S3.F3" title="Figure 3 ‣ 3 Comics Foundations ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">3</span></a>) encapsulates individual scenes within panels of diverse types - from splash panels for opening scenes to widescreen panels for expansive narratives. These panels convey interactions and dialogues through speech balloons, thought bubbles, and expressive onomatopoeia, enriching the narrative experience.
Artistic variations in style and color also play a significant role in comics, ranging from detailed realism to abstract forms, with the color palette setting the mood and atmosphere.</p>
</div>
<div class="ltx_para" id="S3.p3">
<p class="ltx_p" id="S3.p3.1">Despite rigorous analyses <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib55" title="">55</a>]</cite>, no significant differences have been identified in terms of attention, subjectivity, and viewpoint between Manga and American comics. Hence, these distinctions are better explored within their distinct historical, cultural, and artistic contexts:</p>
</div>
<div class="ltx_para" id="S3.p4">
<p class="ltx_p" id="S3.p4.1"><span class="ltx_text ltx_font_bold" id="S3.p4.1.1">American Comics</span>: With a rich history dating back to the 1930s, American comics are celebrated for their colorful and high-quality presentations, primarily known for their superhero narratives. They adhere to a top-to-bottom, left-to-right reading format and are characterized by a dialogue-rich and brisk storytelling style.</p>
</div>
<div class="ltx_para" id="S3.p5">
<p class="ltx_p" id="S3.p5.1"><span class="ltx_text ltx_font_bold" id="S3.p5.1.1">Manga</span>: Emerging in post-World War II Japan and influenced by diverse sources, including Japanese traditional art and American comics, manga is recognized for its unique black-and-white style, though colored variants exist. Covering a broad range of genres, Manga is read right-to-left, top-to-bottom. This style is marked by a distinctive “Japanese Visual Language” that is consistent across genres <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib56" title="">56</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S3.p6">
<p class="ltx_p" id="S3.p6.7"><span class="ltx_text ltx_font_bold" id="S3.p6.7.1">Comic Formalisms.</span> Comics are defined as sequential visual narratives that utilize a combination of images and text within distinct frames (panels) to convey a story or information. Each comic, or manga, can be represented as a document <math alttext="D" class="ltx_Math" display="inline" id="S3.p6.1.m1.1"><semantics id="S3.p6.1.m1.1a"><mi id="S3.p6.1.m1.1.1" xref="S3.p6.1.m1.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.p6.1.m1.1b"><ci id="S3.p6.1.m1.1.1.cmml" xref="S3.p6.1.m1.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.1.m1.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.p6.1.m1.1d">italic_D</annotation></semantics></math>.
Each document <math alttext="D" class="ltx_Math" display="inline" id="S3.p6.2.m2.1"><semantics id="S3.p6.2.m2.1a"><mi id="S3.p6.2.m2.1.1" xref="S3.p6.2.m2.1.1.cmml">D</mi><annotation-xml encoding="MathML-Content" id="S3.p6.2.m2.1b"><ci id="S3.p6.2.m2.1.1.cmml" xref="S3.p6.2.m2.1.1">𝐷</ci></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.2.m2.1c">D</annotation><annotation encoding="application/x-llamapun" id="S3.p6.2.m2.1d">italic_D</annotation></semantics></math> consists of a series of pages <math alttext="P=\{p_{1},p_{2},\ldots,p_{n}\}" class="ltx_Math" display="inline" id="S3.p6.3.m3.4"><semantics id="S3.p6.3.m3.4a"><mrow id="S3.p6.3.m3.4.4" xref="S3.p6.3.m3.4.4.cmml"><mi id="S3.p6.3.m3.4.4.5" xref="S3.p6.3.m3.4.4.5.cmml">P</mi><mo id="S3.p6.3.m3.4.4.4" xref="S3.p6.3.m3.4.4.4.cmml">=</mo><mrow id="S3.p6.3.m3.4.4.3.3" xref="S3.p6.3.m3.4.4.3.4.cmml"><mo id="S3.p6.3.m3.4.4.3.3.4" stretchy="false" xref="S3.p6.3.m3.4.4.3.4.cmml">{</mo><msub id="S3.p6.3.m3.2.2.1.1.1" xref="S3.p6.3.m3.2.2.1.1.1.cmml"><mi id="S3.p6.3.m3.2.2.1.1.1.2" xref="S3.p6.3.m3.2.2.1.1.1.2.cmml">p</mi><mn id="S3.p6.3.m3.2.2.1.1.1.3" xref="S3.p6.3.m3.2.2.1.1.1.3.cmml">1</mn></msub><mo id="S3.p6.3.m3.4.4.3.3.5" xref="S3.p6.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.p6.3.m3.3.3.2.2.2" xref="S3.p6.3.m3.3.3.2.2.2.cmml"><mi id="S3.p6.3.m3.3.3.2.2.2.2" xref="S3.p6.3.m3.3.3.2.2.2.2.cmml">p</mi><mn id="S3.p6.3.m3.3.3.2.2.2.3" xref="S3.p6.3.m3.3.3.2.2.2.3.cmml">2</mn></msub><mo id="S3.p6.3.m3.4.4.3.3.6" xref="S3.p6.3.m3.4.4.3.4.cmml">,</mo><mi id="S3.p6.3.m3.1.1" mathvariant="normal" xref="S3.p6.3.m3.1.1.cmml">…</mi><mo id="S3.p6.3.m3.4.4.3.3.7" xref="S3.p6.3.m3.4.4.3.4.cmml">,</mo><msub id="S3.p6.3.m3.4.4.3.3.3" xref="S3.p6.3.m3.4.4.3.3.3.cmml"><mi id="S3.p6.3.m3.4.4.3.3.3.2" xref="S3.p6.3.m3.4.4.3.3.3.2.cmml">p</mi><mi id="S3.p6.3.m3.4.4.3.3.3.3" xref="S3.p6.3.m3.4.4.3.3.3.3.cmml">n</mi></msub><mo id="S3.p6.3.m3.4.4.3.3.8" stretchy="false" xref="S3.p6.3.m3.4.4.3.4.cmml">}</mo></mrow></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.3.m3.4b"><apply id="S3.p6.3.m3.4.4.cmml" xref="S3.p6.3.m3.4.4"><eq id="S3.p6.3.m3.4.4.4.cmml" xref="S3.p6.3.m3.4.4.4"></eq><ci id="S3.p6.3.m3.4.4.5.cmml" xref="S3.p6.3.m3.4.4.5">𝑃</ci><set id="S3.p6.3.m3.4.4.3.4.cmml" xref="S3.p6.3.m3.4.4.3.3"><apply id="S3.p6.3.m3.2.2.1.1.1.cmml" xref="S3.p6.3.m3.2.2.1.1.1"><csymbol cd="ambiguous" id="S3.p6.3.m3.2.2.1.1.1.1.cmml" xref="S3.p6.3.m3.2.2.1.1.1">subscript</csymbol><ci id="S3.p6.3.m3.2.2.1.1.1.2.cmml" xref="S3.p6.3.m3.2.2.1.1.1.2">𝑝</ci><cn id="S3.p6.3.m3.2.2.1.1.1.3.cmml" type="integer" xref="S3.p6.3.m3.2.2.1.1.1.3">1</cn></apply><apply id="S3.p6.3.m3.3.3.2.2.2.cmml" xref="S3.p6.3.m3.3.3.2.2.2"><csymbol cd="ambiguous" id="S3.p6.3.m3.3.3.2.2.2.1.cmml" xref="S3.p6.3.m3.3.3.2.2.2">subscript</csymbol><ci id="S3.p6.3.m3.3.3.2.2.2.2.cmml" xref="S3.p6.3.m3.3.3.2.2.2.2">𝑝</ci><cn id="S3.p6.3.m3.3.3.2.2.2.3.cmml" type="integer" xref="S3.p6.3.m3.3.3.2.2.2.3">2</cn></apply><ci id="S3.p6.3.m3.1.1.cmml" xref="S3.p6.3.m3.1.1">…</ci><apply id="S3.p6.3.m3.4.4.3.3.3.cmml" xref="S3.p6.3.m3.4.4.3.3.3"><csymbol cd="ambiguous" id="S3.p6.3.m3.4.4.3.3.3.1.cmml" xref="S3.p6.3.m3.4.4.3.3.3">subscript</csymbol><ci id="S3.p6.3.m3.4.4.3.3.3.2.cmml" xref="S3.p6.3.m3.4.4.3.3.3.2">𝑝</ci><ci id="S3.p6.3.m3.4.4.3.3.3.3.cmml" xref="S3.p6.3.m3.4.4.3.3.3.3">𝑛</ci></apply></set></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.3.m3.4c">P=\{p_{1},p_{2},\ldots,p_{n}\}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.3.m3.4d">italic_P = { italic_p start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_p start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_p start_POSTSUBSCRIPT italic_n end_POSTSUBSCRIPT }</annotation></semantics></math>. A page <math alttext="p_{i}" class="ltx_Math" display="inline" id="S3.p6.4.m4.1"><semantics id="S3.p6.4.m4.1a"><msub id="S3.p6.4.m4.1.1" xref="S3.p6.4.m4.1.1.cmml"><mi id="S3.p6.4.m4.1.1.2" xref="S3.p6.4.m4.1.1.2.cmml">p</mi><mi id="S3.p6.4.m4.1.1.3" xref="S3.p6.4.m4.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.4.m4.1b"><apply id="S3.p6.4.m4.1.1.cmml" xref="S3.p6.4.m4.1.1"><csymbol cd="ambiguous" id="S3.p6.4.m4.1.1.1.cmml" xref="S3.p6.4.m4.1.1">subscript</csymbol><ci id="S3.p6.4.m4.1.1.2.cmml" xref="S3.p6.4.m4.1.1.2">𝑝</ci><ci id="S3.p6.4.m4.1.1.3.cmml" xref="S3.p6.4.m4.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.4.m4.1c">p_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.4.m4.1d">italic_p start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> can be described by its layout and the elements it contains: (i) a spatial arrangement of panels <math alttext="\{c_{1},c_{2},\ldots,c_{m}\}" class="ltx_Math" display="inline" id="S3.p6.5.m5.4"><semantics id="S3.p6.5.m5.4a"><mrow id="S3.p6.5.m5.4.4.3" xref="S3.p6.5.m5.4.4.4.cmml"><mo id="S3.p6.5.m5.4.4.3.4" stretchy="false" xref="S3.p6.5.m5.4.4.4.cmml">{</mo><msub id="S3.p6.5.m5.2.2.1.1" xref="S3.p6.5.m5.2.2.1.1.cmml"><mi id="S3.p6.5.m5.2.2.1.1.2" xref="S3.p6.5.m5.2.2.1.1.2.cmml">c</mi><mn id="S3.p6.5.m5.2.2.1.1.3" xref="S3.p6.5.m5.2.2.1.1.3.cmml">1</mn></msub><mo id="S3.p6.5.m5.4.4.3.5" xref="S3.p6.5.m5.4.4.4.cmml">,</mo><msub id="S3.p6.5.m5.3.3.2.2" xref="S3.p6.5.m5.3.3.2.2.cmml"><mi id="S3.p6.5.m5.3.3.2.2.2" xref="S3.p6.5.m5.3.3.2.2.2.cmml">c</mi><mn id="S3.p6.5.m5.3.3.2.2.3" xref="S3.p6.5.m5.3.3.2.2.3.cmml">2</mn></msub><mo id="S3.p6.5.m5.4.4.3.6" xref="S3.p6.5.m5.4.4.4.cmml">,</mo><mi id="S3.p6.5.m5.1.1" mathvariant="normal" xref="S3.p6.5.m5.1.1.cmml">…</mi><mo id="S3.p6.5.m5.4.4.3.7" xref="S3.p6.5.m5.4.4.4.cmml">,</mo><msub id="S3.p6.5.m5.4.4.3.3" xref="S3.p6.5.m5.4.4.3.3.cmml"><mi id="S3.p6.5.m5.4.4.3.3.2" xref="S3.p6.5.m5.4.4.3.3.2.cmml">c</mi><mi id="S3.p6.5.m5.4.4.3.3.3" xref="S3.p6.5.m5.4.4.3.3.3.cmml">m</mi></msub><mo id="S3.p6.5.m5.4.4.3.8" stretchy="false" xref="S3.p6.5.m5.4.4.4.cmml">}</mo></mrow><annotation-xml encoding="MathML-Content" id="S3.p6.5.m5.4b"><set id="S3.p6.5.m5.4.4.4.cmml" xref="S3.p6.5.m5.4.4.3"><apply id="S3.p6.5.m5.2.2.1.1.cmml" xref="S3.p6.5.m5.2.2.1.1"><csymbol cd="ambiguous" id="S3.p6.5.m5.2.2.1.1.1.cmml" xref="S3.p6.5.m5.2.2.1.1">subscript</csymbol><ci id="S3.p6.5.m5.2.2.1.1.2.cmml" xref="S3.p6.5.m5.2.2.1.1.2">𝑐</ci><cn id="S3.p6.5.m5.2.2.1.1.3.cmml" type="integer" xref="S3.p6.5.m5.2.2.1.1.3">1</cn></apply><apply id="S3.p6.5.m5.3.3.2.2.cmml" xref="S3.p6.5.m5.3.3.2.2"><csymbol cd="ambiguous" id="S3.p6.5.m5.3.3.2.2.1.cmml" xref="S3.p6.5.m5.3.3.2.2">subscript</csymbol><ci id="S3.p6.5.m5.3.3.2.2.2.cmml" xref="S3.p6.5.m5.3.3.2.2.2">𝑐</ci><cn id="S3.p6.5.m5.3.3.2.2.3.cmml" type="integer" xref="S3.p6.5.m5.3.3.2.2.3">2</cn></apply><ci id="S3.p6.5.m5.1.1.cmml" xref="S3.p6.5.m5.1.1">…</ci><apply id="S3.p6.5.m5.4.4.3.3.cmml" xref="S3.p6.5.m5.4.4.3.3"><csymbol cd="ambiguous" id="S3.p6.5.m5.4.4.3.3.1.cmml" xref="S3.p6.5.m5.4.4.3.3">subscript</csymbol><ci id="S3.p6.5.m5.4.4.3.3.2.cmml" xref="S3.p6.5.m5.4.4.3.3.2">𝑐</ci><ci id="S3.p6.5.m5.4.4.3.3.3.cmml" xref="S3.p6.5.m5.4.4.3.3.3">𝑚</ci></apply></set></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.5.m5.4c">\{c_{1},c_{2},\ldots,c_{m}\}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.5.m5.4d">{ italic_c start_POSTSUBSCRIPT 1 end_POSTSUBSCRIPT , italic_c start_POSTSUBSCRIPT 2 end_POSTSUBSCRIPT , … , italic_c start_POSTSUBSCRIPT italic_m end_POSTSUBSCRIPT }</annotation></semantics></math>, also known as Layout; (ii) each panel <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.p6.6.m6.1"><semantics id="S3.p6.6.m6.1a"><msub id="S3.p6.6.m6.1.1" xref="S3.p6.6.m6.1.1.cmml"><mi id="S3.p6.6.m6.1.1.2" xref="S3.p6.6.m6.1.1.2.cmml">c</mi><mi id="S3.p6.6.m6.1.1.3" xref="S3.p6.6.m6.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.6.m6.1b"><apply id="S3.p6.6.m6.1.1.cmml" xref="S3.p6.6.m6.1.1"><csymbol cd="ambiguous" id="S3.p6.6.m6.1.1.1.cmml" xref="S3.p6.6.m6.1.1">subscript</csymbol><ci id="S3.p6.6.m6.1.1.2.cmml" xref="S3.p6.6.m6.1.1.2">𝑐</ci><ci id="S3.p6.6.m6.1.1.3.cmml" xref="S3.p6.6.m6.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.6.m6.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.6.m6.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> is a bounded area containing images and possibly text; panels are the primary unit for storytelling in comics; (iii) gutters: the space between panels, often used by artist to leave readers imagining the missing piece of the story, which helps in sequencing and timing; and (iv) text elements, including balloons (speech or thought), captions, and sound effects (onomatopoeias).
Each panel <math alttext="c_{i}" class="ltx_Math" display="inline" id="S3.p6.7.m7.1"><semantics id="S3.p6.7.m7.1a"><msub id="S3.p6.7.m7.1.1" xref="S3.p6.7.m7.1.1.cmml"><mi id="S3.p6.7.m7.1.1.2" xref="S3.p6.7.m7.1.1.2.cmml">c</mi><mi id="S3.p6.7.m7.1.1.3" xref="S3.p6.7.m7.1.1.3.cmml">i</mi></msub><annotation-xml encoding="MathML-Content" id="S3.p6.7.m7.1b"><apply id="S3.p6.7.m7.1.1.cmml" xref="S3.p6.7.m7.1.1"><csymbol cd="ambiguous" id="S3.p6.7.m7.1.1.1.cmml" xref="S3.p6.7.m7.1.1">subscript</csymbol><ci id="S3.p6.7.m7.1.1.2.cmml" xref="S3.p6.7.m7.1.1.2">𝑐</ci><ci id="S3.p6.7.m7.1.1.3.cmml" xref="S3.p6.7.m7.1.1.3">𝑖</ci></apply></annotation-xml><annotation encoding="application/x-tex" id="S3.p6.7.m7.1c">c_{i}</annotation><annotation encoding="application/x-llamapun" id="S3.p6.7.m7.1d">italic_c start_POSTSUBSCRIPT italic_i end_POSTSUBSCRIPT</annotation></semantics></math> contains characters (depicted through various visual styles and expressions), backgrounds (visual scenery behind the main characters or action), and foregrounds (objects and elements in front of the characters).</p>
</div>
</section>
<section class="ltx_section" id="S4">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">4 </span><span class="ltx_text ltx_font_smallcaps" id="S4.1.1">Tasks and Datasets</span>
</h2>
<div class="ltx_para" id="S4.p1">
<p class="ltx_p" id="S4.p1.1">This section summarizes the commonly used Comics datasets, as detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4.T2" title="TABLE II ‣ 4 Tasks and Datasets ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">II</span></a>. Some of these datasets are either intended for training or evaluation, depending on their sizes.</p>
</div>
<figure class="ltx_table" id="S4.T2">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T2.9.1.1" style="font-size:90%;">TABLE II</span>: </span><span class="ltx_text" id="S4.T2.10.2" style="font-size:90%;">Overview of Comic/Manga Datasets and Tasks, their information (availability, published year, source), and properties (languages, number of comic/manga books and pages). The table rows are repeated according to the supported Tasks. Accessibility is indicated with <span class="ltx_text" id="S4.T2.10.2.1" style="color:#FF0000;"> ✗</span> for no longer existing datasets, <span class="ltx_text" id="S4.T2.10.2.2" style="color:#808080;"> ✗</span> indicates existing but not accessible, and <span class="ltx_text" id="S4.T2.10.2.3" style="color:#00E000;"> ✓</span> means existing and accessible. The link <span class="ltx_text" id="S4.T2.10.2.4" style="color:#FF00FF;">[proj]</span> directs to the project websites, while <span class="ltx_text" id="S4.T2.10.2.5" style="color:#0000FF;">[data]</span> directs to dataset websites. For CoMix, <span class="ltx_text ltx_font_italic" id="S4.T2.10.2.6">mix</span> means that it inherits from a mixture of four datasets <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib57" title="">57</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib58" title="">58</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite>. For VLRC, <span class="ltx_text ltx_font_italic" id="S4.T2.10.2.7">Analysis</span> indicates annotations are about statistical analysis of panel size, color, layout, etc. However, the dataset does not provide images and annotations have been gathered directly on physical books.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_align_middle" id="S4.T2.11">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T2.11.1.1">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T2.11.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.1.1.1.1">Task</span></td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_tt" id="S4.T2.11.1.1.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.1.1.2.1">From</span></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_tt" id="S4.T2.11.1.1.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.1.1.3.1">Name</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.11.1.1.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.1.1.4.1">Year</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.11.1.1.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.1.1.5.1">Access.</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.11.1.1.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.1.1.6.1">Language</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.11.1.1.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.1.1.7.1">Origin</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_tt" id="S4.T2.11.1.1.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.1.1.8.1"># books</span></td>
<td class="ltx_td ltx_align_center ltx_border_tt" id="S4.T2.11.1.1.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.1.1.9.1"># pages</span></td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.2.2">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.2.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Image Classification</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.11.2.2.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.2.2.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.2.2.3.1">Sequencity <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib59" title="">59</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.2.2.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.2.2.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.2.2.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.2.2.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, JP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.2.2.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.2.2.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.11.2.2.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">140000</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.3.3">
<td class="ltx_td ltx_border_r" id="S4.T2.11.3.3.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.3.3.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.3.3.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.3.3.3.1">BAM! <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib60" title="">60</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.3.3.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.3.3.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.3.3.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.3.3.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.3.3.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.3.3.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.3.3.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2500000</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.4.4">
<td class="ltx_td ltx_border_r" id="S4.T2.11.4.4.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.4.4.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.4.4.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.4.4.3.1">Manga109 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib35" title="">35</a>]</cite></span> <a class="ltx_ref ltx_href" href="http://www.manga109.org/en/download.html" title="">[proj]</a><a class="ltx_ref ltx_href" href="http://www.manga109.org/en/download.html" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.4.4.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2018</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.4.4.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.4.4.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.4.4.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">JP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.4.4.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1970-2010</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.4.4.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">109</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.4.4.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">21142</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.5.5">
<td class="ltx_td ltx_border_r" id="S4.T2.11.5.5.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.5.5.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib8" title="">8</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.5.5.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.5.5.3.1">EmoRecCom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib62" title="">62</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://sites.google.com/view/emotion-recognition-for-comics" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://competitions.codalab.org/competitions/30954#participate-get_data" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.5.5.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2021</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.5.5.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.5.5.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.5.5.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.5.5.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1938-1954</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.5.5.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.5.5.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.6.6">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.6.6.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Object Detection</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.11.6.6.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.6.6.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.6.6.3.1">Fahad18 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib63" title="">63</a>]</cite></span> <a class="ltx_ref ltx_href" href="http://www.cat.uab.cat/Research/object-detection/" title="">[proj]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.6.6.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2012</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.6.6.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.6.6.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.6.6.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.6.6.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.6.6.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.11.6.6.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">586</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.7.7">
<td class="ltx_td ltx_border_r" id="S4.T2.11.7.7.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.7.7.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.7.7.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.7.7.3.1">eBDtheque <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib57" title="">57</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://ebdtheque.univ-lr.fr/" title="">[proj]</a><a class="ltx_ref ltx_href" href="https://ebdtheque.univ-lr.fr/registration/" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.7.7.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2013</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.7.7.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.7.7.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.7.7.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, FR, JP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.7.7.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1905-2012</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.7.7.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">25</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.7.7.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">100</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.8.8">
<td class="ltx_td ltx_border_r" id="S4.T2.11.8.8.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.8.8.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib57" title="">57</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.8.8.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.8.8.3.1">sun70 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib64" title="">64</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.8.8.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2013</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.8.8.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.8.8.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.8.8.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">FR</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.8.8.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.8.8.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.8.8.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">60</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.9.9">
<td class="ltx_td ltx_border_r" id="S4.T2.11.9.9.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.9.9.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.9.9.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.9.9.3.1">COMICS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib8" title="">8</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://github.com/miyyer/comics" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://obj.umiacs.umd.edu/comics/index.html" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.9.9.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.9.9.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.9.9.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.9.9.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.9.9.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1938-1954</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.9.9.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3948</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.9.9.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">198657</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.10.10">
<td class="ltx_td ltx_border_r" id="S4.T2.11.10.10.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.10.10.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.10.10.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.10.10.3.1">BAM! <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib60" title="">60</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.10.10.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.10.10.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.10.10.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.10.10.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.10.10.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.10.10.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.10.10.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2500000</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.11.11">
<td class="ltx_td ltx_border_r" id="S4.T2.11.11.11.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.11.11.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.11.11.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.11.11.3.1">JC2463 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib34" title="">34</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.11.11.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.11.11.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.11.11.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.11.11.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">JP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.11.11.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.11.11.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">14</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.11.11.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2463</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.12.12">
<td class="ltx_td ltx_border_r" id="S4.T2.11.12.12.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.12.12.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.12.12.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.12.12.3.1">AEC912 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib34" title="">34</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.12.12.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.12.12.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.12.12.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.12.12.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, FR</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.12.12.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.12.12.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.12.12.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">912</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.13.13">
<td class="ltx_td ltx_border_r" id="S4.T2.11.13.13.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.13.13.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.13.13.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.13.13.3.1">GCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib65" title="">65</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://groups.uni-paderborn.de/graphic-literature/gncorpus/corpus.php" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://groups.uni-paderborn.de/graphic-literature/gncorpus/download.php" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.13.13.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.13.13.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.13.13.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.13.13.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, JP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.13.13.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1978-2013</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.13.13.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">253</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.13.13.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">38000</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.14.14">
<td class="ltx_td ltx_border_r" id="S4.T2.11.14.14.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.14.14.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib59" title="">59</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.14.14.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.14.14.3.1">Sequencity612 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib59" title="">59</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.14.14.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.14.14.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.14.14.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.14.14.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, JP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.14.14.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.14.14.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.14.14.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">612</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.15.15">
<td class="ltx_td ltx_border_r" id="S4.T2.11.15.15.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.15.15.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib57" title="">57</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.15.15.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.15.15.3.1">SSGCI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib66" title="">66</a>]</cite></span> <a class="ltx_ref ltx_href" href="http://icpr2016-ssgci.univ-lr.fr/challenge/dataset-download/" title="">[proj]</a> <a class="ltx_ref ltx_href" href="http://icpr2016-ssgci.univ-lr.fr/challenge/dataset-download/" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.15.15.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2016</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.15.15.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.15.15.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.15.15.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, FR, JP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.15.15.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1905-2012</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.15.15.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.15.15.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">500</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.16.16">
<td class="ltx_td ltx_border_r" id="S4.T2.11.16.16.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.16.16.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.16.16.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.16.16.3.1">Comics3w <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib67" title="">67</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://philokey.github.io/sren.html" title="">[proj]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.16.16.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.16.16.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.16.16.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.16.16.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">JP, EN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.16.16.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.16.16.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">103</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.16.16.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">29845</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.17.17">
<td class="ltx_td ltx_border_r" id="S4.T2.11.17.17.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.17.17.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.17.17.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.17.17.3.1">comics2k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib68" title="">68</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://naoto0804.github.io/cross_domain_detection/" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://github.com/naoto0804/cross-domain-detection/tree/master/datasets" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.17.17.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2018</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.17.17.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.17.17.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.17.17.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.17.17.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.17.17.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.17.17.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.18.18">
<td class="ltx_td ltx_border_r" id="S4.T2.11.18.18.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.18.18.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.18.18.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.18.18.3.1">DCM772 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib58" title="">58</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://paperswithcode.com/dataset/dcm" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://git.univ-lr.fr/crigau02/dcm_dataset" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.18.18.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2018</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.18.18.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.18.18.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.18.18.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.18.18.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1938-1954</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.18.18.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">27</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.18.18.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">772</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.19.19">
<td class="ltx_td ltx_border_r" id="S4.T2.11.19.19.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.19.19.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.19.19.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.19.19.3.1">Manga109 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib35" title="">35</a>]</cite></span> <a class="ltx_ref ltx_href" href="http://www.manga109.org/en/download.html" title="">[proj]</a><a class="ltx_ref ltx_href" href="http://www.manga109.org/en/download.html" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.19.19.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2018</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.19.19.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.19.19.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.19.19.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">JP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.19.19.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1970-2010</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.19.19.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">109</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.19.19.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">21142</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.20.20">
<td class="ltx_td ltx_border_r" id="S4.T2.11.20.20.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.20.20.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.20.20.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.20.20.3.1">BCBId <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib69" title="">69</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://sites.google.com/view/banglacomicbookdataset" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://sites.google.com/view/banglacomicbookdataset/contacts?authuser=0" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.20.20.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.20.20.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.20.20.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.20.20.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">BN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.20.20.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.20.20.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">64</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.20.20.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3327</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.21.21">
<td class="ltx_td ltx_border_r" id="S4.T2.11.21.21.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.21.21.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.21.21.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.21.21.3.1">COO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib4" title="">4</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://github.com/ku21fan/COO-Comic-Onomatopoeia" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://github.com/manga109/public-annotations#comic-onomatxopoeia-coo" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.21.21.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.21.21.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.21.21.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.21.21.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">JP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.21.21.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1970-2010</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.21.21.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">109</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.21.21.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">10602</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.22.22">
<td class="ltx_td ltx_border_r" id="S4.T2.11.22.22.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.22.22.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib8" title="">8</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.22.22.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.22.22.3.1">COMICS-Text+ <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib70" title="">70</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://github.com/gsoykan/comics_text_plus" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://github.com/gsoykan/comics_text_plus#getting-started" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.22.22.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2022</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.22.22.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.22.22.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.22.22.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.22.22.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1938-1954</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.22.22.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3948</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.22.22.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">198657</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.23.23">
<td class="ltx_td ltx_border_r" id="S4.T2.11.23.23.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.23.23.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.23.23.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.23.23.3.1">PopManga <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://github.com/gsoykan/comics_text_plus" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://github.com/gsoykan/comics_text_plus#getting-started" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.23.23.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.23.23.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.23.23.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.23.23.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.23.23.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1990-2020</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.23.23.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">25</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.23.23.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1925</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.24.24">
<td class="ltx_td ltx_border_r" id="S4.T2.11.24.24.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.24.24.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">mix</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.24.24.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.24.24.3.1">CoMix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib42" title="">42</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://github.com/emanuelevivoli/CoMix-dataset" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://rrc.cvc.uab.es/?ch=31" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.24.24.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.24.24.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.24.24.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.24.24.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, FR</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.24.24.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1938-2023</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.24.24.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">100</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.24.24.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3800</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.25.25">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.25.25.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Re-Identification</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.11.25.25.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.25.25.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.25.25.3.1">Fahad18 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib63" title="">63</a>]</cite></span> <a class="ltx_ref ltx_href" href="http://www.cat.uab.cat/Research/object-detection/" title="">[proj]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.25.25.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2012</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.25.25.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.25.25.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.25.25.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.25.25.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.25.25.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.11.25.25.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">586</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.26.26">
<td class="ltx_td ltx_border_r" id="S4.T2.11.26.26.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.26.26.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.26.26.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.26.26.3.1">Ho42 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib71" title="">71</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.26.26.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2013</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.26.26.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.26.26.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.26.26.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.26.26.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.26.26.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.26.26.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">42</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.27.27">
<td class="ltx_td ltx_border_r" id="S4.T2.11.27.27.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.27.27.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.27.27.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.27.27.3.1">Manga109 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib35" title="">35</a>]</cite></span> <a class="ltx_ref ltx_href" href="http://www.manga109.org/en/download.html" title="">[proj]</a><a class="ltx_ref ltx_href" href="http://www.manga109.org/en/download.html" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.27.27.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2018</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.27.27.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.27.27.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.27.27.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">JP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.27.27.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1970-2010</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.27.27.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">109</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.27.27.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">21142</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.28.28">
<td class="ltx_td ltx_border_r" id="S4.T2.11.28.28.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.28.28.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.28.28.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.28.28.3.1">PopManga <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://github.com/gsoykan/comics_text_plus" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://github.com/gsoykan/comics_text_plus#getting-started" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.28.28.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.28.28.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.28.28.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.28.28.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.28.28.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1990-2020</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.28.28.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">25</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.28.28.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1925</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.29.29">
<td class="ltx_td ltx_border_r" id="S4.T2.11.29.29.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.29.29.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">mix</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.29.29.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.29.29.3.1">CoMix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib42" title="">42</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://github.com/emanuelevivoli/CoMix-dataset" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://rrc.cvc.uab.es/?ch=31" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.29.29.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.29.29.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.29.29.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.29.29.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, FR</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.29.29.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1938-2023</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.29.29.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">100</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.29.29.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3800</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.30.30">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.30.30.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Linking</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.11.30.30.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.30.30.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.30.30.3.1">eBDtheque <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib57" title="">57</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://ebdtheque.univ-lr.fr/" title="">[proj]</a><a class="ltx_ref ltx_href" href="https://ebdtheque.univ-lr.fr/registration/" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.30.30.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2013</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.30.30.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.30.30.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.30.30.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, FR, JP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.30.30.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1905-2012</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.30.30.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.11.30.30.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">100</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.31.31">
<td class="ltx_td ltx_border_r" id="S4.T2.11.31.31.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.31.31.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib57" title="">57</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.31.31.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.31.31.3.1">sun70 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib64" title="">64</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.31.31.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2013</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.31.31.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.31.31.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.31.31.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">FR</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.31.31.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.31.31.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">6</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.31.31.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">60</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.32.32">
<td class="ltx_td ltx_border_r" id="S4.T2.11.32.32.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.32.32.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.32.32.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.32.32.3.1">GCN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib65" title="">65</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://groups.uni-paderborn.de/graphic-literature/gncorpus/corpus.php" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://groups.uni-paderborn.de/graphic-literature/gncorpus/download.php" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.32.32.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2017</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.32.32.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.32.32.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.32.32.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, JP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.32.32.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1978-2013</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.32.32.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">253</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.32.32.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">38000</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.33.33">
<td class="ltx_td ltx_border_r" id="S4.T2.11.33.33.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.33.33.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.33.33.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.33.33.3.1">Manga109 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib35" title="">35</a>]</cite></span> <a class="ltx_ref ltx_href" href="http://www.manga109.org/en/download.html" title="">[proj]</a><a class="ltx_ref ltx_href" href="http://www.manga109.org/en/download.html" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.33.33.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2018</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.33.33.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.33.33.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.33.33.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">JP</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.33.33.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1970-2010</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.33.33.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">109</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.33.33.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">21142</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.34.34">
<td class="ltx_td ltx_border_r" id="S4.T2.11.34.34.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_border_r" id="S4.T2.11.34.34.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.34.34.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.34.34.3.1">PopManga <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://github.com/gsoykan/comics_text_plus" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://github.com/gsoykan/comics_text_plus#getting-started" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.34.34.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.34.34.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.34.34.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.34.34.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.34.34.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1990-2020</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.34.34.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">25</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.34.34.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1925</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.35.35">
<td class="ltx_td ltx_border_r" id="S4.T2.11.35.35.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.35.35.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">mix</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.35.35.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.35.35.3.1">CoMix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib42" title="">42</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://github.com/emanuelevivoli/CoMix-dataset" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://rrc.cvc.uab.es/?ch=31" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.35.35.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.35.35.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.35.35.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.35.35.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, FR</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.35.35.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1938-2023</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.35.35.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">100</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.35.35.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3800</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.36.36">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.36.36.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Segmentation</td>
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T2.11.36.36.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib59" title="">59</a>]</cite></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.36.36.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T2.11.36.36.3.1">Sequencity4k <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib38" title="">38</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.36.36.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.36.36.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.36.36.5.1" style="color:#FF0000;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.36.36.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, FR, JP</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.36.36.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.36.36.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.11.36.36.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">4479</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.37.37">
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.37.37.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Dialog generation</td>
<td class="ltx_td ltx_border_r ltx_border_t" id="S4.T2.11.37.37.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T2.11.37.37.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.37.37.3.1">PopManga <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://github.com/gsoykan/comics_text_plus" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://github.com/gsoykan/comics_text_plus#getting-started" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.37.37.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.37.37.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.37.37.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.37.37.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.37.37.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1990-2020</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T2.11.37.37.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">25</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T2.11.37.37.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1925</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.38.38">
<td class="ltx_td ltx_border_r" id="S4.T2.11.38.38.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T2.11.38.38.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">mix</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T2.11.38.38.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.38.38.3.1">CoMix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib42" title="">42</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://github.com/emanuelevivoli/CoMix-dataset" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://rrc.cvc.uab.es/?ch=31" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.38.38.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2024</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.38.38.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.38.38.5.1" style="color:#00E000;">✓</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.38.38.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">EN, FR</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.38.38.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1938-2023</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T2.11.38.38.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">100</td>
<td class="ltx_td ltx_align_center" id="S4.T2.11.38.38.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3800</td>
</tr>
<tr class="ltx_tr" id="S4.T2.11.39.39">
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.11.39.39.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_italic" id="S4.T2.11.39.39.1.1">Analysis</span></td>
<td class="ltx_td ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.11.39.39.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"></td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.11.39.39.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">
<span class="ltx_text ltx_font_bold" id="S4.T2.11.39.39.3.1">VLRC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib47" title="">47</a>]</cite></span> <a class="ltx_ref ltx_href" href="https://dataverse.nl/" title="">[proj]</a> <a class="ltx_ref ltx_href" href="https://dataverse.nl/dataset.xhtml?persistentId=doi:10.34894/LWMZ7G" style="color:#0000FF;" title="">[data]</a>
</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.11.39.39.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2023</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.11.39.39.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T2.11.39.39.5.1" style="color:#808080;">✗</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.11.39.39.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">JP, FR, EN, 6+</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.11.39.39.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1940-present</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r ltx_border_t" id="S4.T2.11.39.39.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">376</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_t" id="S4.T2.11.39.39.9" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">7773</td>
</tr>
</tbody>
</table>
</figure>
<section class="ltx_subsection" id="S4.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.1 </span><span class="ltx_text ltx_font_italic" id="S4.SS1.1.1">Annotations overview</span>
</h3>
<div class="ltx_para" id="S4.SS1.p1">
<p class="ltx_p" id="S4.SS1.p1.1">The available current comics datasets were introduced for different tasks. They vary in the year of release, the language and the origin. This is shown in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4.T2" title="TABLE II ‣ 4 Tasks and Datasets ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">II</span></a>. From the table, it is noticeable that the available datasets vary significantly in annotation types, making them cover different tasks.</p>
</div>
<div class="ltx_para" id="S4.SS1.p2">
<p class="ltx_p" id="S4.SS1.p2.1">eBDtheque <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib57" title="">57</a>]</cite> offers comprehensive annotations in high quality for Character, Balloon, and Panel detection, along with Text-Character association. This makes it usable for object detection and linking tasks but it comes with a drawback: it is composed of only 100 pages. In contrast, the COMICS dataset <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib8" title="">8</a>]</cite> focuses on a set of tasks called closure, based on automatic panel and text detection over a large number of pages. However, it was introduced with poor annotations. Only recently some works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib70" title="">70</a>]</cite> have improved OCR text detection on the COMICS dataset. In the same vein, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib72" title="">72</a>]</cite> shows that improving OCR accuracy (using Textract) could lead to marginal accuracy gain in closure tasks for the COMICS dataset. Both <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib72" title="">72</a>]</cite> released the novel OCR annotations to the research community.</p>
</div>
<div class="ltx_para" id="S4.SS1.p3">
<p class="ltx_p" id="S4.SS1.p3.1">It is to note also that some of the dataset with high-quality annotations and large number of pages are not accessible. For instance, BCBId <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib69" title="">69</a>]</cite>, though promising high-quality annotations, falls short in delivery. Similarly, VLRC <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib47" title="">47</a>]</cite>, while rich in annotations, lacks digital applicability due to its focus on physical books.</p>
</div>
<div class="ltx_para" id="S4.SS1.p4">
<p class="ltx_p" id="S4.SS1.p4.1">Manga109 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>]</cite> is the first to provide high-level annotations, covering more than 20k pages. The dataset has been improved over several iterations, further expanding the scope of the dataset by adding annotations for onomatopoeias <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib4" title="">4</a>]</cite> and dialog <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib73" title="">73</a>]</cite>. The actual state of Manga109 is the result of many years of annotation and improvement by subsequent developments. More recently, following the Manga109 wave, PopManga <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite> has been proposed as an English-language alternative, including annotations for recognition and linking, but on a smaller scale (2k images).</p>
</div>
<div class="ltx_para" id="S4.SS1.p5">
<p class="ltx_p" id="S4.SS1.p5.1">In summary, while datasets for Comics Understanding exist, there is a need for more approaches to standardize and unify their annotations.
The Comics Dataset Framework <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib2" title="">2</a>]</cite> proposed a unification of detection annotations as well as a standardization of metrics and settings used to train and measure models. The work was subsequently extended in CoMix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib42" title="">42</a>]</cite>, where mainly English-language comics (and manga) were selected and the annotations extended to the task of single-page dialogue transcription. In this work, performances of several state-of-the-art models, including Magi <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite>, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib14" title="">14</a>]</cite>, DINOv2 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib39" title="">39</a>]</cite>, and GPT4 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib74" title="">74</a>]</cite>, were measured, for the first time.</p>
</div>
<div class="ltx_para" id="S4.SS1.p6">
<p class="ltx_p" id="S4.SS1.p6.1">However, as highlighted in the first column of Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4.T2" title="TABLE II ‣ 4 Tasks and Datasets ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">II</span></a>, the complexity of annotations and tasks is limited. Object detection (panel, characters, texts), text-to-character linking, and re-identifications are integral tasks to understanding the narrative and dialogue in comics. These tasks, along with more recent developments, will be examined in the context of our taxonomy in the following Section.</p>
</div>
</section>
<section class="ltx_subsection" id="S4.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.2 </span><span class="ltx_text ltx_font_italic" id="S4.SS2.1.1">Datasets overview</span>
</h3>
<div class="ltx_para" id="S4.SS2.p1">
<p class="ltx_p" id="S4.SS2.p1.1">The domain of Comics Understanding encompasses a range of datasets focused on processing and examining comic media. A review of various datasets, as detailed in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4.T2" title="TABLE II ‣ 4 Tasks and Datasets ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">II</span></a>, reveals that several are no longer accessible (marked with <span class="ltx_text" id="S4.SS2.p1.1.1" style="color:#FF0000;"> ✗</span>) or have restricted access (<span class="ltx_text" id="S4.SS2.p1.1.2" style="color:#808080;"> ✗</span>). Those available (<span class="ltx_text" id="S4.SS2.p1.1.3" style="color:#00E000;"> ✓</span>) often require approval from respective research groups in order to obtain them. This is a common practice in the field of “sharing copyrighted materials” as in fact they can be used only for research purposes in the majority of cases. If the copyright does not allow for redistribution, it is a common practice to release annotations and a script to download and structure the dataset accordingly (as for <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite>).
As we can see from Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4.T2" title="TABLE II ‣ 4 Tasks and Datasets ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">II</span></a>, many datasets share common origins (column <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.4">“From”</span>).
Notably, these datasets vary significantly in terms of style, quality, and size. The common languages of these datasets are English, French, and Japanese and none of the available data seems to be collected from recent books (after 2010), probably due to copyright limits. We found the above limitation to be one of the main issues in the comic domain: (almost) every work proposes its own (sometimes private) dataset on which results are drawn with little comparison, making these works hard to replicate. Datasets are limited in size and the model’s <span class="ltx_text ltx_font_italic" id="S4.SS2.p1.1.5">code and weights</span> are rarely shared. Luckily, this has started changing recently <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib2" title="">2</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib42" title="">42</a>]</cite>.</p>
</div>
<figure class="ltx_table" id="S4.T3">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="S4.T3.2.1.1" style="font-size:90%;">TABLE III</span>: </span><span class="ltx_text" id="S4.T3.3.2" style="font-size:90%;">Number of single instance annotation (declared) in papers. The “Obj. cls” stands for Object-based classification, and refers to the number of classes available if one wants to classify the detected instances in fine-grained classes.</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="S4.T3.4">
<thead class="ltx_thead">
<tr class="ltx_tr" id="S4.T3.4.1.1">
<th class="ltx_td ltx_align_right ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.4.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.1.1">Mode</span></th>
<th class="ltx_td ltx_align_left ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.4.1.1.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.2.1">Name</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.4.1.1.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.3.1">Panel</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.4.1.1.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.4.1">Char.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.4.1.1.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.5.1">Face</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.4.1.1.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.6.1">Textbox</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_tt" id="S4.T3.4.1.1.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.7.1">Onomat.</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="S4.T3.4.1.1.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.1.1.8.1">Obj. cls</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="S4.T3.4.2.1">
<td class="ltx_td ltx_align_right ltx_border_r ltx_border_t" id="S4.T3.4.2.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Color</td>
<td class="ltx_td ltx_align_left ltx_border_r ltx_border_t" id="S4.T3.4.2.1.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.2.1.2.1">Fahad18 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib63" title="">63</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.2.1.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.2.1.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">18</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.2.1.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.2.1.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="S4.T3.4.2.1.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_t" id="S4.T3.4.2.1.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">18</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.3.2" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.4.3.2.1" style="background-color:#F3F3F3;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.3.2.1.1" style="background-color:#F3F3F3;">Color</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.3.2.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.3.2.2.1" style="background-color:#F3F3F3;">Ho42 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib71" title="">71</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.3.2.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.3.2.3.1" style="background-color:#F3F3F3;">200</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.3.2.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.3.2.4.1" style="background-color:#F3F3F3;">100</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.3.2.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.3.2.5.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.3.2.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.3.2.6.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.3.2.7" style="background-color:#F3F3F3;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.3.2.7.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.3.2.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.3.2.8.1" style="background-color:#F3F3F3;">-</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.4.3">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.4.4.3.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Color</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.4.3.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.4.3.2.1">eBDtheque <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib57" title="">57</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.4.3.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">850</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.4.3.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1550</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.4.3.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.4.3.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1092</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.4.3.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.4.3.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">5</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.5.4" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.4.5.4.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.5.4.1.1" style="background-color:#F3F3F3;">Color</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.5.4.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.5.4.2.1" style="background-color:#F3F3F3;">SSGCI <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib66" title="">66</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.5.4.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.5.4.3.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.5.4.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.5.4.4.1" style="background-color:#F3F3F3;">50</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.5.4.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.5.4.5.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.5.4.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.5.4.6.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.5.4.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.5.4.7.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.5.4.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.5.4.8.1" style="background-color:#F3F3F3;">1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.6.5">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.4.6.5.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Color</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.6.5.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.6.5.2.1">COMICS <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib8" title="">8</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib70" title="">70</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.6.5.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1229664</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.6.5.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.6.5.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.6.5.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2498657</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.6.5.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.6.5.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.7.6" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.4.7.6.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.7.6.1.1" style="background-color:#F3F3F3;">BW</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.7.6.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.7.6.2.1" style="background-color:#F3F3F3;">Comics3w <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib75" title="">75</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.7.6.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.7.6.3.1" style="background-color:#F3F3F3;">159529</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.7.6.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.7.6.4.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.7.6.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.7.6.5.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.7.6.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.7.6.6.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.7.6.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.7.6.7.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.7.6.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.7.6.8.1" style="background-color:#F3F3F3;">1</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.8.7">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.4.8.7.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">BW</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.8.7.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.8.7.2.1">JC2463 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib34" title="">34</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.8.7.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.8.7.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.8.7.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">15801</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.8.7.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.8.7.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.8.7.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.9.8" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.4.9.8.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.9.8.1.1" style="background-color:#F3F3F3;">Color</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.9.8.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.9.8.2.1" style="background-color:#F3F3F3;">AEC912 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib34" title="">34</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.9.8.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.9.8.3.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.9.8.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.9.8.4.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.9.8.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.9.8.5.1" style="background-color:#F3F3F3;">8184</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.9.8.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.9.8.6.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.9.8.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.9.8.7.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.9.8.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.9.8.8.1" style="background-color:#F3F3F3;">2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.10.9">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.4.10.9.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Color</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.10.9.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.10.9.2.1">DCM772 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib58" title="">58</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.10.9.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">4470</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.10.9.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">10757</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.10.9.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">5438</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.10.9.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.10.9.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.10.9.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">4</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.11.10" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.4.11.10.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.11.10.1.1" style="background-color:#F3F3F3;">BW</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.11.10.2" style="background-color:#F3F3F3;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.11.10.2.1" style="background-color:#F3F3F3;">Manga109 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib35" title="">35</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib4" title="">4</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.11.10.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.11.10.3.1" style="background-color:#F3F3F3;">103900</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.11.10.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.11.10.4.1" style="background-color:#F3F3F3;">157152</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.11.10.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.11.10.5.1" style="background-color:#F3F3F3;">118715</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.11.10.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.11.10.6.1" style="background-color:#F3F3F3;">147918</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.11.10.7" style="background-color:#F3F3F3;padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.11.10.7.1" style="background-color:#F3F3F3;">61465</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.11.10.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.11.10.8.1" style="background-color:#F3F3F3;">4</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.12.11">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.4.12.11.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Color</td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.12.11.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.12.11.2.1">EmoRecCom <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib62" title="">62</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.12.11.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">10199</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.12.11.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.12.11.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.12.11.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.12.11.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.12.11.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">8</td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.13.12" style="background-color:#F3F3F3;">
<td class="ltx_td ltx_align_right ltx_border_r" id="S4.T3.4.13.12.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.13.12.1.1" style="background-color:#F3F3F3;">BW</span></td>
<td class="ltx_td ltx_align_left ltx_border_r" id="S4.T3.4.13.12.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.13.12.2.1" style="background-color:#F3F3F3;">PopManga <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.13.12.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.13.12.3.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.13.12.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.13.12.4.1" style="background-color:#F3F3F3;">18783</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.13.12.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.13.12.5.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.13.12.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.13.12.6.1" style="background-color:#F3F3F3;">20843</span></td>
<td class="ltx_td ltx_align_center ltx_border_r" id="S4.T3.4.13.12.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.13.12.7.1" style="background-color:#F3F3F3;">-</span></td>
<td class="ltx_td ltx_align_center" id="S4.T3.4.13.12.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text" id="S4.T3.4.13.12.8.1" style="background-color:#F3F3F3;">2</span></td>
</tr>
<tr class="ltx_tr" id="S4.T3.4.14.13">
<td class="ltx_td ltx_align_right ltx_border_bb ltx_border_r" id="S4.T3.4.14.13.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">mix</td>
<td class="ltx_td ltx_align_left ltx_border_bb ltx_border_r" id="S4.T3.4.14.13.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="S4.T3.4.14.13.2.1">CoMix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib42" title="">42</a>]</cite></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.4.14.13.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">22176</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.4.14.13.4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">48903</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.4.14.13.5" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">32625</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.4.14.13.6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">37923</td>
<td class="ltx_td ltx_align_center ltx_border_bb ltx_border_r" id="S4.T3.4.14.13.7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">-</td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="S4.T3.4.14.13.8" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">4</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="S4.SS2.p2">
<p class="ltx_p" id="S4.SS2.p2.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4.T3" title="TABLE III ‣ 4.2 Datasets overview ‣ 4 Tasks and Datasets ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">III</span></a> we report, for every dataset, the declared numbers of annotations available. On the right-most column, we report the number of classes annotated, which correspond to different class objects for the detection, or the number of classes for classification annotations. For instance, in <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.1">Fahad18</span> there are 18 characters, thus the number of classes is 18. However, in <span class="ltx_text ltx_font_italic" id="S4.SS2.p2.1.2">eBDtheque</span>, there are 5 classes, but only panel, character, and face annotations. This is because the characters are separated into three sub-categories: human-like, animal-like, and object-like. Here, we describe the datasets chronologically.</p>
</div>
<div class="ltx_para" id="S4.SS2.p3">
<p class="ltx_p" id="S4.SS2.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p3.1.1">eBDtheque:</span> The eBDtheque<span class="ltx_note ltx_role_footnote" id="footnote1"><sup class="ltx_note_mark">1</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">1</sup><span class="ltx_tag ltx_tag_note">1</span><a class="ltx_ref ltx_href" href="http://ebdtheque.univ-lr.fr/registration" title="">http://ebdtheque.univ-lr.fr/registration</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib57" title="">57</a>]</cite> is a collection of 100 pages over 20 books, selected from J-comi<span class="ltx_note ltx_role_footnote" id="footnote2"><sup class="ltx_note_mark">2</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">2</sup><span class="ltx_tag ltx_tag_note">2</span>previously <a class="ltx_ref ltx_href" href="http://www.j-comi.jp" title="">http://www.j-comi.jp</a>, now no longer available.</span></span></span>, mainly in the French language (with also English and Japanese comics). <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib76" title="">76</a>]</cite> led a huge effort in labeling 850 panels, 1092 balloons, 1550 characters, and 4691 text lines, provided by domain experts. For balloons, they provide instance segmentation and classification ( speech, thought, narration, illustrative).</p>
</div>
<div class="ltx_para" id="S4.SS2.p4">
<p class="ltx_p" id="S4.SS2.p4.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p4.1.1">Manga109:</span> The Manga109<span class="ltx_note ltx_role_footnote" id="footnote3"><sup class="ltx_note_mark">3</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">3</sup><span class="ltx_tag ltx_tag_note">3</span><a class="ltx_ref ltx_href" href="http://www.manga109.org/index_en.php" title="">http://www.manga109.org/index_en.php</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib61" title="">61</a>]</cite> dataset contains 109 manga volumes from 93 different authors. Authors successfully obtained the copyrighted material from manga drawers with a “restrictive term-of-use” which implies redistribution only with the scope of research and the possibility to use them in publication with proper referencing the author. These mangas were published between the 1970s and 2010s and are categorized into 12 different genres such as fantasy, humor, sports, etc. This dataset has been further extended by COO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib4" title="">4</a>]</cite> which added onomatopoeias polygons annotations and links among truncated onomatopoeias. In fact, on average there are 5.8 onomatopoeias per page in Manga109, while COMICS has much fewer (0.09). Another set of annotations comes from Manga-Dialog <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib73" title="">73</a>]</cite> which added a text-character link for every speaking text box.</p>
</div>
<div class="ltx_para" id="S4.SS2.p5">
<p class="ltx_p" id="S4.SS2.p5.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p5.1.1">COMICS:</span> The COMICS<span class="ltx_note ltx_role_footnote" id="footnote4"><sup class="ltx_note_mark">4</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">4</sup><span class="ltx_tag ltx_tag_note">4</span><a class="ltx_ref ltx_href" href="https://obj.umiacs.umd.edu/comics/index.html" title="">https://obj.umiacs.umd.edu/comics/index.html</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib8" title="">8</a>]</cite> contains 1,229,664 panels paired with automatic textbox transcriptions using an old-OCR system. The 3,948 books are gathered from the Digital Comics Museum (American golden-age comics books) published between 1938 and 1954. The dataset includes ground truth (manually annotated) labeled data such as the rectangular bounding boxes of panels on 500 pages and 1,500 textboxes. Subsequent works have expanded original annotations with better OCR transcriptions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib70" title="">70</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib72" title="">72</a>]</cite>. In particular, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib70" title="">70</a>]</cite> developed a pipeline for OCR processing and labeling of comic books from COMICS, while <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib72" title="">72</a>]</cite> extracted all OCR transcripts from COMIC using new-generation OCR textract<span class="ltx_note ltx_role_footnote" id="footnote5"><sup class="ltx_note_mark">5</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">5</sup><span class="ltx_tag ltx_tag_note">5</span>Used through API at <a class="ltx_ref ltx_href" href="https://docs.aws.amazon.com/textract" title="">https://docs.aws.amazon.com/textract/</a></span></span></span>. Lastly, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib77" title="">77</a>]</cite> includes strips of dialog prediction based on “persona” information.</p>
</div>
<div class="ltx_para" id="S4.SS2.p6">
<p class="ltx_p" id="S4.SS2.p6.1"><span class="ltx_text ltx_font_bold" id="S4.SS2.p6.1.1">DCM772:</span> The DCM772<span class="ltx_note ltx_role_footnote" id="footnote6"><sup class="ltx_note_mark">6</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">6</sup><span class="ltx_tag ltx_tag_note">6</span><a class="ltx_ref ltx_href" href="https://git.univ-lr.fr/crigau02/dcm_dataset" title="">https://git.univ-lr.fr/crigau02/dcm_dataset</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib58" title="">58</a>]</cite> contains 772 (pages) images from 27 golden-age comic books from the same COMICS source. Annotations, however, are more precise as they contain panels, the associated characters bounding boxes, and also face boxes.</p>
</div>
<figure class="ltx_figure" id="S4.F4"><img alt="Refer to caption" class="ltx_graphics ltx_centering ltx_img_landscape" height="137" id="S4.F4.g1" src="x3.png" width="349"/>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S4.F4.2.1.1" style="font-size:90%;">Figure 4</span>: </span><span class="ltx_text" id="S4.F4.3.2" style="font-size:90%;"> Example of different comic datasets, from Black-and-White to Color, from comics-style to Manga-style.</span></figcaption>
</figure>
</section>
<section class="ltx_subsection" id="S4.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">4.3 </span><span class="ltx_text ltx_font_italic" id="S4.SS3.1.1">Datasets for Evaluation</span>
</h3>
<div class="ltx_para" id="S4.SS3.p1">
<p class="ltx_p" id="S4.SS3.p1.1">Across the datasets presented in Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S4.T2" title="TABLE II ‣ 4 Tasks and Datasets ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">II</span></a>, two recent datasets are provided as evaluation sets. In particular, PopManga proposes two splits (validation and test) while CoMix enhances annotations for various existing datasets among which also PopManga.</p>
</div>
<div class="ltx_para" id="S4.SS3.p2">
<p class="ltx_p" id="S4.SS3.p2.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p2.1.1">PopManga:</span> The PopManga<span class="ltx_note ltx_role_footnote" id="footnote7"><sup class="ltx_note_mark">7</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">7</sup><span class="ltx_tag ltx_tag_note">7</span><a class="ltx_ref ltx_href" href="https://github.com/ragavsachdeva/Magi/tree/main/datasets" title="">https://github.com/ragavsachdeva/Magi</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite> dataset contains English manga titles from the most popular mangas. The dataset contains two test splits: test-seen with 1,136 pages and test-unseen with 789 pages, obtained from 15 and 10 books respectively. The test sets have been annotated with Text (both textboxes and onomatopoeias annotated as text), and Characters.</p>
</div>
<div class="ltx_para" id="S4.SS3.p3">
<p class="ltx_p" id="S4.SS3.p3.1"><span class="ltx_text ltx_font_bold" id="S4.SS3.p3.1.1">CoMix:</span> The CoMix<span class="ltx_note ltx_role_footnote" id="footnote8"><sup class="ltx_note_mark">8</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">8</sup><span class="ltx_tag ltx_tag_note">8</span><a class="ltx_ref ltx_href" href="https://github.com/emanuelevivoli/comix-dataset" title="">https://github.com/emanuelevivoli/comix-dataset</a></span></span></span> <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib42" title="">42</a>]</cite> dataset contains English and French comics and manga titles from the most popular datasets: eBDtheque, COMICS, DCM, and PopManga. The dataset contains two splits: validation (available with annotations) and the held-out test split (available only through the evaluation server<span class="ltx_note ltx_role_footnote" id="footnote9"><sup class="ltx_note_mark">9</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">9</sup><span class="ltx_tag ltx_tag_note">9</span>Accessible at <a class="ltx_ref ltx_href" href="https://rrc.cvc.uab.es/?ch=31" title="">Robust Reading Competition</a> website</span></span></span>). It contains 3.8k images across 100 books, and annotations for multi-task, from detection and linking to dialog generation.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S5">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">5 </span><span class="ltx_text ltx_font_smallcaps" id="S5.1.1">Taxonomy: <span class="ltx_text ltx_font_italic" id="S5.1.1.1">LoCU</span></span>
</h2>
<div class="ltx_para" id="S5.p1">
<p class="ltx_p" id="S5.p1.1">Developing a comprehensive taxonomy for comics within the Vision-Language (VL) domain requires a careful overview of existing work in VL surveys and tasks. Our approach synthesizes insights from various benchmarks and taxonomies, forming the foundation for our unique classification – the <span class="ltx_text ltx_font_italic" id="S5.p1.1.1">Layers of Comics Understanding</span> (<span class="ltx_text ltx_font_italic" id="S5.p1.1.2">LoCU</span>).</p>
</div>
<div class="ltx_para" id="S5.p2">
<p class="ltx_p" id="S5.p2.1"><span class="ltx_text ltx_font_bold" id="S5.p2.1.1">Relevant Taxonomies.</span> Initial inspiration stems from the breadth and depth of multimodal questions covered in recent benchmarks like MMMU <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib78" title="">78</a>]</cite>, highlighting the necessity for a taxonomy that accounts for modal complexity and task-specific intricacies in comics. This need is further echoed in the domain-specific taxonomy proposed for video understanding in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib79" title="">79</a>]</cite>, particularly their dimensional analysis approach, which informs our consideration of spatial and temporal domains in comics.
Shifting focus to VL multimodal evaluation, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib80" title="">80</a>]</cite> reframes VL tasks as Visual Question Answering (VQA), emphasizing a range of abilities from grounding to numerical calculation. While this framework offers valuable categorizations, it lacks a comprehensive structure for VL tasks, particularly those pertinent to comics. Similarly, the multimodal challenges outlined in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib81" title="">81</a>]</cite> offer insights into the representation, translation, alignment, fusion, and co-learning challenges in multimodal learning. These challenges guide our approach to defining taxonomic classes and subclasses within the VL domain, though with a more task-centric focus.
Drawing further on <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib82" title="">82</a>]</cite>, we observed the classification of VL tasks into grounding, retrieval, understanding, and generation, with specific adjustments and additions to accommodate comic-specific tasks. Our taxonomy expands on this structure, adding categories such as tagging, augmentation, analysis, segmentation, modification, and synthesis, addressing the wider range of tasks observed in comic studies.</p>
</div>
<div class="ltx_para" id="S5.p3">
<p class="ltx_p" id="S5.p3.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S2.T1" title="TABLE I ‣ 2.1.3 Modern Foundational models ‣ 2.1 The Comics Research Epochs ‣ 2 Background ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">I</span></a>, we present 31 Vision-Language multimodal tasks, categorized according to the modalities involved, forming the backbone of the <span class="ltx_text ltx_font_italic" id="S5.p3.1.1">LoCU</span> framework. This taxonomy, spanning 10 distinct task groups across five layers, aims to capture the multifaceted nature of Comics Understanding. Layer 0, representing fundamental image processing and viewing capabilities, sets the stage for increasingly complex interactions in the subsequent layers. More details are provided in Appendix <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#Ax2" title="B. Taxonomy ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_title">B. Taxonomy</span></a>.
This structured approach allows for a comprehensive and nuanced understanding of the range and complexity of tasks within the comics domain, laying a foundation for future exploration and innovation in comic analysis and understanding.</p>
</div>
</section>
<section class="ltx_section" id="S6">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">6 </span><span class="ltx_text ltx_font_smallcaps" id="S6.1.1">Layer 1: Tagging and Augmentation</span>
</h2>
<div class="ltx_para" id="S6.p1">
<p class="ltx_p" id="S6.p1.1">The first layer comprises tasks with unimodal input (single image or sequence of images) and unimodal output (Images or Text): <span class="ltx_text ltx_font_italic" id="S6.p1.1.1">Tagging</span> and <span class="ltx_text ltx_font_italic" id="S6.p1.1.2">Augmentation</span>.</p>
</div>
<section class="ltx_subsection" id="S6.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.1 </span><span class="ltx_text ltx_font_italic" id="S6.SS1.2.1">Tagging</span>
</h3>
<div class="ltx_para" id="S6.SS1.p1">
<p class="ltx_p" id="S6.SS1.p1.1">Tagging tasks predominantly involve classification outputs, albeit with varying input types.</p>
</div>
<figure class="ltx_figure" id="S6.SS1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="181" id="S6.SS1.1.g1" src="x4.png" width="348"/>
</figure>
<section class="ltx_subsubsection" id="S6.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.1 </span>Image Classification</h4>
<div class="ltx_para" id="S6.SS1.SSS1.p1">
<p class="ltx_p" id="S6.SS1.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS1.p1.1.1">Definition:</span> The task of <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS1.p1.1.2">Image Classification</span> requires classifying an image into one of several predefined categories. In the comics domain, the task can be applied at panel or page level. In both cases, the classes vary from artist names, comic/manga styles or the image type (whether it’s an ads or front page or story page).</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p2">
<p class="ltx_p" id="S6.SS1.SSS1.p2.1">Early efforts in this domain, such as those by Chu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib31" title="">31</a>]</cite>, focused on classifying manga panels by artistic style using Support Vector Machines (SVM) and manga-specific feature vectors. Their feature vectors are built on edge-detected lines creating a 20-dimensional vector from elements such as angle, orientation, the density of segments, etc.
Hiroe et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib83" title="">83</a>]</cite> employed a novel method using SVMs to classify comic books based on the frequency of exclamation marks, demonstrating marks-per-page histogram as an innovative approach to content approximation.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p3">
<p class="ltx_p" id="S6.SS1.SSS1.p3.1">Daiku et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib84" title="">84</a>]</cite> expanded the scope to page-level classification within manga stories using Convolutional Neural Networks (CNN), distinct from Page Stream Segmentation (discussed in Subsection <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S6.SS1.SSS4" title="6.1.4 Page Stream Segmentation ‣ 6.1 Tagging ‣ 6 Layer 1: Tagging and Augmentation ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">6.1.4</span></a>).
Jiang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib85" title="">85</a>]</cite> introduced the Consensus Style Centralizing Auto-Encoder (CSCAE) for style classification, particularly distinguishing between Shonen (for boy) and Shojo (for girl) manga styles using robust style feature representations. They used low-rank and group sparsity constraints for consensus for the classification of manga character faces. Lastly, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib86" title="">86</a>]</cite> Terauchi et al. presented an approach for classifying manga based on the author’s unique style, employing a Variational Autoencoder (VAE) for this purpose. They also proposed a “four-scene comics story dataset” for this classification task into Mow, Seinen, and Shonen <span class="ltx_text ltx_font_italic" id="S6.SS1.SSS1.p3.1.1">touches</span> (styles).</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS1.p4">
<p class="ltx_p" id="S6.SS1.SSS1.p4.1">In a more recent work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib87" title="">87</a>]</cite>, the authors tackled the challenge of (multi-)genre classification at the story level in Manga. They introduced a Panel-Page-aware Comic genre classification model, which takes page sequences as input and produces class-wise probabilities, relying on the attention mechanism to jointly attend page features and panel boxes, then pooled into a transformer encoder. The transformer classification token is merged with a Graph Convolution Network (GCN) over the labels graph, which predicts the probability distribution of the labels for the comic stories.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.2 </span>Emotion Classification</h4>
<div class="ltx_para" id="S6.SS1.SSS2.p1">
<p class="ltx_p" id="S6.SS1.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS2.p1.1.1">Definition:</span> Emotion Classification in comics involves categorizing a single panel or an entire page (looking both at the image and the text) into predefined emotional categories, enhancing the understanding of the narrative’s emotional context.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p2">
<p class="ltx_p" id="S6.SS1.SSS2.p2.1">A seminal study by Tanaka et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib88" title="">88</a>]</cite> explored the interplay between speech balloon shapes and emotions in comics. They analyzed the relationship between balloon styles and the emotions expressed within them, using the Manga109 dataset as a foundation for their study.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p3">
<p class="ltx_p" id="S6.SS1.SSS2.p3.1">The EmoReCon challenge at ICDAR 2021 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib62" title="">62</a>]</cite> marked a significant advancement in this field. Participants were tasked with classifying 8 emotions from comic panels, using a dataset specifically designed for this purpose <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib62" title="">62</a>]</cite>. Notably, one of the winning teams employed a three-level fusion approach (early, mid, and late), integrating EfficientNetB3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib89" title="">89</a>]</cite> and RoBERTa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib90" title="">90</a>]</cite> for visual and textual embeddings, respectively. This mid-fusion strategy outperformed other architectural configurations, signifying the potential of multimodal fusion. The second winning team proposed an early fusion approach with ResNet50 image features and OCR-extracted text as input for a transformer encoder BERT-style. In this case, the prediction is drawn from a weighted average over the layers’ cls tokens, with comparable results with the previous method.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p4">
<p class="ltx_p" id="S6.SS1.SSS2.p4.1">Not only have the emotions of comic characters been analyzed, but in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib91" title="">91</a>]</cite> authors studied readers’ emotions through physiological signals. Through the relationship between a change in the reader’s physiological signal and the page of the reading, authors classify the manga across three classes: comedy, romance, and horror.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS2.p5">
<p class="ltx_p" id="S6.SS1.SSS2.p5.1">A recent work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib92" title="">92</a>]</cite> introduced two noteworthy datasets derived from Manga109: MangaAD+ and MangaEmo+. While MangaAD+ is a curated collection of 600 pages featuring unannotated elements such as shop names and ads - in Manga109 only the balloon text and onomatopoeias are annotated -, MangaEmo+ extends this dataset by annotating each page with a range of 8 emotions for multi-label classification. They employed a modified version of Faster R-CNN, trained on augmented text data, for atypical text detection. The subsequent emotion classification task was approached using EfficientNet, which extracted visual features combined with the above custom text detection model. However, neither of the two datasets has been made publicly available.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.3 </span>Action Detection</h4>
<div class="ltx_para" id="S6.SS1.SSS3.p1">
<p class="ltx_p" id="S6.SS1.SSS3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS3.p1.1.1">Definition:</span> Action Detection in comics is characterized by the analysis of sequences of consecutive panels. This task involves processing multiple images to classify the depicted sequence into one of several predefined action categories.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS3.p2">
<p class="ltx_p" id="S6.SS1.SSS3.p2.1">Unique to Action Detection is its multimodal input nature, where each panel in the sequence contributes to a holistic understanding of the action. Unlike single image analysis, this task requires the model to interpret a series of images, each adding context and continuity to the narrative. Despite being popular for Videos <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib93" title="">93</a>]</cite>, this task has not yet been explored in comics apart from image-cloze task <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib8" title="">8</a>]</cite> where the model is challenged to select the correct next panel from two options. This task, while elementary, underscores the potential for more sophisticated Action Detection in comics, a domain where sequential imagery plays a pivotal role.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.1.4 </span>Page Stream Segmentation</h4>
<div class="ltx_para" id="S6.SS1.SSS4.p1">
<p class="ltx_p" id="S6.SS1.SSS4.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS1.SSS4.p1.1.1">Definition:</span> Page Stream Segmentation (PSS) in comics entails segmenting a comic book into different stories or sections. In its simplistic version, the task can be seen as categorizing each page with a tag among cover, regular page, ads, or last page. In its more advanced form, it includes extracting salient information from the stories such as title, authors, drawer, price, characters, and synopsis.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS4.p2">
<p class="ltx_p" id="S6.SS1.SSS4.p2.1">The concept of PSS, while not new in document analysis, has seen various methodologies employed over time. Traditional approaches, such as those utilizing Support Vector Machines (SVM) for single page classification <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib94" title="">94</a>]</cite>, have evolved with the advent of Convolutional Neural Networks (CNNs) <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib95" title="">95</a>]</cite>, and more recently, attention-based multimodal fusion techniques <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib96" title="">96</a>]</cite>. These methods have been pivotal in processing and understanding page layouts and structures in various document formats.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS4.p3">
<p class="ltx_p" id="S6.SS1.SSS4.p3.1">Despite its longstanding presence in document analysis, PSS remains largely uncharted in the comics domain. Comics present a unique challenge for PSS due to their complex structure; a single volume can interweave multiple stories, interspersed with advertisements and narrative text. This intricacy demands a nuanced approach to segment and understand the diverse content within comic books.</p>
</div>
<div class="ltx_para" id="S6.SS1.SSS4.p4">
<p class="ltx_p" id="S6.SS1.SSS4.p4.1">A potential advancement in PSS for comics involves not just segmenting the pages but also tagging these segments with metadata like story titles, authors, and synopses. This represents a promising field of study, extending traditional image classification into a more intricate, story-centric context.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.2 </span><span class="ltx_text ltx_font_italic" id="S6.SS2.2.1">Augmentation</span>
</h3>
<div class="ltx_para" id="S6.SS2.p1">
<p class="ltx_p" id="S6.SS2.p1.1">Augmentation tasks, in the context of comics, involve image manipulation techniques aimed at enhancing or altering the visual presentation. The primary tasks identified under this category are Super-Resolution (SR), Style-Transfer (ST), Vectorization, and Depth Estimation. Each of these tasks revolves around processing a single image input to produce an enhanced or stylistically altered output.</p>
</div>
<figure class="ltx_figure" id="S6.SS2.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="185" id="S6.SS2.1.g1" src="x5.png" width="348"/>
</figure>
<section class="ltx_subsubsection" id="S6.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.1 </span>Super-Resolution</h4>
<div class="ltx_para" id="S6.SS2.SSS1.p1">
<p class="ltx_p" id="S6.SS2.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS1.p1.1.1">Definition:</span> The Super-Resolution task in comics primarily targets the enhancement of image quality, especially for images that are compressed, photograph-based, or scanned.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS1.p2">
<p class="ltx_p" id="S6.SS2.SSS1.p2.1">Recent advancements in manga super-resolution (SR) focus on preserving visual integrity during enhancement. Yao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib97" title="">97</a>]</cite> employ deep learning to classify and tailor SR models for manga screentone, ensuring semantic preservation. Dai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib98" title="">98</a>]</cite> introduce the Structured Fusion Attention Network (SFAN), which optimizes feature extraction and reconstruction quality through attention modules, achieving superior performance on datasets like Manga109. These methods enhance image quality while maintaining the artistic nuances critical to manga, effectively balancing technical efficiency with artistic integrity.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.2 </span>Style-Transfer</h4>
<div class="ltx_para" id="S6.SS2.SSS2.p1">
<p class="ltx_p" id="S6.SS2.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS2.p1.1.1">Definition:</span> Style-transfer in comics encompasses a range of image-to-image modifications, including photo-to-comic transformation, image-to-image translation, and colorization, characterized by similar input and output attributes.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p2">
<p class="ltx_p" id="S6.SS2.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS2.p2.1.1">Photo-to-Comic Transformation:</span>
Photo-to-comic has mainly involved mangas and involves replacing colors and textures with halftone patterns. Traditionally, it often consists of manually selecting and adjusting the colors. Methods have evolved from basic pattern generation to sophisticated techniques capturing structural and tonal similarities. Classic approaches relied on algorithmic solutions and were limited in their ability to handle multiple screentone. Recent advancements by Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib99" title="">99</a>]</cite> utilize deep learning for semantic region segmentation in cartoon images, followed by screentone application. However, these methods are primarily effective for color-based inputs, with limited applicability to line-based images.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p3">
<p class="ltx_p" id="S6.SS2.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS2.p3.1.1">Image-to-Image Translation:</span>
Topal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib100" title="">100</a>]</cite> explored style transfer as a transfer for the detection task. Specifically, they used style transfer networks like CycleGAN and CartoonGAN to train a detection network, which was then fine-tuned on specific comic datasets for character and face detection. This approach, to use style transfer techniques to enhance performances in other tasks, is not new <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib68" title="">68</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib101" title="">101</a>]</cite>.</p>
</div>
<figure class="ltx_figure" id="S6.F5">
<div class="ltx_flex_figure">
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S6.F5.1" style="width:99.7pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_square" height="673" id="S6.F5.1.g1" src="x6.png" width="831"/>
</div>
</div>
<div class="ltx_flex_cell ltx_flex_size_2">
<div class="ltx_block ltx_figure_panel ltx_minipage ltx_align_center ltx_align_middle" id="S6.F5.2" style="width:99.7pt;">
<img alt="Refer to caption" class="ltx_graphics ltx_img_landscape" height="640" id="S6.F5.2.g1" src="x7.png" width="830"/>
</div>
</div>
</div>
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_figure"><span class="ltx_text" id="S6.F5.4.1.1" style="font-size:90%;">Figure 5</span>: </span><span class="ltx_text" id="S6.F5.5.2" style="font-size:90%;">Colorization vs. Screening</span></figcaption>
</figure>
<div class="ltx_para" id="S6.SS2.SSS2.p4">
<p class="ltx_p" id="S6.SS2.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS2.p4.1.1">Colorization:</span>
Colorization in comics, specifically in manga, represents the task of transforming black-and-white images into full-color pages. It is a complex process that requires a balance of artistic understanding and technical precision.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p5">
<p class="ltx_p" id="S6.SS2.SSS2.p5.1">Furusawa et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib102" title="">102</a>]</cite> introduced a comprehensive approach to the colorization of manga. Their method allows for user interaction, where color dots can be added to guide the colorization process. This approach not only automates the colorization but also gives users the flexibility to influence the final output, ensuring that the automatic colorization aligns with the intended aesthetic vision.
In a different approach, Hensman et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib103" title="">103</a>]</cite> presented a model based on Conditional Generative Adversarial Networks (cGANs). Their model, specifically tuned for each reference image, colorizes manga panels to mimic colorization styles. This method implies training a new GAN for each reference image, making it versatile for different styles but requiring significant computational resources for each new style adaptation.
Subsequently, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib104" title="">104</a>]</cite> authors investigated screentone synthesis, a critical aspect of manga art that involves translating line drawings into screentone-patterned images. They utilized Manga109 for comparison with standard image-to-image translation methods like pix2pix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib105" title="">105</a>]</cite>. Their approach, although similar in architecture to U-Net used in previous studies <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib106" title="">106</a>]</cite>, stands out by explicitly considering screentone labels, thus preserving the integrity of screentone patterns in the final output.
Moving towards a more advanced solution, Golyadkin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib107" title="">107</a>]</cite> introduced a two-stage architecture for manga page colorization. The first stage involves drafting the coloring using the Pixel2Style2Pixel architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib108" title="">108</a>]</cite>, followed by a conditional GAN in the second stage to refine the output. This method elegantly fuses color drafts with color hints from users, generating high-quality colorizations. They demonstrated improvements both qualitatively and quantitatively, indicating a significant advancement in manga colorization technology.
Building upon previous methods, Jiramahapokee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib109" title="">109</a>]</cite> further refined the colorization process. They leveraged a multi-encoder Variational Autoencoder (VAE) for improved color region consistency and combined it with the shading strengths of other tools like manga-colorization-v2. Their approach also incorporated CIELAB <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib110" title="">110</a>]</cite> interpolation to enhance color saturation and authenticity, leading to outputs that are not only aesthetically pleasing but also consistent and detailed in their shading quality.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p6">
<p class="ltx_p" id="S6.SS2.SSS2.p6.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS2.p6.1.1">Screening:</span>
Screening, or adding screentone to black-and-white manga drawings, has been a focus of several studies. A notable contribution by Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib111" title="">111</a>]</cite> introduced a two-stage architecture for this process. The first stage involves generating grayscale shading using a six-layer Swin-transformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib112" title="">112</a>]</cite>, which then serves as input for the second stage, a reference-based screentone generation module. This algorithmic process extracts screentone patterns from a reference manga and applies them to line drawings, considering the shading nuances provided by the first stage.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS2.p7">
<p class="ltx_p" id="S6.SS2.SSS2.p7.1">These advancements in colorization represent a significant contribution to the domain of comics, particularly manga, where colorization can profoundly impact the narrative and aesthetic appeal of the stories.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.3 </span>Vectorization</h4>
<div class="ltx_para" id="S6.SS2.SSS3.p1">
<p class="ltx_p" id="S6.SS2.SSS3.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS3.p1.1.1">Definition:</span> Vectorization in the context of comics, particularly manga, involves transforming raster images into vector formats. This process includes operations like screening, structural lines extraction, and converting images into vector graphics, each with unique methodologies and applications.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS3.p2">
<p class="ltx_p" id="S6.SS2.SSS3.p2.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS3.p2.1.1">Structural Lines Extraction:</span>
This process, structural lines extraction, focuses on removing screentones to leave only the defining lines of figures and shapes. It can be seen as the reverse operation of the screening, which corresponds to adding the screentone to a structural line image. In this task, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib106" title="">106</a>]</cite> developed a CNN-based method using a U-Net architecture with residual blocks to effectively extract structural lines from manga pages. This method is adaptable for various applications, including manga retargeting and colorization, and can handle diverse patterns and textures, resulting in clean skeleton sketches.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS3.p3">
<p class="ltx_p" id="S6.SS2.SSS3.p3.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS3.p3.1.1">Manga Vectorization:</span>
Various works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib113" title="">113</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib114" title="">114</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib115" title="">115</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib116" title="">116</a>]</cite> have proposed different approaches for this task. In particular, Yao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib117" title="">117</a>]</cite> developed a method involving adaptive binarization and screentone detection, refining borders, estimating lighting, and compensating for missing strokes, leading to high-quality, resolution-independent rendering. A recent innovative approach, MARVEL by Su et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib118" title="">118</a>]</cite>, employs Reinforcement Learning to train an agent in selecting drawing primitives that recreate raster manga images in vector format. While effective, this approach works with squared manga patches and may result in visual gaps in certain renderings. Moreover, this method is not convenient in shape rendering, as it uses primitives and cannot manipulate them. MARVEL represents an interesting exploration into the use of machine learning for vectorization, highlighting both the potential and the complexities of this task.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S6.SS2.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">6.2.4 </span>Depth Estimation</h4>
<div class="ltx_para" id="S6.SS2.SSS4.p1">
<p class="ltx_p" id="S6.SS2.SSS4.p1.1"><span class="ltx_text ltx_font_bold" id="S6.SS2.SSS4.p1.1.1">Definition:</span> Depth Estimation within the comic domain refers to the process of predicting the spatial distance between objects and the viewer in a two-dimensional comic panel. It serves a crucial purpose in the <span class="ltx_text ltx_font_italic" id="S6.SS2.SSS4.p1.1.2">reconfiguration</span> of comic panels and unlocks potential developments in digital artistry.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS4.p2">
<p class="ltx_p" id="S6.SS2.SSS4.p2.1">Up to now, this necessity arose particularly when adapting comics for various digital media, such as smartphones, tablets, and e-readers, where the original layout may not fit the display dimensions optimally. The primary methodologies for reconfiguring comic panels involve: (i) Adjusting panel proportions and rescaling them to fit the new medium without compromising the narrative or visual integrity. (ii) Strategically cropping panels while ensuring that key objects within each panel are preserved and remain visible in the adapted format. Both techniques rely heavily on depth estimation to identify and prioritize the most significant elements in each panel.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS4.p3">
<p class="ltx_p" id="S6.SS2.SSS4.p3.1">One significant challenge in this domain is the lack of ground truth data for comics. To address this, Bhattacharjee et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib7" title="">7</a>]</cite> adopted an innovative approach using Image-to-Image style-transfer. They transformed comic images to resemble natural images, upon which depth estimation techniques, involving Laplacian edges and feature-based GANs, were applied to predict depth effectively. Building on this, a subsequent study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib119" title="">119</a>]</cite>, based on their earlier work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib120" title="">120</a>]</cite>, introduced a Swin Transformer for multitasking, including both depth estimation and object segmentation. This multi-decoder-head architecture, while effective, presents challenges in scalability due to the complexity of accommodating multiple tasks.</p>
</div>
<div class="ltx_para" id="S6.SS2.SSS4.p4">
<p class="ltx_p" id="S6.SS2.SSS4.p4.1">Depth Estimation plays a pivotal role in adapting comics to the varying formats of digital consumption. By ensuring that key visual elements are retained and appropriately displayed, these methodologies not only enhance the reader’s experience but also preserve the artistic intent of the original work.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S6.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">6.3 </span><span class="ltx_text ltx_font_italic" id="S6.SS3.1.1">Satellite Tasks</span>
</h3>
<div class="ltx_para" id="S6.SS3.p1">
<p class="ltx_p" id="S6.SS3.p1.1">Some of the tasks that fit under the Augmentation umbrella have not found a space in our taxonomy, e.g. the task of de-warping. The task involves correcting distortions typically found in scanned comic pages. Addressing the de-warping challenge, a recent study by Garai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib121" title="">121</a>]</cite> proposed a novel mathematical model to describe the warping process. This model estimates distortion factors based on the boundaries of panels in a comic document image, aiming to rectify these warps effectively. While their approach yielded promising qualitative results, particularly in the image areas, challenges persist in handling text distortion, where some distortions proved unrecoverable.</p>
</div>
</section>
</section>
<section class="ltx_section" id="S7">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">7 </span><span class="ltx_text ltx_font_smallcaps" id="S7.1.1">Layer 2: Grounding, Analysis, and Segmentation</span>
</h2>
<div class="ltx_para" id="S7.p1">
<p class="ltx_p" id="S7.p1.1">The second layer of our <span class="ltx_text ltx_font_italic" id="S7.p1.1.1">LoCU</span> framework comprises three distinct task groups: <span class="ltx_text ltx_font_bold" id="S7.p1.1.2">grounding</span>, <span class="ltx_text ltx_font_bold" id="S7.p1.1.3">analysis</span>, and <span class="ltx_text ltx_font_bold" id="S7.p1.1.4">segmentation</span>. These tasks dig deeper into the intricate components of comics, focusing on detailed elements like panels, text, and characters and their ordering and associations.</p>
</div>
<section class="ltx_subsection" id="S7.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.1 </span><span class="ltx_text ltx_font_italic" id="S7.SS1.2.1">Grounding</span>
</h3>
<div class="ltx_para" id="S7.SS1.p1">
<p class="ltx_p" id="S7.SS1.p1.1">Grounding in comics involves identifying and classifying specific elements within a comic’s panel, such as text, characters, and objects. Under this task name umbrella, many individual tasks belong such as Detection, Character Re-identification, and sentence Grouding.</p>
</div>
<figure class="ltx_figure" id="S7.SS1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="189" id="S7.SS1.1.g1" src="x8.png" width="348"/>
</figure>
<section class="ltx_subsubsection" id="S7.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.1 </span>Detection</h4>
<div class="ltx_para" id="S7.SS1.SSS1.p1">
<p class="ltx_p" id="S7.SS1.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS1.p1.1.1">Definition:</span> In <span class="ltx_text ltx_font_italic" id="S7.SS1.SSS1.p1.1.2">Detection</span> task, given an image and a set of class names (or tags), the goal is to find all instances of those classes in the image and locate them with bounding boxes.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS1.p2">
<p class="ltx_p" id="S7.SS1.SSS1.p2.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS1.p2.1.1">Panels.</span>
Initial studies in digital Manga, such as those by Ponsard et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib122" title="">122</a>]</cite> and Arai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib123" title="">123</a>]</cite>, emphasized automated segmentation and panel detection, underscoring the importance of maintaining reading order. Arai et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib124" title="">124</a>]</cite> further developed these ideas with a focus on real-time detection and extraction techniques, which evolved into more sophisticated SVG-based approaches as explored in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib125" title="">125</a>]</cite>. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib126" title="">126</a>]</cite> contributed advanced methods for panel extraction using region growing techniques and mathematical morphology, combined with speech balloon detection.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS1.p3">
<p class="ltx_p" id="S7.SS1.SSS1.p3.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS1.p3.1.1">Text.</span>
Jomaa et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib127" title="">127</a>]</cite> employed a tracking algorithm for panel extraction, while speech balloons were identified using Robert’s edge detection operator and classified with thresholding. Liu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib128" title="">128</a>]</cite> focused on balloon extraction based on text character locations. Pal et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib129" title="">129</a>]</cite> performed text extraction using SIFT local features, Space Pyramid Matching (SPM), and SVM on eBDtheque and Bangla comics images. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib130" title="">130</a>]</cite> authors contributed to text extraction in Manga109 combining region proposal generation with SVM classifiers. Chu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib131" title="">131</a>]</cite> and Qin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib34" title="">34</a>]</cite> modified existing architectures like Faster R-CNN for detecting text and faces in comics. In particular, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib131" title="">131</a>]</cite> proposed additional aspect ratio regions, while <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib34" title="">34</a>]</cite> built two comics datasets, namely JC2463 (Manga black and white style) and AEC912 (colored American comics style) to test face detection evaluation.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS1.p4">
<p class="ltx_p" id="S7.SS1.SSS1.p4.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS1.p4.1.1">Face.</span>
Interestingly, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib132" title="">132</a>]</cite> authors benchmarked Fast R-CNN, Faster R-CNN and SSD in panel, character, face, and balloon detection. They used the Manga109 dataset, discovering that Fast R-CNN works better in panels when there are clear boundaries, Faster R-CNN when boundaries are not well defined, and SSD needs already cut panels as it is limited for full-page complex mangas.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS1.p5">
<p class="ltx_p" id="S7.SS1.SSS1.p5.1">So far, many methods have used classical detection architectures and applied them to the comic domain to extract elements such as panels, characters, text, etc. A new direction is drawn by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib133" title="">133</a>]</cite> challenged the conventional four-edge polygon representation of comic panels, suggesting segmentation-based approaches using U-Net for more accurate extraction. Here, authors performed extraction through segmentation, employing a U-Net that classifies pixels into background, panels, and borders.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS1.p6">
<p class="ltx_p" id="S7.SS1.SSS1.p6.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS1.p6.1.1">Onomatopoeia.</span>
Recently, the focus has shifted to onomatopoeia detection, a relatively unexplored area in comic analysis. The release of the COO dataset by Baek et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib4" title="">4</a>]</cite> has paved the way for new research in this domain. Authors proposed a modified version of M4C <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib134" title="">134</a>]</cite> designed to link truncated onomatopoeias together. Following this, Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib92" title="">92</a>]</cite> proposed text augmentation techniques for onomatopoeias using EfficientNet.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS1.p7">
<p class="ltx_p" id="S7.SS1.SSS1.p7.1">Sharma et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib135" title="">135</a>]</cite> furthered this research by introducing their dataset for panel and character detection, employing a Faster R-CNN model. However, they did not provide comparative analysis with other models.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS1.p8">
<p class="ltx_p" id="S7.SS1.SSS1.p8.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS1.p8.1.1">Benchmarks.</span>
Lately, Dutta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib136" title="">136</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib69" title="">69</a>]</cite> introduced the BCBId dataset, benchmarking existing datasets (eBDtheque, Manga109, and DCM) and employing YOLO architectures for panel and character detection. They presented comparisons with existing methods, showing enhanced performance in detecting panels and characters across multiple datasets. However, it is unclear if these results are truly comparable. This is due to the fact that in previous (and contemporary) works, authors don’t provide neither information about the settings used (in terms of train-val-split), nor about hyperparameters and model settings to replicate the weights. Only recently, Vivoli et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib2" title="">2</a>]</cite> fairly compares models across various datasets and styles, unifying annotations for multiple detection classes and providing model weights, code, and data to replicate.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.2 </span>Character Re-Identification</h4>
<div class="ltx_para" id="S7.SS1.SSS2.p1">
<p class="ltx_p" id="S7.SS1.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS2.p1.1.1">Definition:</span> <span class="ltx_text ltx_font_italic" id="S7.SS1.SSS2.p1.1.2">Character Re-Identification</span>, also known as character retrieval in comics analysis, has evolved from focusing on visual features alone to incorporating textual information alongside character images.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS2.p2">
<p class="ltx_p" id="S7.SS1.SSS2.p2.1">Initial studies by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib63" title="">63</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib29" title="">29</a>]</cite> employed techniques like PHOG and SIFT keypoints for character retrieval. Iwata et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib137" title="">137</a>]</cite>, inspired by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib138" title="">138</a>]</cite>, adapted these methods specifically for manga characters, considering characters rather than entire page ROIs. Authors further explored applications of these techniques in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib139" title="">139</a>]</cite>. However, recently, Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib5" title="">5</a>]</cite> noted the limitations of handcrafted features, particularly their lack of robustness to variations in pose, expression, and character shape.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS2.p3">
<p class="ltx_p" id="S7.SS1.SSS2.p3.1">In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib5" title="">5</a>]</cite>, authors tackled manga character retrieval and verification. While retrieval is a known task <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S8.SS1" title="8.1 Retrieval ‣ 8 Layer 3: Retrieval and Modification ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">8.1</span></a>, verification refers to the task of classifying whether two images (a pair) belong to the same character or not. These two tasks are challenging since the manga character images have a long-tailed distribution and large quality variations. While addressing challenges posed by long-tailed distribution and quality variations in manga characters, they proposed a novel dual loss approach, combining dual ring loss and dual adaptive re-weighting loss, to overcome biases introduced by imbalanced training data.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS2.p4">
<p class="ltx_p" id="S7.SS1.SSS2.p4.1">In animated cartoons, where characters undergo shape and color changes, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib140" title="">140</a>]</cite> introduced an approach based on Self-Supervised Learning (SSL) and Multi-object Tracking (MOT) to identify character clusters and link character proposals, facilitating character re-identification across different scenes.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS2.p5">
<p class="ltx_p" id="S7.SS1.SSS2.p5.1">Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib141" title="">141</a>]</cite> identified challenges unique to manga, such as similarities in faces but differences in bodies. They proposed the Face-body and Spatial-temporal Associated Clustering method (FSAC), which uses face-body graphs and spatial-temporal relationship correction to refine clustering and overcome artistic exaggerations and deformations, through the design of a temporal-spatial-related triplet loss to ﬁne-tune the clustering.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS2.p6">
<p class="ltx_p" id="S7.SS1.SSS2.p6.1">Recognizing the challenges of occlusion in manga, Zhang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib142" title="">142</a>]</cite> developed Occlusion-Aware Manga Character Re-identification (OAM-ReID) with self-paced contrastive learning. By synthesizing data with occluded speech balloons and incomplete bodies, their framework becomes more effective in learning representative features of characters. They introduce a combination of contrastive, occlusion, and re-identification losses, training a vanilla ViT and TransReID as baselines on 10 Manga109 books.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS2.p7">
<p class="ltx_p" id="S7.SS1.SSS2.p7.1">Soykan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib143" title="">143</a>]</cite> leveraged SimCLR <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib144" title="">144</a>]</cite> with various data augmentation techniques for character re-identification, calling it “Identity-aware SimCLR”, focusing on both local (face) and global (body) representations. Similarly, Sachdeva et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite> proposed a multi-task Transformer model (Relationformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib145" title="">145</a>]</cite>) for character clustering and linking detected character bounding boxes at the page level.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS2.p8">
<p class="ltx_p" id="S7.SS1.SSS2.p8.1">Finally, <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib146" title="">146</a>]</cite> introduced an improved YOLOv5 and XGBoosting approach for object detection, achieving state-of-the-art results across multiple datasets (StyleObject7K, ClipArt1K, Watercolor2K, and Comic2K). Their method highlights the effectiveness of well-annotated training data and extensive data augmentation in detection tasks.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.1.3 </span>Sentence-based Grounding</h4>
<div class="ltx_para" id="S7.SS1.SSS3.p1">
<p class="ltx_p" id="S7.SS1.SSS3.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS1.SSS3.p1.1.1">Definition:</span> Sentence-based grounding is a Vision-Language task that links textual content to specific visual elements within a panel. In comics, this task translates into grounding the sentence elements both into a single panel and a full page. Moreover, the complexity of the task varies depending on what is the sentence to be grounded: a panel description or a dialogue or narrative description.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS3.p2">
<p class="ltx_p" id="S7.SS1.SSS3.p2.1">In image and video Multimedia processing various works tackle the task with custom CNNs and LSTMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib147" title="">147</a>]</cite> approaches, in an unsupervised manner <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib148" title="">148</a>]</cite> or with custom multimodal fusion approaches <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib149" title="">149</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib40" title="">40</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S7.SS1.SSS3.p3">
<p class="ltx_p" id="S7.SS1.SSS3.p3.1">However, the only work that used a similar approach (GroundingDINO) in a zero-shot setting is <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite> which used it to automatically annotate the training set.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S7.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.2 </span><span class="ltx_text ltx_font_italic" id="S7.SS2.2.1">Analysis</span>
</h3>
<div class="ltx_para" id="S7.SS2.p1">
<p class="ltx_p" id="S7.SS2.p1.1">Analysis tasks involve a deeper examination of the relationship between text and visual elements in comics, including character-text association, panel sorting, dialog transcription, and translation.</p>
</div>
<figure class="ltx_figure" id="S7.SS2.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="178" id="S7.SS2.1.g1" src="x9.png" width="349"/>
</figure>
<section class="ltx_subsubsection" id="S7.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.1 </span>Text-character association</h4>
<div class="ltx_para" id="S7.SS2.SSS1.p1">
<p class="ltx_p" id="S7.SS2.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.SSS1.p1.1.1">Definition:</span> The task of <span class="ltx_text ltx_font_italic" id="S7.SS2.SSS1.p1.1.2">text-character association</span> in comics aims to link textual content, usually dialogue, to the corresponding speaking character. This task often involves a two-step process: first identifying panels, text boxes, and character boxes, and then associating the text with its speaker.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS1.p2">
<p class="ltx_p" id="S7.SS2.SSS1.p2.1">Rigaud et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib150" title="">150</a>]</cite> tackled this task by using geometric graph analysis and anchor point selection. They developed a distance-based algorithm, both global (across the page) and single-frame (within a panel), relying on Euclidean distances and balloon tail information.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS1.p3">
<p class="ltx_p" id="S7.SS2.SSS1.p3.1">Advancing from Rigaud et al.’s work, Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib151" title="">151</a>]</cite> employed a multitask learning approach with a custom Mask R-CNN (adding a new “PairPool” head for the association prediction) which they called ComicMTL. Their method simultaneously addressed panel and character detection, balloon segmentation, text recognition, and text-speaker association, demonstrating its effectiveness on the DCM772 and eBDtheque datasets.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS1.p4">
<p class="ltx_p" id="S7.SS2.SSS1.p4.1">In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib152" title="">152</a>]</cite>, authors introduced a novel perspective by considering the continuity of comics. Their work focused on estimating the reading order of frames and texts, vital for associating text with off-panel speakers. However, their algorithmic approach raised concerns regarding scalability and generalizability.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS1.p5">
<p class="ltx_p" id="S7.SS2.SSS1.p5.1">Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib73" title="">73</a>]</cite> extended the Manga109 dataset with the Manga109 dialog dataset, enabling more precise speaker detection. They classified text-character links into “easy” (character in the panel) and “hard” (character not in the panel) configurations, using Faster R-CNN for detection and subsequent association classification. The challenges in this approach, when balloons are spoken alternatively, highlight the potential role of NLP models in handling complex dialogue scenarios.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS1.p6">
<p class="ltx_p" id="S7.SS2.SSS1.p6.1">In a recent study, Sachdeva et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite> proposed a multi-task Transformer-based model (Relationformer <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib145" title="">145</a>]</cite>) with specialized MLP heads for detection and linking tasks. While their model offers an “end-to-end” solution, it operates in two iterations: first for detection and then for linking characters and text. However, their model’s comparison with state-of-the-art methods, particularly in detection, is limited, highlighting the need for more comprehensive benchmarking. In fact, the model does not compare with previous state-of-the-art models (YOLO-based) for detection, but only with publicly available GroundingDino <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib40" title="">40</a>]</cite>, in a zero-shot setting.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS1.p7">
<p class="ltx_p" id="S7.SS2.SSS1.p7.1">The character-text association is a critical but challenging task in comics analysis, requiring a nuanced understanding of narrative structure and visual-textual interplay. Future research may benefit from integrating advanced NLP techniques to better interpret complex dialogue scenarios. Additionally, there is a growing need for unified benchmarks and replicable methodologies, as indicated by the diverse approaches in the <span class="ltx_text ltx_font_italic" id="S7.SS2.SSS1.p7.1.1">LoCU</span> framework, to facilitate comparative analysis and further advancements in this field.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.2 </span>Panel Sorting</h4>
<div class="ltx_para" id="S7.SS2.SSS2.p1">
<p class="ltx_p" id="S7.SS2.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.SSS2.p1.1.1">Definition:</span> <span class="ltx_text ltx_font_italic" id="S7.SS2.SSS2.p1.1.2">Sorting</span> individual comic panels into their correct narrative order presents a unique challenge, both for humans and automated systems.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS2.p2">
<p class="ltx_p" id="S7.SS2.SSS2.p2.1">Ueno et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib153" title="">153</a>]</cite> conducted a study revealing that while people can relatively easily identify the first and last panels in a sequence, accuracy diminishes with the presentation of all panels from four-panel strips. This suggests the complexity involved in discerning narrative continuity in comics.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS2.p3">
<p class="ltx_p" id="S7.SS2.SSS2.p3.1">The same study attempted to automate panel sorting using an AlexNet-like architecture. However, the CNN’s performance was poor, indicating the difficulty of the task and the need for more advanced or specialized algorithms in panel sorting.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S7.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.2.3 </span>Dialog transcription</h4>
<div class="ltx_para" id="S7.SS2.SSS3.p1">
<p class="ltx_p" id="S7.SS2.SSS3.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS2.SSS3.p1.1.1">Definition:</span> <span class="ltx_text ltx_font_italic" id="S7.SS2.SSS3.p1.1.2">Dialog transcription</span> in comics aims to create an automatic transcript of dialogues, identifying the speaker and the sequence of speech.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS3.p2">
<p class="ltx_p" id="S7.SS2.SSS3.p2.1">The process typically involves several steps: (i) detecting panels, text boxes, and character boxes, (ii) associating text with speakers, and (iii) sorting text boxes according to their reading order to generate a coherent dialogue transcript.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS3.p3">
<p class="ltx_p" id="S7.SS2.SSS3.p3.1">This task has been addressed by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib73" title="">73</a>]</cite> for Japanese Mangas, proposing the needed annotations for Manga109, and more recently by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite> for English Mangas called PopManga. However, a few limitations arise from these approaches: sorting the text boxes is always done as a postprocessing operation, depending only on algorithmic approaches. Cases in which balloons alternate among close panels are inherently mistaken. Recently, Sachdeva et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib43" title="">43</a>]</cite> proposed Magiv2 which tackled the problem of dialog transcription adding also names (from a name bank) allowing it to operate on the whole collection of pages.</p>
</div>
<div class="ltx_para" id="S7.SS2.SSS3.p4">
<p class="ltx_p" id="S7.SS2.SSS3.p4.1">In CoMix <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib42" title="">42</a>]</cite>, a first benchmark for this task was proposed, authors enhanced the previous annotations and proposed a new metric for character naming (based on ANLS) and dialog transcription (based on Hungarian matching and edit distance).</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S7.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.3 </span><span class="ltx_text ltx_font_italic" id="S7.SS3.2.1">Segmentation</span>
</h3>
<div class="ltx_para" id="S7.SS3.p1">
<p class="ltx_p" id="S7.SS3.p1.1">Segmentation in comic analysis, particularly instance segmentation, presents a nuanced approach to identifying and isolating various elements within a comic panel.</p>
</div>
<figure class="ltx_figure" id="S7.SS3.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="206" id="S7.SS3.1.g1" src="x10.png" width="349"/>
</figure>
<section class="ltx_subsubsection" id="S7.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.3.1 </span>Instance segmentation</h4>
<div class="ltx_para" id="S7.SS3.SSS1.p1">
<p class="ltx_p" id="S7.SS3.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS3.SSS1.p1.1.1">Definition:</span> <span class="ltx_text ltx_font_italic" id="S7.SS3.SSS1.p1.1.2">Instance segmentation</span> in comics extends beyond the scope of typical object detection. Rather than just identifying and classifying elements within bounding boxes, this task involves generating precise pixel-wise masks for each instance.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS1.p2">
<p class="ltx_p" id="S7.SS3.SSS1.p2.1">By focusing on segmentation instead of detection, we can achieve more accurate panel separation and representation. This precision is especially beneficial when reconfiguring or reformatting comics for different layouts. It ensures that each panel is cleanly isolated, without the remnants of adjacent panels or the original layout, which is a common issue with less precise bounding boxes.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS1.p3">
<p class="ltx_p" id="S7.SS3.SSS1.p3.1">One of the key challenges in instance segmentation within comics is dealing with the diversity of artistic styles and the complexity of layouts. Comics often feature intricate designs, overlapping elements, and varying levels of detail, which can complicate the segmentation process. As described in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib47" title="">47</a>]</cite>, there are dozens of configurations in which panels appear to respect each other (pure grid, vertical and horizontal staggering, bleed, inset, overlap, etc.). Addressing the task of separating panels and understanding their relative positions within Cohn nomenclature is hard with bounding boxes when delimitations are not precise.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS1.p4">
<p class="ltx_p" id="S7.SS3.SSS1.p4.1">Future research in comic instance segmentation could explore advanced deep-learning models that are adept at handling these complexities. The development of specialized algorithms that can adapt to the unique characteristics of different comic styles would significantly advance the field. There’s also potential for integrating instance segmentation with other tasks like character-text association and panel sorting to create a more comprehensive comic analysis system.</p>
</div>
<div class="ltx_para" id="S7.SS3.SSS1.p5">
<p class="ltx_p" id="S7.SS3.SSS1.p5.1">Across the presented datasets, no one proposes semantic segmentations.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S7.SS4">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">7.4 </span><span class="ltx_text ltx_font_italic" id="S7.SS4.1.1">Future Tasks</span>
</h3>
<div class="ltx_para" id="S7.SS4.p1">
<p class="ltx_p" id="S7.SS4.p1.1">The Layer of Comics Understanding is a general framework with which we aim to collect various Vision-Language tasks. Therefore, it includes also tasks that are currently not explored in comics, such as Translation.</p>
</div>
<section class="ltx_subsubsection" id="S7.SS4.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">7.4.1 </span>Translation</h4>
<div class="ltx_para" id="S7.SS4.SSS1.p1">
<p class="ltx_p" id="S7.SS4.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S7.SS4.SSS1.p1.1.1">Definition:</span> The task of translation in comics refers to translating the textual elements that appear in a panel or a page, according to the scene. It goes beyond simple text translation, intertwining linguistic, graphical, and spatial challenges. It necessitates an integrated approach that respects the visual and narrative structure of the comic.</p>
</div>
<div class="ltx_para" id="S7.SS4.SSS1.p2">
<p class="ltx_p" id="S7.SS4.SSS1.p2.1">While <span class="ltx_text ltx_font_italic" id="S7.SS4.SSS1.p2.1.1">translation</span> in comics may seem like a straightforward NLP problem, it involves unique complications. Text in comics appears in various forms such as balloons, onomatopoeias, and scene text, each requiring a different approach.
In <span class="ltx_text ltx_font_italic" id="S7.SS4.SSS1.p2.1.2">Balloon Text</span>, translated text must fit within the space of the original speech balloon, preserving the visual integrity of the panel. Complications arise when the text to be translated contains elements like sarcasm, irony, jokes, or idiomatic expressions.
In <span class="ltx_text ltx_font_italic" id="S7.SS4.SSS1.p2.1.3">Onomatopoeias</span>, both linguistic translation and graphic replacement are often required to maintain the style consistent with the target language.
Lastly, in <span class="ltx_text ltx_font_italic" id="S7.SS4.SSS1.p2.1.4">Scene Text</span>, translating involves both linguistic translation and potentially replacing the text within the scene, which can be a complex graphic design task.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S8">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">8 </span><span class="ltx_text ltx_font_smallcaps" id="S8.1.1">Layer 3: Retrieval and Modification</span>
</h2>
<div class="ltx_para" id="S8.p1">
<p class="ltx_p" id="S8.p1.1">Layer 3 in the <span class="ltx_text ltx_font_italic" id="S8.p1.1.1">Layers of Comics Understanding</span> focuses into advanced image and text understanding through tasks such as Retrieval (uni-modal and multi-modal) and Modification.</p>
</div>
<section class="ltx_subsection" id="S8.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.1 </span><span class="ltx_text ltx_font_italic" id="S8.SS1.2.1">Retrieval</span>
</h3>
<figure class="ltx_figure" id="S8.SS1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="176" id="S8.SS1.1.g1" src="x11.png" width="349"/>
</figure>
<section class="ltx_subsubsection" id="S8.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.1 </span>Unimodal Retrieval</h4>
<div class="ltx_para" id="S8.SS1.SSS1.p1">
<p class="ltx_p" id="S8.SS1.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S8.SS1.SSS1.p1.1.1">Definition:</span> Unimodal retrieval in comics involves searching for a specific media element (image or text) within a database, using a query of the same format (image or text, respectively).</p>
</div>
<div class="ltx_para" id="S8.SS1.SSS1.p2">
<p class="ltx_p" id="S8.SS1.SSS1.p2.1">Pioneering works in this area have explored various aspects of comic retrieval. Wu et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib154" title="">154</a>]</cite> developed a comic retrieval system that leveraged OCR and external text.</p>
</div>
<div class="ltx_para" id="S8.SS1.SSS1.p3">
<p class="ltx_p" id="S8.SS1.SSS1.p3.1">One significant article <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib138" title="">138</a>]</cite> focused on retrieving full pages or books containing similar elements. This approach is particularly relevant in copyright violation contexts. The authors implemented feature vectors for each image, storing them in a database, and used these vectors to find the most similar documents. Their feature representation method was based on the Histogram of Oriented Gradients (HOG), which outperformed the traditional SIFT method. Notably, they applied HOG not to the entire images but selectively to Regions Of Interest (ROIs) identified in the comics, such as faces or specific markings.</p>
</div>
<div class="ltx_para" id="S8.SS1.SSS1.p4">
<p class="ltx_p" id="S8.SS1.SSS1.p4.1">Building on this approach, authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib137" title="">137</a>]</cite> utilized a similar methodology for manga character retrieval. They focused on identifying and extracting distinctive features from characters, enhancing the ability to retrieve specific characters from large databases.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S8.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.1.2 </span>Cross-modal retrieval</h4>
<div class="ltx_para" id="S8.SS1.SSS2.p1">
<p class="ltx_p" id="S8.SS1.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S8.SS1.SSS2.p1.1.1">Definition:</span> Cross-modal retrieval in comics encompasses tasks where the query and the retrieval results are from different modalities, such as using sketches or text queries for retrieving a comic image.</p>
</div>
<div class="ltx_para" id="S8.SS1.SSS2.p2">
<p class="ltx_p" id="S8.SS1.SSS2.p2.1"><span class="ltx_text ltx_font_bold" id="S8.SS1.SSS2.p2.1.1">Sketch-based Retrieval.</span>
Matsui et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib32" title="">32</a>]</cite> introduced a framework for retrieving manga images from sketches using the Fine Multi-scale Edge Orientation Histogram (FMEOH). This method indexed different-sized squares on a page and enabled efficient retrieval from sketch queries.
Further advancing this area, authors <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib155" title="">155</a>]</cite> proposed a two-step system that first processes manga and then retrieves images based on sketches. This framework was one of the first to employ the Manga109 dataset and included margin labeling, objectness-based edge orientation histograms, and approximate nearest neighbor search. Narita et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib156" title="">156</a>]</cite> explored CNN-based feature extraction for sketches and manga images (without and with screentone, respectively), demonstrating improvements over algorithmic approaches.</p>
</div>
<div class="ltx_para" id="S8.SS1.SSS2.p3">
<p class="ltx_p" id="S8.SS1.SSS2.p3.1"><span class="ltx_text ltx_font_bold" id="S8.SS1.SSS2.p3.1.1">Multimodal Learning and Character Retrieval.</span>
Nguyen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib157" title="">157</a>]</cite> proposed a multitask multimodal approach for character image and text learning, aiming at tasks such as character retrieval and emotion recognition. Their method focused on leveraging both visual and verbal information in manga content, in a CLIP-like style aiming at performing various character tasks (e.g. detection, retrieval, emotion classification), trained uniquely with visual information and verbal information in manga image content.
Later, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib158" title="">158</a>]</cite> authors introduced ComicLib, a dataset for comic sketch research, providing benchmarks across various tasks like colorization, generation, and retrieval. They conducted extensive comparison experiments with other datasets (e.g. QuickDraw <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib159" title="">159</a>]</cite>) to provide a benchmark for ComicLib on common tasks like colorization (Style2Paints <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib160" title="">160</a>]</cite>), generation (DCGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib161" title="">161</a>]</cite>), retrieval (image-to-image with ResNet), detection (with Faster R-CNN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib162" title="">162</a>]</cite>, YOLOv3 <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib163" title="">163</a>]</cite>, and SSD <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib164" title="">164</a>]</cite>) and recognition. However, the dataset’s availability is unclear.</p>
</div>
<div class="ltx_para" id="S8.SS1.SSS2.p4">
<p class="ltx_p" id="S8.SS1.SSS2.p4.1"><span class="ltx_text ltx_font_bold" id="S8.SS1.SSS2.p4.1.1">Text-to-Image Retrieval.</span>
Shen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib165" title="">165</a>]</cite> presented a system for text-to-image content retrieval within Manga frames. This multi-staged system integrates object detection, text recognition, and a vision-text encoder to facilitate efficient search and retrieval of dialogues and scenes. Their method is composed of an object detection model for identifying text and frame bounding boxes (DETR-based <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib166" title="">166</a>]</cite>), a Vision Encoder-Decoder model for text recognition (DiT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib167" title="">167</a>]</cite>), a text encoder for embedding text (multilingual DistilBERT <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib168" title="">168</a>]</cite>), and a vision-text encoder with unified embedding space (CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib14" title="">14</a>]</cite>). They experimented with Japanese and GPT-4<span class="ltx_note ltx_role_footnote" id="footnote10"><sup class="ltx_note_mark">10</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">10</sup><span class="ltx_tag ltx_tag_note">10</span>Using the website <a class="ltx_ref ltx_href" href="https://platform.openai.com/" title="">https://platform.openai.com/</a></span></span></span>translate/rephrased English sentences, on the annotated panels from “Dollgun Book”. Despite being a composition of existing systems on panel level, on a small distribution of data, the work is a first direction to more complex Comic retrieval.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S8.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.2 </span><span class="ltx_text ltx_font_italic" id="S8.SS2.2.1">Modification</span>
</h3>
<figure class="ltx_figure" id="S8.SS2.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="172" id="S8.SS2.1.g1" src="x12.png" width="348"/>
</figure>
<div class="ltx_para" id="S8.SS2.p1">
<p class="ltx_p" id="S8.SS2.p1.1">In the realm of comics, modification tasks such as image inpainting and image editing play a crucial role in streamlining the storytelling process and enhancing creative expression.</p>
</div>
<section class="ltx_subsubsection" id="S8.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.2.1 </span>Image Inpainting and Editing</h4>
<div class="ltx_para" id="S8.SS2.SSS1.p1">
<p class="ltx_p" id="S8.SS2.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S8.SS2.SSS1.p1.1.1">Definition:</span> Image inpainting and editing in the context of comics involve various techniques and tools that allow for the alteration or enhancement of comic panels, facilitating more dynamic and engaging storytelling.</p>
</div>
<div class="ltx_para" id="S8.SS2.SSS1.p2">
<p class="ltx_p" id="S8.SS2.SSS1.p2.1">One innovative tool in this space is CodeToon, presented by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib169" title="">169</a>]</cite>. CodeToon is a comic authoring tool that facilitates a code-driven storytelling process. It leverages two mechanisms: story ideation from code through metaphorical interpretation and automatic comic generation from these ideated stories. Significant attention has been given to the interface design to address common challenges faced by comic creators, such as generating stories directly from code snippets and seamlessly transitioning these stories into comic format.</p>
</div>
<div class="ltx_para" id="S8.SS2.SSS1.p3">
<p class="ltx_p" id="S8.SS2.SSS1.p3.1">Another notable contribution is ComicScript by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib170" title="">170</a>]</cite>, also known as interactive data comics<span class="ltx_note ltx_role_footnote" id="footnote11"><sup class="ltx_note_mark">11</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">11</sup><span class="ltx_tag ltx_tag_note">11</span>Accessible at <a class="ltx_ref ltx_href" href="https://interactivedatacomics.github.io/" title="">https://interactivedatacomics.github.io/</a>.</span></span></span>. ComicScript is a suite of operations designed in collaboration with practitioners to create and modify comics. These operations allow for the addition and removal of panels, support branching narratives, perspective changes, and detailed exploration. They also facilitate data manipulation and interaction within the comics, enhancing personalization and reader engagement. To validate the utility of ComicScript, professional illustrators, designers, and data comics enthusiasts were recruited to craft interactive comics. Their experiences provided valuable insights into the authoring workflow and the potential of interactive comics as a storytelling medium.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S8.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">8.3 </span><span class="ltx_text ltx_font_italic" id="S8.SS3.1.1">Future Tasks</span>
</h3>
<section class="ltx_subsubsection" id="S8.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">8.3.1 </span>Personalized Image retrieval</h4>
<div class="ltx_para" id="S8.SS3.SSS1.p1">
<p class="ltx_p" id="S8.SS3.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S8.SS3.SSS1.p1.1.1">Definition:</span> Personalized Image Retrieval (PIR) focuses on developing person-centric models that can efficiently retrieve specific comic panels based on a compound query involving both image and text inputs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib171" title="">171</a>]</cite>. PIR aims to identify and retrieve images or videos that correspond to a compound query, which typically includes an image of a person’s face combined with a textual scene or action description.</p>
</div>
<div class="ltx_para" id="S8.SS3.SSS1.p2">
<p class="ltx_p" id="S8.SS3.SSS1.p2.1">In the realm of comics, this translates to the capability of retrieving panels where a specific character appears, given an image of the character, the character’s name, or a description of the character. This task becomes increasingly complex when the retrieval criteria include context-specific actions (e.g., a character eating pizza or jumping out of a window) or interactions with other characters.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S9">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">9 </span><span class="ltx_text ltx_font_smallcaps" id="S9.1.1">Layer 4: Advanced Visual-Language Understanding</span>
</h2>
<section class="ltx_subsection" id="S9.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.1 </span><span class="ltx_text ltx_font_italic" id="S9.SS1.2.1">Understanding</span>
</h3>
<div class="ltx_para" id="S9.SS1.p1">
<p class="ltx_p" id="S9.SS1.p1.1">This layer investigates tasks that require a deeper integration of visual elements and language, advancing beyond basic identification or retrieval to more complex forms of comprehension and reasoning within comics.</p>
</div>
<figure class="ltx_figure" id="S9.SS1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="164" id="S9.SS1.1.g1" src="x13.png" width="348"/>
</figure>
<section class="ltx_subsubsection" id="S9.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.1.1 </span>Visual Question Answering</h4>
<div class="ltx_para" id="S9.SS1.SSS1.p1">
<p class="ltx_p" id="S9.SS1.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S9.SS1.SSS1.p1.1.1">Definition:</span> Given an image-question pair, <span class="ltx_text ltx_font_italic" id="S9.SS1.SSS1.p1.1.2">Visual Question Answer</span> requires answering a question based on the image. Most studies treat VQA as a classiﬁcation problem on a predeﬁned answer set, but its general definition would be an open answer where no specific set of options is provided.</p>
</div>
<div class="ltx_para" id="S9.SS1.SSS1.p2">
<p class="ltx_p" id="S9.SS1.SSS1.p2.1">The first notable attempt in the domain of the comic is by <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib172" title="">172</a>]</cite> with ComicVQA. However, their approach, more akin to retrieval based on specific “Doraemon” comics datasets, diverges from the typical VQA framework.
Later, Sumi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib173" title="">173</a>]</cite> presented a system, ComicsQA, with a unique twist, converting user QA interactions into comic stories, reflecting the user’s situation and offering solutions.</p>
</div>
<div class="ltx_para" id="S9.SS1.SSS1.p3">
<p class="ltx_p" id="S9.SS1.SSS1.p3.1">The creation of a comprehensive VQA dataset specifically for comics, encompassing single-panel, single-page, and multi-page formats, remains an open challenge. In a recent survey <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib174" title="">174</a>]</cite>, Zeng et al. show the massive work that has been carried out on VQA on multi-modal data across different mediums (movies, comics, etc.) and Multimodal Machine Comprehension <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib175" title="">175</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib176" title="">176</a>]</cite>. This, together with multiple datasets across various mediums <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib177" title="">177</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib178" title="">178</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib179" title="">179</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib180" title="">180</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib181" title="">181</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib181" title="">181</a>]</cite> are laying the groundwork for advancing VQA also in comics.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S9.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">9.2 </span><span class="ltx_text ltx_font_italic" id="S9.SS2.1.1">Future Tasks</span>
</h3>
<div class="ltx_para" id="S9.SS2.p1">
<p class="ltx_p" id="S9.SS2.p1.1">In the context of Understanding, a number of Vision Language tasks could be of interest in comics, both for pertaining and fine-tuning.</p>
</div>
<section class="ltx_subsubsection" id="S9.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.1 </span>Visual Entailment</h4>
<div class="ltx_para" id="S9.SS2.SSS1.p1">
<p class="ltx_p" id="S9.SS2.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S9.SS2.SSS1.p1.1.1">Definition:</span> Visual entailment (VE) in comics is an area yet to be deeply explored. This task involves assessing whether a given image semantically entails the accompanying text. The challenge lies in interpreting the visual narrative in conjunction with textual elements to ascertain semantic consistency or correlation. It can be seen as a simplified version of the text-closure task where we aim at correctly classifying whether the image is represented by the given text, however, depending on the provided text, VE could quickly lead to complex scenarios.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S9.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.2 </span>Visual Dialog</h4>
<div class="ltx_para" id="S9.SS2.SSS2.p1">
<p class="ltx_p" id="S9.SS2.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S9.SS2.SSS2.p1.1.1">Definition:</span> The task of Visual Dialogs (VisDial) corresponds to answering a specific question, having an image, a question about the image, and the previous dialog history about the image.</p>
</div>
<div class="ltx_para" id="S9.SS2.SSS2.p2">
<p class="ltx_p" id="S9.SS2.SSS2.p2.1">In comics, Visual dialog is an area still in its infancy and requires constructing a narrative dialogue based on visual cues within the comic panels. This task demands a comprehensive understanding of the storyline, character interactions, and visual symbolism to generate contextually relevant dialogues.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S9.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">9.2.3 </span>Visual Reasoning</h4>
<div class="ltx_para" id="S9.SS2.SSS3.p1">
<p class="ltx_p" id="S9.SS2.SSS3.p1.1"><span class="ltx_text ltx_font_bold" id="S9.SS2.SSS3.p1.1.1">Definition:</span> Visual reasoning in comics extends beyond simple question answering to include the ability to interpret and analyze the visual content in depth. This task necessitates understanding the objects, their interactions, and the underlying narrative structure within the comic panels. It’s a sophisticated blend of visual comprehension and logical deduction, aimed at uncovering deeper layers of meaning in the comics.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S10">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">10 </span><span class="ltx_text ltx_font_smallcaps" id="S10.1.1">Layer 5: Generation and Synthesis</span>
</h2>
<div class="ltx_para" id="S10.p1">
<p class="ltx_p" id="S10.p1.1">The last layer explores the creative frontier in comics analysis, where the synthesis and generation of comics from various media sources play a pivotal role.</p>
</div>
<section class="ltx_subsection" id="S10.SS1">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.1 </span><span class="ltx_text ltx_font_italic" id="S10.SS1.2.1">Generation</span>
</h3>
<div class="ltx_para" id="S10.SS1.p1">
<p class="ltx_p" id="S10.SS1.p1.1">The field of Generation in comics has seen lots of attraction on Manga style comics rather than American ones. Their popularity has attracted the research community towards creating comics from various media such as video, images, and text descriptions.</p>
</div>
<figure class="ltx_figure" id="S10.SS1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_landscape" height="202" id="S10.SS1.1.g1" src="x14.png" width="348"/>
</figure>
<section class="ltx_subsubsection" id="S10.SS1.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">10.1.1 </span>Comics generation from other media.</h4>
<div class="ltx_para" id="S10.SS1.SSS1.p1">
<p class="ltx_p" id="S10.SS1.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S10.SS1.SSS1.p1.1.1">Definition:</span> The task of generating comics from other media comprehends an umbrella of tasks with various input types (video, audio, charts, and comics itself) whose output is a comic panel, a sequence of panels, a page, or a book.</p>
</div>
<div class="ltx_para" id="S10.SS1.SSS1.p2">
<p class="ltx_p" id="S10.SS1.SSS1.p2.1">One of the earliest attempts at comic generation from other media is the “Comic Live Chat” by Matsuda et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib182" title="">182</a>]</cite>, which transformed video meetings into comics. This approach involved selecting keyframes and rendering the dialogues of meeting participants in a comic format. Tanapichet et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib183" title="">183</a>]</cite> proposed a system for creating comic strips from cartoon animations using optical flow techniques, demonstrating a novel approach to narrative translation from one visual medium to another. Hoashi et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib184" title="">184</a>]</cite> worked on creating manga previews by detecting panels and text balloons, while in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib185" title="">185</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib186" title="">186</a>]</cite> authors explored generating manga faces and caricatures from real images. Cao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib187" title="">187</a>]</cite> and Herranz et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib188" title="">188</a>]</cite> introduced methods for comic-like video summarization. These systems developed pipelines for layout proposal, image reconfiguration, and balloon creation, treating comic creation as a series of steps from keyframe selection to final panel layout. On the same task, to automatically create comics from video content, Jing et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib189" title="">189</a>]</cite> pioneered video transformation using TV show subtitles and a speaker detection algorithm to generate comic layouts optimized through an energy-based metric. This approach was a significant step forward compared to earlier methods like <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib190" title="">190</a>]</cite>.</p>
</div>
<div class="ltx_para" id="S10.SS1.SSS1.p3">
<p class="ltx_p" id="S10.SS1.SSS1.p3.1">Giving it more time, a groundbreaking approach by Pesko et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib191" title="">191</a>]</cite> employed neural style transfer, creating visually compelling comic representations from video-sourced data<span class="ltx_note ltx_role_footnote" id="footnote12"><sup class="ltx_note_mark">12</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">12</sup><span class="ltx_tag ltx_tag_note">12</span>GitHub repository: <a class="ltx_ref ltx_href" href="https://github.com/maciej3031/comixify" title="">https://github.com/maciej3031/comixify</a></span></span></span>. While their startup has since become closed-sourced, their initial approach signifies a significant leap in comic generation technology. Yang et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib192" title="">192</a>]</cite> presented a system to generate comics from movies, employing a multi-page layout framework and emotion-aware balloon generation, showcasing the potential of comprehensive systems in comic generation. To extract keyframes from movies, they used GIST <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib193" title="">193</a>]</cite> similarity between two frames. To detect the speaker, they perform lip motion analysis and use that to associate balloons with talking characters. Finally, the movie frames are rendered in a comic-layout page after the adaption to comic style.
Moreover, in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib194" title="">194</a>]</cite> authors represent conversational videos to comics “fixed-layout” representation, developing a system that learns guidance ﬁeld which provides a prior prediction of the possible positions of word balloons while making the word balloons not overlap with other nonverbal information (e.g. hand gestures, visual clues in the background, etc). Their input is raw video, thus the text dialog is captured from audio and rendered in the location chosen from their method.</p>
</div>
<div class="ltx_para" id="S10.SS1.SSS1.p4">
<p class="ltx_p" id="S10.SS1.SSS1.p4.1">Comics have been used also in education <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib195" title="">195</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib196" title="">196</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib197" title="">197</a>]</cite> to teach and explain in an easier way different concepts. An example of this is <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib198" title="">198</a>]</cite> where authors proposed a system that crafts data stories from a collection of user-created charts, using a comic-style panel to imply the underlying sequence and logic of data-driven narratives.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S10.SS1.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">10.1.2 </span>Image-to-Text generation</h4>
<div class="ltx_para" id="S10.SS1.SSS2.p1">
<p class="ltx_p" id="S10.SS1.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S10.SS1.SSS2.p1.1.1">Definition:</span> Image-to-Text is a general umbrella term that covers tasks from captioning, to textbox prediction in comics, masking a specific part of the dialog.</p>
</div>
<div class="ltx_para" id="S10.SS1.SSS2.p2">
<p class="ltx_p" id="S10.SS1.SSS2.p2.1">Motivated by supporting the Blind or Low Vision community (BLV) - also known as People with Visual Impairements (PVI) - Ramaprasad et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib199" title="">199</a>]</cite> developed a two-step method to create natural language descriptions of comic strips. This method first extracts information about panels, characters, and text using computer vision techniques, followed by the use of a multimodal large language model (MLLM) to generate descriptions.
This work stands as one of the few promoting MLLM specifically for comic strip captioning, despite using “out of the box” architectures like Grounding DINO <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib40" title="">40</a>]</cite> to extract arbitrary elements, CLIP <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib14" title="">14</a>]</cite> to match character images with name-descriptions, and LLaVa <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib200" title="">200</a>]</cite> as MLLM.</p>
</div>
<div class="ltx_para" id="S10.SS1.SSS2.p3">
<p class="ltx_p" id="S10.SS1.SSS2.p3.1">Vivoli et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib72" title="">72</a>]</cite> proposed an advanced version of the text-cloze task with image-to-text generation, increasing the challenge level. They adopted the VL-T5 model <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib16" title="">16</a>]</cite> using two configurations with similar performances but different model sizes: with BLIP-2 visual backbone (1.3B) or a ResNet-50 fine-tuned with SimCLR on comics (0.2B). They surpassed previous state of the art in the standard “text-cloze” task, and propose benchmarks in the generation version of the task.
Authors in <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib77" title="">77</a>]</cite> presented a similar task but incorporated character descriptions and transcriptions, emphasizing a language model-centric approach. Guo et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib201" title="">201</a>]</cite> introduced the multi-modal manga complement task. This innovative task combines visual sequences from comic pages with corresponding text dialogues, challenging the model to complete the narrative appropriately. They propose an effective method with CLIP as Feature Encoding, a custom cross-attention module called Fine-grained Visual Prompt Generation, and a transformer-based encoder-decoder architecture called Dialog complement.
In a recent work <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite>, authors proposed a Relationformer-based architecture that generates dialog for manga pages. Their approach includes detection, matching, and sorting algorithms to produce an ordered dialog that aligns with the visual narrative.
However, all these works are limited by the dialog appearing in the comic. No work has explored producing a multi-page description of the scene, the actions, and the surrounding environment of every single panel.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S10.SS1.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">10.1.3 </span>Text-to-Image generation</h4>
<div class="ltx_para" id="S10.SS1.SSS3.p1">
<p class="ltx_p" id="S10.SS1.SSS3.p1.1"><span class="ltx_text ltx_font_bold" id="S10.SS1.SSS3.p1.1.1">Definition:</span> The Text-to-Image task comprehends generating an image, or a sequence of images, from a text description.</p>
</div>
<div class="ltx_para" id="S10.SS1.SSS3.p2">
<p class="ltx_p" id="S10.SS1.SSS3.p2.1">In the field of Text-to-Image generation, one of the earliest endeavors was by Jin et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib202" title="">202</a>]</cite>, who successfully created anime-style character faces using a GAN trained on a curated dataset of anime faces, showcasing the potential of GANs in generating stylized images. Inoue et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib68" title="">68</a>]</cite> introduced novel methods for domain transfer and pseudo-labeling in image generation, marking significant advancements in the area.
StoryGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib203" title="">203</a>]</cite> emerged as a groundbreaking initiative in story visualization, employing a sequential conditional GAN framework equipped with a context encoder to dynamically track story flow and a story-level discriminator. Enhancements to StoryGAN have included a dual learning framework to bolster semantic consistency between stories and images by refining captions <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib204" title="">204</a>]</cite> and a recursive architecture to handle structured text inputs via constituency parsing trees <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib205" title="">205</a>]</cite>. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib206" title="">206</a>]</cite>, a deep learning pipeline was proposed for generating synthetic graphic novels using GPT-2 for text and StyleGAN2 for image synthesis, trained on a large collection of manga pages. Proven-Bessel et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib207" title="">207</a>]</cite> developed a text-to-image GAN for generating single-panel comics in the style of Dilbert, showcasing new possibilities for comic generation from text. Another approach involved FastGAN <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib208" title="">208</a>]</cite>, enhanced with a condition vector for generating high-quality images from small datasets, such as manga faces drawn from Osamu Tezuka’s works <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib209" title="">209</a>]</cite>, which showed improved FID scores compared to the original FastGAN.
In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib210" title="">210</a>]</cite>, a multilingual text-to-image model was developed for creating webtoons, demonstrating the adaptability of GANs to multilingual contexts. However, despite various training experiments, the webtoon fine-tuned GAN underperformed compared to fine-tuning Korean MSCOCO, with the latter still producing abstract images that noticeably differed from authentic comics.</p>
</div>
<div class="ltx_para" id="S10.SS1.SSS3.p3">
<p class="ltx_p" id="S10.SS1.SSS3.p3.1">Significant progress has also been observed in story visualization through diffusion models. Innovations like StoryDALL-E <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib211" title="">211</a>]</cite> employ autoregressive Transformers for extensive model tuning and adaptive adjustments.
Everaert et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib212" title="">212</a>]</cite> discovered the ability to adapt Stable Diffusion to various styles, including comics, by adjusting the initial latent tensor, showing how diffusion models can contribute to comic generation. Despite diffusion models, both on pixel and on latent space, being not good at generating text, the comic panels generated are in line with COMIC data distribution.
The topic of <span class="ltx_text ltx_font_italic" id="S10.SS1.SSS3.p3.1.1">story visualization</span> as a subfield of <span class="ltx_text ltx_font_italic" id="S10.SS1.SSS3.p3.1.2">text-to-image generation</span> is very active, moving also towards integration of more powerful LLMs <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib213" title="">213</a>]</cite>, with the main challenges being the consistency of characters and style across multiple images (panels).</p>
</div>
<div class="ltx_para" id="S10.SS1.SSS3.p4">
<p class="ltx_p" id="S10.SS1.SSS3.p4.1">Lastly, as we saw previously for cross-modal retrieval tasks, using existing models as building blocks to automatically annotate new data is a good option, especially when the task doesn’t exist or annotations are expensive. In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib214" title="">214</a>]</cite>, authors used an LLM (ChatGPT) to generate storylines and dialogue of a “possible continuation” of the well-known manga <span class="ltx_text ltx_font_italic" id="S10.SS1.SSS3.p4.1.1">One Piece</span>. Then, they fine-tuned stable diffusion with LoRA and ControlNet to adapt to the manga layout, colorization, and character aesthetic. They finally generated the comic using the fine-tuned stable diffusion, rendered it in panels, and added the LLM dialogs to balloons and narration boxes.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S10.SS1.SSS4">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">10.1.4 </span>Sound Generation</h4>
<div class="ltx_para" id="S10.SS1.SSS4.p1">
<p class="ltx_p" id="S10.SS1.SSS4.p1.1"><span class="ltx_text ltx_font_bold" id="S10.SS1.SSS4.p1.1.1">Definition:</span> Sound generation in the comics domain stands for generating sound effects for a single panel, or in the transaction from one panel to another.</p>
</div>
<div class="ltx_para" id="S10.SS1.SSS4.p2">
<p class="ltx_p" id="S10.SS1.SSS4.p2.1">In <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib215" title="">215</a>]</cite>, authors proposed a system that automatically narrates the input comic page of “Clubhouse Rascals” by synthesizing realistic speeches for each character. The synthesized speeches carry the identity properties (gender and age) and emotional conditions (e.g., happy, sad, angry) of the characters inferred from the comic input. They perform a series of subsequential steps including detection of panels, speech balloons, text, character, and character’s face, as well as character re-identification, text-character association, and dialog generation. They classify every character’s face with gender, age, and emotional state, and generate the comic speech synthesis given all this information as input to a waveform reconstruction. They used the Manga109 dataset for all experiments.</p>
</div>
<div class="ltx_para" id="S10.SS1.SSS4.p3">
<p class="ltx_p" id="S10.SS1.SSS4.p3.1">In a later study <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib216" title="">216</a>]</cite>, authors explored whether having scene description and sound effects could improve the user experience of comics for People with Visual Impairments (PVI), discovering that both types are welcomed with the first (scene description) helping fr concentration and understanding of the situation, and the second one (sound effect) perceived as more immersive and realistic book-reading experience.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S10.SS2">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.2 </span><span class="ltx_text ltx_font_italic" id="S10.SS2.2.1">Synthesis</span>
</h3>
<div class="ltx_para" id="S10.SS2.p1">
<p class="ltx_p" id="S10.SS2.p1.1">The group of Synthesis tasks, compared to the “generation” one, refers to the creation of more complex, structured, and often temporally extended outputs, such as creating a full graphic novel from a complex storyline or merging multiple elements (characters, scenes, dialogues) in a consistent way across multiple pages.</p>
</div>
<figure class="ltx_figure" id="S10.SS2.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_centering ltx_img_landscape" height="181" id="S10.SS2.1.g1" src="x15.png" width="348"/>
</figure>
<section class="ltx_subsubsection" id="S10.SS2.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">10.2.1 </span>3D generation from images</h4>
<div class="ltx_para" id="S10.SS2.SSS1.p1">
<p class="ltx_p" id="S10.SS2.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S10.SS2.SSS1.p1.1.1">Definition:</span> The task of <span class="ltx_text ltx_font_italic" id="S10.SS2.SSS1.p1.1.2">generating 3D models from 2D comic illustrations</span> presents unique challenges. Unlike conventional portrait illustrations, comics often involve stylized and exaggerated features, adding complexity to this already intricate process.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS1.p2">
<p class="ltx_p" id="S10.SS2.SSS1.p2.1">While there are advancements in using 3D character assets for tasks like pose estimation <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib217" title="">217</a>]</cite>, re-targetting <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib218" title="">218</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib219" title="">219</a>]</cite>, and character reposing <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib220" title="">220</a>]</cite>, the field lacks scalable training resources due to the unavailability of suitable 3D character assets. In light of these issues, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib221" title="">221</a>]</cite> addressed this gap by formalizing the stylized reconstruction task with the introduction of the <span class="ltx_text ltx_font_italic" id="S10.SS2.SSS1.p2.1.1">AnimeRecon</span> benchmark and the <span class="ltx_text ltx_font_italic" id="S10.SS2.SSS1.p2.1.2">Vroid</span> dataset. The Vroid dataset, in particular, provides a wealth of 3D assets for scalable training in this domain. A significant part of their work involves solving the challenge of contour removal from illustrations, which is a crucial step in achieving accurate 3D reconstruction from stylized comic images.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS1.p3">
<p class="ltx_p" id="S10.SS2.SSS1.p3.1">While there are numerous works on 3D reconstruction from line drawings in general <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib222" title="">222</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib223" title="">223</a>]</cite>, the specific application in comics is underexplored, likely due to the absence of suitable datasets and ground truth references for comics.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS1.p4">
<p class="ltx_p" id="S10.SS2.SSS1.p4.1">It is worth noting that the exploration of 3D reconstruction from comic images holds great potential. It not only enriches the visual experience of comics but also opens up possibilities for animations, interactive media, and virtual reality applications within the comic domain.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S10.SS2.SSS2">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">10.2.2 </span>Video generation</h4>
<div class="ltx_para" id="S10.SS2.SSS2.p1">
<p class="ltx_p" id="S10.SS2.SSS2.p1.1"><span class="ltx_text ltx_font_bold" id="S10.SS2.SSS2.p1.1.1">Definition:</span> Video generation in comic refers to the <span class="ltx_text ltx_font_italic" id="S10.SS2.SSS2.p1.1.2">synthesis of video content from static comic panels</span>. This process represents a significant leap in merging traditional comic art with dynamic multimedia.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS2.p2">
<p class="ltx_p" id="S10.SS2.SSS2.p2.1">Recent advancements have seen significant progress in generating videos from both images <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib224" title="">224</a>, <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib225" title="">225</a>]</cite> and text <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib226" title="">226</a>]</cite>, although accurately defining metrics and benchmarks remains challenging <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib79" title="">79</a>]</cite>. These developments are particularly notable in the broader Vision and Language community, showcasing the fast-paced evolution of the field.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS2.p3">
<p class="ltx_p" id="S10.SS2.SSS2.p3.1">The domain of comics, however, is still in the early stages of exploring video generation capabilities. In one of the initial attempts, Cao et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib227" title="">227</a>]</cite> introduced a framework to animate manga panels. This method involves classifying panels based on motion and emotion and using these classifications to animate backgrounds and characters. The final video creation considers both intra-panel animations and transitions between panels.
Later, Gupta et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib228" title="">228</a>]</cite> further advanced this concept with the Comic-to-Video Network (C2VNet). This framework creates panel-audio videos by cropping comic panels and adding a highlighting effect to speech balloons, simulating characters’ dialogues. Alongside this, they introduced the IMCDB, an annotated dataset of Indian Mythological Comics in English, facilitating research in this area.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS2.p4">
<p class="ltx_p" id="S10.SS2.SSS2.p4.1">While these efforts mark initial steps, they still fall short of the more advanced capabilities seen in general Vision-Language video generation, such as producing fully animated video clips with sound from a single image. The potential for integrating comics with video and audio elements presents an exciting opportunity for enhancing storytelling and audience engagement in the comic medium.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS2.p5">
<p class="ltx_p" id="S10.SS2.SSS2.p5.1">Future research in this area could lead to new forms of interactive and immersive comic experiences, blending traditional art with contemporary digital technologies.</p>
</div>
</section>
<section class="ltx_subsubsection" id="S10.SS2.SSS3">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">10.2.3 </span>Narrative-based Comic Generation</h4>
<div class="ltx_para" id="S10.SS2.SSS3.p1">
<p class="ltx_p" id="S10.SS2.SSS3.p1.1"><span class="ltx_text ltx_font_bold" id="S10.SS2.SSS3.p1.1.1">Definition:</span> The task involves generating a comic (whether a panel, strip, or page) from a plain text description. The term “Narrative-based” highlights that the text not only describes the scene but also conveys the story’s narration.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS3.p2">
<p class="ltx_p" id="S10.SS2.SSS3.p2.1">The pioneering efforts in the realm of narrative-based comic generation commenced with the introduction of StoryGAN by Li et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib229" title="">229</a>]</cite>, which marked a significant advancement in generating image sequences from storylines using a sequential conditional GAN framework. This innovation incorporated a Context Encoder, which dynamically tracked the story flow, laying the groundwork for future developments in this area.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS3.p3">
<p class="ltx_p" id="S10.SS2.SSS3.p3.1">Building on this foundation, Maharana et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib230" title="">230</a>]</cite> enhanced visual quality and coherence by incorporating a dual learning framework that utilized video captioning, a copy-transform mechanism, and MART-based transformers. This approach significantly improved the semantic alignment between the story and generated images, thus enriching the story visualization process.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS3.p4">
<p class="ltx_p" id="S10.SS2.SSS3.p4.1">In terms of character coherence and continuity, Chen et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib231" title="">231</a>]</cite> focused on maintaining consistent character portrayal throughout the narrative. They adapted Vector-Quantized Variational Autoencoders (VQ-VAE) with a text-to-visual-token architecture, ensuring character coherence in the visual narratives. Furthering this direction, Maharana et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib211" title="">211</a>]</cite> introduced the concept of story continuation, where the generated visual story is conditioned on a source image, allowing narratives to incorporate new characters more fluidly. They also introduced the DiDeMoSV dataset, which provided a platform for exploring these complex narrative structures.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS3.p5">
<p class="ltx_p" id="S10.SS2.SSS3.p5.1">Recent advancements in story visualization are exemplified by Pan et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib232" title="">232</a>]</cite>, with presenting AR-LDM, a latent diffusion model that significantly raised the standards for visual quality in natural image datasets like VIST. This model was auto-regressively conditioned on history captions and generated images, capturing complex interactions between frames. Rahman et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib233" title="">233</a>]</cite> introduced a framework with a visual memory module that implicitly captured actor and background context, generating frames that were not only of high visual quality but also consistent with the story. Peng et al. <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib234" title="">234</a>]</cite> proposed PCSG, a diffusion-based text-to-image synthesis framework with controllable plugins for character consistency, scene layout specification, and character pose specification, further enhancing the personalized aspects of story visualization.</p>
</div>
<div class="ltx_para" id="S10.SS2.SSS3.p6">
<p class="ltx_p" id="S10.SS2.SSS3.p6.1">These emerging trends and future directions in narrative-based comic generation showcase a notable shift towards more sophisticated, context-aware, and visually coherent storytelling techniques. The integration of elements like character consistency, scene dynamics, and actor-background interaction is pivotal in creating compelling visual narratives. Future research in this area can further refine these techniques, potentially leading to automated comic creation tools that adapt to various narrative styles and complexities, opening new horizons in the field of comics and visual storytelling.</p>
</div>
</section>
</section>
<section class="ltx_subsection" id="S10.SS3">
<h3 class="ltx_title ltx_title_subsection">
<span class="ltx_tag ltx_tag_subsection">10.3 </span><span class="ltx_text ltx_font_italic" id="S10.SS3.1.1">Future Tasks</span>
</h3>
<section class="ltx_subsubsection" id="S10.SS3.SSS1">
<h4 class="ltx_title ltx_title_subsubsection">
<span class="ltx_tag ltx_tag_subsubsection">10.3.1 </span>Comics to Scene graph</h4>
<div class="ltx_para" id="S10.SS3.SSS1.p1">
<p class="ltx_p" id="S10.SS3.SSS1.p1.1"><span class="ltx_text ltx_font_bold" id="S10.SS3.SSS1.p1.1.1">Definition:</span> Transforming comics into scene graphs is an emerging area of research. This task involves dissecting comic panels to identify and represent the relationships between different elements within the panel in a graph format.</p>
</div>
<div class="ltx_para" id="S10.SS3.SSS1.p2">
<p class="ltx_p" id="S10.SS3.SSS1.p2.1">The last work that tackles this task is <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib41" title="">41</a>]</cite>, which employs a Relationformer architecture <cite class="ltx_cite ltx_citemacro_cite">[<a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#bib.bib145" title="">145</a>]</cite>, explicitly designed for image-to-graph tasks. However, the authors convert the full page into different types of nodes (panels, text, characters) and only consider two types of arches: characters-characters (re-identification) and text-characters (speaker identification). The inherently complexity of the comics, such as the elements in the image (e.g. the background and foreground objects) have not been considered yet. The task has yet to be explored to fine-grained graph creation from single panels, full pages, or a book.</p>
</div>
</section>
</section>
</section>
<section class="ltx_section" id="S11">
<h2 class="ltx_title ltx_title_section">
<span class="ltx_tag ltx_tag_section">11 </span><span class="ltx_text ltx_font_smallcaps" id="S11.1.1">CONCLUSION</span>
</h2>
<div class="ltx_para" id="S11.p1">
<p class="ltx_p" id="S11.p1.1">In reviewing the diverse and innovative efforts in the realm of comic research and technology, it becomes evident that this field is in a state of dynamic and exciting evolution. The array of tasks and applications that researchers have embarked upon reflects not only the complexity of comics as a medium but also their immense potential as a bridge between artistic expression and technological advancement.</p>
</div>
<div class="ltx_para" id="S11.p2">
<p class="ltx_p" id="S11.p2.1">We have covered numerous works and categorized them into distinct layers based on a proposed taxonomy. We have aimed to distill the key topics, methods, challenges, and emerging directions that can shape future research in Comics Understanding.</p>
</div>
<div class="ltx_para" id="S11.p3">
<p class="ltx_p" id="S11.p3.1">In the future, we will continue to monitor advancements in this field, and we will update our findings in the following repository: <a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/emanuelevivoli/awesome-comics-understanding" title="">https://github.com/emanuelevivoli/awesome-comics-understanding</a>.</p>
</div>
</section>
<section class="ltx_bibliography" id="bib">
<h2 class="ltx_title ltx_title_bibliography" style="font-size:90%;">References</h2>
<ul class="ltx_biblist">
<li class="ltx_bibitem" id="bib.bib1">
<span class="ltx_tag ltx_tag_bibitem">[1]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.1.1" style="font-size:90%;">
S. McCloud.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.2.1" style="font-size:90%;">Understanding Comics: The Invisible Art.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib1.3.1" style="font-size:90%;">41(1):66–69.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib2">
<span class="ltx_tag ltx_tag_bibitem">[2]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.1.1" style="font-size:90%;">
E. Vivoli et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib2.2.1" style="font-size:90%;">Comics datasets framework: Mix of comics datasets for detection benchmarking, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib3">
<span class="ltx_tag ltx_tag_bibitem">[3]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.1.1" style="font-size:90%;">
D. Bhattacharjee and B. Aydemir.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib3.2.1" style="font-size:90%;">Eccv workshop: Ai for visual art.
</span>
</span>
<span class="ltx_bibblock"><a class="ltx_ref ltx_url ltx_font_typewriter" href="https://github.com/IVRL/AI4VA" style="font-size:90%;" title="">https://github.com/IVRL/AI4VA</a><span class="ltx_text" id="bib.bib3.3.1" style="font-size:90%;">, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib4">
<span class="ltx_tag ltx_tag_bibitem">[4]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.1.1" style="font-size:90%;">
J. Baek et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.2.1" style="font-size:90%;">COO/ Comic Onomatopoeia Dataset for Recognizing Arbitrary or Truncated Texts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib4.3.1" style="font-size:90%;">arXiv.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib5">
<span class="ltx_tag ltx_tag_bibitem">[5]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.1.1" style="font-size:90%;">
Y. Li et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.2.1" style="font-size:90%;">Dual loss for manga character recognition with imbalanced training data.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib5.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib5.4.2" style="font-size:90%;">Proc. Int. Conf. Pattern Recognit.</span><span class="ltx_text" id="bib.bib5.5.3" style="font-size:90%;">, pp. 2166–2171. Institute of Electrical and Electronics Engineers Inc.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib6">
<span class="ltx_tag ltx_tag_bibitem">[6]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.1.1" style="font-size:90%;">
O. Augereau et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib6.2.1" style="font-size:90%;">A Survey of Comics Research in Computer Science.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib6.3.1" style="font-size:90%;">Journal of Imaging</span><span class="ltx_text" id="bib.bib6.4.2" style="font-size:90%;">, 4(7):87, July 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib7">
<span class="ltx_tag ltx_tag_bibitem">[7]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.1.1" style="font-size:90%;">
D. Bhattacharjee et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.2.1" style="font-size:90%;">Estimating Image Depth in the Comics Domain.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib7.3.1" style="font-size:90%;">arXiv.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib8">
<span class="ltx_tag ltx_tag_bibitem">[8]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.1.1" style="font-size:90%;">
M. Iyyer et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.2.1" style="font-size:90%;">The Amazing Mysteries of the Gutter: Drawing Inferences Between Panels in Comic Book Narratives.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib8.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib8.4.2" style="font-size:90%;">Proc. - IEEE Conf. Comput. Vis. Pattern Recognit, CVPR</span><span class="ltx_text" id="bib.bib8.5.3" style="font-size:90%;">, volume 2017-January, pp. 6478–6487. arXiv, May 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib9">
<span class="ltx_tag ltx_tag_bibitem">[9]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.1.1" style="font-size:90%;">
H. Liu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib9.2.1" style="font-size:90%;">Improved baselines with visual instruction tuning, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib10">
<span class="ltx_tag ltx_tag_bibitem">[10]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.1.1" style="font-size:90%;">
B. Li et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib10.2.1" style="font-size:90%;">Llava-next: Stronger llms supercharge multimodal capabilities in the wild, May 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib11">
<span class="ltx_tag ltx_tag_bibitem">[11]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.1.1" style="font-size:90%;">
L. Xue et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib11.2.1" style="font-size:90%;">xgen-mm (blip-3): A family of open large multimodal models, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib12">
<span class="ltx_tag ltx_tag_bibitem">[12]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.1.1" style="font-size:90%;">
H. Laurençon et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib12.2.1" style="font-size:90%;">Building and better understanding vision-language models: insights and future directions, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib13">
<span class="ltx_tag ltx_tag_bibitem">[13]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.1.1" style="font-size:90%;">
Y. Yao et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib13.2.1" style="font-size:90%;">Minicpm-v: A gpt-4v level mllm on your phone, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib14">
<span class="ltx_tag ltx_tag_bibitem">[14]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.1.1" style="font-size:90%;">
A. Radford et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib14.2.1" style="font-size:90%;">Learning Transferable Visual Models From Natural Language Supervision.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib15">
<span class="ltx_tag ltx_tag_bibitem">[15]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.1.1" style="font-size:90%;">
C. Jia et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib15.2.1" style="font-size:90%;">Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib16">
<span class="ltx_tag ltx_tag_bibitem">[16]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.1.1" style="font-size:90%;">
J. Cho et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib16.2.1" style="font-size:90%;">Unifying Vision-and-Language Tasks via Text Generation.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib17">
<span class="ltx_tag ltx_tag_bibitem">[17]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.1.1" style="font-size:90%;">
J. Li et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib17.2.1" style="font-size:90%;">BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib18">
<span class="ltx_tag ltx_tag_bibitem">[18]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.1.1" style="font-size:90%;">
A. Singh et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib18.2.1" style="font-size:90%;">FLAVA: A Foundational Language And Vision Alignment Model.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib19">
<span class="ltx_tag ltx_tag_bibitem">[19]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.1.1" style="font-size:90%;">
D. Zhu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib19.2.1" style="font-size:90%;">MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib20">
<span class="ltx_tag ltx_tag_bibitem">[20]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.1.1" style="font-size:90%;">
J. Chen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib20.2.1" style="font-size:90%;">MiniGPT-v2: Large language model as a unified interface for vision-language multi-task learning.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib21">
<span class="ltx_tag ltx_tag_bibitem">[21]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.1.1" style="font-size:90%;">
W. Wang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib21.2.1" style="font-size:90%;">CogVLM: Visual Expert for Pretrained Language Models.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib22">
<span class="ltx_tag ltx_tag_bibitem">[22]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.1.1" style="font-size:90%;">
R. Girdhar et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib22.2.1" style="font-size:90%;">ImageBind: One Embedding Space To Bind Them All.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib23">
<span class="ltx_tag ltx_tag_bibitem">[23]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.1.1" style="font-size:90%;">
B. Zhu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib23.2.1" style="font-size:90%;">LanguageBind: Extending Video-Language Pretraining to N-modality by Language-based Semantic Alignment.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib24">
<span class="ltx_tag ltx_tag_bibitem">[24]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.1.1" style="font-size:90%;">
M. Yasunaga et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib24.2.1" style="font-size:90%;">Retrieval-Augmented Multimodal Language Modeling.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib25">
<span class="ltx_tag ltx_tag_bibitem">[25]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.1.1" style="font-size:90%;">
R. Gal et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib25.2.1" style="font-size:90%;">An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib26">
<span class="ltx_tag ltx_tag_bibitem">[26]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.1.1" style="font-size:90%;">
L. Yang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib26.2.1" style="font-size:90%;">Diffusion Models: A Comprehensive Survey of Methods and Applications.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib27">
<span class="ltx_tag ltx_tag_bibitem">[27]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.1.1" style="font-size:90%;">
N. Cohn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.2.1" style="font-size:90%;">Visual Narrative Structure.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib27.3.1" style="font-size:90%;">37(3):413–452.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib28">
<span class="ltx_tag ltx_tag_bibitem">[28]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.1.1" style="font-size:90%;">
Y.-F. Chou and Z.-C. Shih.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.2.1" style="font-size:90%;">Comic character animation using Bayesian estimation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib28.3.1" style="font-size:90%;">22(5):457–470.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib29">
<span class="ltx_tag ltx_tag_bibitem">[29]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.1.1" style="font-size:90%;">
S. A. Amirshahi et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib29.2.1" style="font-size:90%;">Phog analysis of self-similarity in esthetic images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib29.3.1" style="font-size:90%;">Proc SPIE</span><span class="ltx_text" id="bib.bib29.4.2" style="font-size:90%;">, 8291:46–, 02 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib30">
<span class="ltx_tag ltx_tag_bibitem">[30]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.1.1" style="font-size:90%;">
W. Sun and K. Kise.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib30.2.1" style="font-size:90%;">Detection of exact and similar partial copies for copyright protection of manga.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib30.3.1" style="font-size:90%;">International Journal on Document Analysis and Recognition</span><span class="ltx_text" id="bib.bib30.4.2" style="font-size:90%;">, 16(4):331–349, December 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib31">
<span class="ltx_tag ltx_tag_bibitem">[31]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.1.1" style="font-size:90%;">
W.-T. Chu and Y.-C. Chao.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.2.1" style="font-size:90%;">Line-based drawing style description for manga classification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib31.3.1" style="font-size:90%;">pp. 781–784, 11 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib32">
<span class="ltx_tag ltx_tag_bibitem">[32]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.1.1" style="font-size:90%;">
Y. Matsui et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.2.1" style="font-size:90%;">Sketch2Manga: Sketch-based manga retrieval.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib32.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib32.4.2" style="font-size:90%;">IEEE Int. Conf. Image Process., ICIP</span><span class="ltx_text" id="bib.bib32.5.3" style="font-size:90%;">, pp. 3097–3101. Institute of Electrical and Electronics Engineers Inc., 2014.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib33">
<span class="ltx_tag ltx_tag_bibitem">[33]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.1.1" style="font-size:90%;">
C. Rigaud et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.2.1" style="font-size:90%;">An Active Contour Model for Speech Balloon Detection in Comics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib33.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib33.4.2" style="font-size:90%;">2013 12th International Conference on Document Analysis and Recognition</span><span class="ltx_text" id="bib.bib33.5.3" style="font-size:90%;">, pp. 1240–1244, August 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib34">
<span class="ltx_tag ltx_tag_bibitem">[34]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.1.1" style="font-size:90%;">
X. Qin et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.2.1" style="font-size:90%;">A Faster R-CNN Based Method for Comic Characters Face Detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib34.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib34.4.2" style="font-size:90%;">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</span><span class="ltx_text" id="bib.bib34.5.3" style="font-size:90%;">, volume 01, pp. 1074–1080, November 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib35">
<span class="ltx_tag ltx_tag_bibitem">[35]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.1.1" style="font-size:90%;">
T. Ogawa et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.2.1" style="font-size:90%;">Object Detection for Comics using Manga109 Annotations.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib35.3.1" style="font-size:90%;">arXiv.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib36">
<span class="ltx_tag ltx_tag_bibitem">[36]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.1.1" style="font-size:90%;">
D. Dubray and J. Laubrock.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.2.1" style="font-size:90%;">Deep CNN-based Speech Balloon Detection and Segmentation for Comic Books.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib36.3.1" style="font-size:90%;">arXiv.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib37">
<span class="ltx_tag ltx_tag_bibitem">[37]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.1.1" style="font-size:90%;">
A. Dutta et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib37.2.1" style="font-size:90%;">CNN-based segmentation of speech balloons and narrative text boxes from comic book page images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib37.3.1" style="font-size:90%;">International Journal on Document Analysis and Recognition (IJDAR)</span><span class="ltx_text" id="bib.bib37.4.2" style="font-size:90%;">, 24(1-2):49–62, February 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib38">
<span class="ltx_tag ltx_tag_bibitem">[38]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.1.1" style="font-size:90%;">
N.-V. Nguyen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.2.1" style="font-size:90%;">A learning approach with incomplete pixel-level labels for deep neural networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib38.3.1" style="font-size:90%;">130:111–125.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib39">
<span class="ltx_tag ltx_tag_bibitem">[39]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.1.1" style="font-size:90%;">
M. Oquab et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib39.2.1" style="font-size:90%;">DINOv2: Learning Robust Visual Features without Supervision.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib40">
<span class="ltx_tag ltx_tag_bibitem">[40]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.1.1" style="font-size:90%;">
S. Liu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib40.2.1" style="font-size:90%;">Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib41">
<span class="ltx_tag ltx_tag_bibitem">[41]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.1.1" style="font-size:90%;">
R. Sachdeva and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib41.2.1" style="font-size:90%;">The Manga Whisperer: Automatically Generating Transcriptions for Comics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib42">
<span class="ltx_tag ltx_tag_bibitem">[42]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.1.1" style="font-size:90%;">
E. Vivoli et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib42.2.1" style="font-size:90%;">Comix: A comprehensive benchmark for multi-task comic understanding, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib43">
<span class="ltx_tag ltx_tag_bibitem">[43]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.1.1" style="font-size:90%;">
R. Sachdeva et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib43.2.1" style="font-size:90%;">Tails tell tales: Chapter-wide manga transcriptions with character names, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib44">
<span class="ltx_tag ltx_tag_bibitem">[44]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.1.1" style="font-size:90%;">
I. Santos et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.2.1" style="font-size:90%;">Artificial Neural Networks and Deep Learning in the Visual Arts: A review.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib44.3.1" style="font-size:90%;">33(1):121–157.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib45">
<span class="ltx_tag ltx_tag_bibitem">[45]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.1.1" style="font-size:90%;">
O. Augereau et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.2.1" style="font-size:90%;">An Overview of Comics Research in Computer Science.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib45.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib45.4.2" style="font-size:90%;">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</span><span class="ltx_text" id="bib.bib45.5.3" style="font-size:90%;">, pp. 54–59, November 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib46">
<span class="ltx_tag ltx_tag_bibitem">[46]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib46.1.1" style="font-size:90%;">
M. Nairat.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib46.2.1" style="font-size:90%;">Generative Comics - A Computational Approach to Creating Comics Material</span><span class="ltx_text" id="bib.bib46.3.2" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib47">
<span class="ltx_tag ltx_tag_bibitem">[47]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.1.1" style="font-size:90%;">
N. Cohn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib47.2.1" style="font-size:90%;">The Visual Language Research Corpus (VLRC) Project.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib48">
<span class="ltx_tag ltx_tag_bibitem">[48]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.1.1" style="font-size:90%;">
N. Cohn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.2.1" style="font-size:90%;">Beyond speech balloons and thought bubbles: The integration of text and image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib48.3.1" style="font-size:90%;">2013(197):35–63.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib49">
<span class="ltx_tag ltx_tag_bibitem">[49]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.1.1" style="font-size:90%;">
N. Cohn et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.2.1" style="font-size:90%;">The cognition of comics: What “comics” can tell us about the mind.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib49.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib49.4.2" style="font-size:90%;">Proc. Annu. Meet. Cogn. Sci. Soc., CogSci</span><span class="ltx_text" id="bib.bib49.5.3" style="font-size:90%;">, pp. 66–67. The Cognitive Science Society.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib50">
<span class="ltx_tag ltx_tag_bibitem">[50]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.1.1" style="font-size:90%;">
N. Cohn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.2.1" style="font-size:90%;">The architecture of visual narrative comprehension: The interaction of narrative structure and page layout in understanding comics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib50.3.1" style="font-size:90%;">5.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib51">
<span class="ltx_tag ltx_tag_bibitem">[51]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.1.1" style="font-size:90%;">
N. Cohn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.2.1" style="font-size:90%;">A multimodal parallel architecture: A cognitive framework for multimodal interactions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib51.3.1" style="font-size:90%;">146:304–323.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib52">
<span class="ltx_tag ltx_tag_bibitem">[52]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.1.1" style="font-size:90%;">
N. Cohn et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.2.1" style="font-size:90%;">A Picture is Worth More Words Over Time: Multimodality and Narrative Structure Across Eight Decades of American Superhero Comics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib52.3.1" style="font-size:90%;">6(1):19–37.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib53">
<span class="ltx_tag ltx_tag_bibitem">[53]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.1.1" style="font-size:90%;">
N. Cohn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib53.2.1" style="font-size:90%;">Who Understands Comics?: Questioning the Universality of Visual Language Comprehension</span><span class="ltx_text" id="bib.bib53.3.2" style="font-size:90%;">.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib53.4.1" style="font-size:90%;">Bloomsbury Academic.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib54">
<span class="ltx_tag ltx_tag_bibitem">[54]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.1.1" style="font-size:90%;">
B. Klomberg et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.2.1" style="font-size:90%;">Running through the Who, Where, and When: A Cross-cultural Analysis of Situational Changes in Comics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib54.3.1" style="font-size:90%;">59(9):669–684.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib55">
<span class="ltx_tag ltx_tag_bibitem">[55]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.1.1" style="font-size:90%;">
N. Cohn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.2.1" style="font-size:90%;">A different kind of cultural frame: An analysis of panels in American comics and Japanese manga.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib55.3.1" style="font-size:90%;">12(1):120–134.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib56">
<span class="ltx_tag ltx_tag_bibitem">[56]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.1.1" style="font-size:90%;">
N. Cohn and S. Ehly.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.2.1" style="font-size:90%;">The vocabulary of manga: Visual morphology in dialects of Japanese Visual Language.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib56.3.1" style="font-size:90%;">92:17–29.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib57">
<span class="ltx_tag ltx_tag_bibitem">[57]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.1.1" style="font-size:90%;">
C. Guérin et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.2.1" style="font-size:90%;">eBDtheque: A representative database of comics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib57.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib57.4.2" style="font-size:90%;">2013 12th International Conference on Document Analysis and Recognition</span><span class="ltx_text" id="bib.bib57.5.3" style="font-size:90%;">, pp. 1145–1149, August 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib58">
<span class="ltx_tag ltx_tag_bibitem">[58]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.1.1" style="font-size:90%;">
N.-V. Nguyen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib58.2.1" style="font-size:90%;">Digital Comics Image Indexing Based on Deep Learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib58.3.1" style="font-size:90%;">Journal of Imaging</span><span class="ltx_text" id="bib.bib58.4.2" style="font-size:90%;">, 4(7):89, July 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib59">
<span class="ltx_tag ltx_tag_bibitem">[59]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.1.1" style="font-size:90%;">
N.-V. Nguyen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.2.1" style="font-size:90%;">Comic characters detection using deep learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib59.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib59.4.2" style="font-size:90%;">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</span><span class="ltx_text" id="bib.bib59.5.3" style="font-size:90%;">, volume 3, pp. 41–46. IEEE Computer Society, November 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib60">
<span class="ltx_tag ltx_tag_bibitem">[60]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.1.1" style="font-size:90%;">
M. J. Wilber et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib60.2.1" style="font-size:90%;">BAM! The Behance Artistic Media Dataset for Recognition Beyond Photography.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib61">
<span class="ltx_tag ltx_tag_bibitem">[61]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.1.1" style="font-size:90%;">
A. Fujimoto et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.2.1" style="font-size:90%;">Manga109 dataset and creation of metadata.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib61.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib61.4.2" style="font-size:90%;">Proceedings of the 1st International Workshop on coMics ANalysis, Processing and Understanding (Manpu)</span><span class="ltx_text" id="bib.bib61.5.3" style="font-size:90%;">, pp. 1–5, December 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib62">
<span class="ltx_tag ltx_tag_bibitem">[62]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.1.1" style="font-size:90%;">
N.-V. Nguyen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.2.1" style="font-size:90%;">ICDAR 2021 Competition on Multimodal Emotion Recognition on Comics Scenes.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib62.3.1" style="font-size:90%;">In J. Lladós et al., editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib62.4.2" style="font-size:90%;">Document Analysis and Recognition – ICDAR 2021</span><span class="ltx_text" id="bib.bib62.5.3" style="font-size:90%;">, volume 12824 of </span><span class="ltx_text ltx_font_italic" id="bib.bib62.6.4" style="font-size:90%;">Lecture Notes in Computer Science</span><span class="ltx_text" id="bib.bib62.7.5" style="font-size:90%;">, pp. 767–782, Cham, 2021. Springer International Publishing.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib63">
<span class="ltx_tag ltx_tag_bibitem">[63]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.1.1" style="font-size:90%;">
F. S. Khan et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.2.1" style="font-size:90%;">Color attributes for object detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib63.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib63.4.2" style="font-size:90%;">2012 IEEE Conference on Computer Vision and Pattern Recognition</span><span class="ltx_text" id="bib.bib63.5.3" style="font-size:90%;">, pp. 3306–3313.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib64">
<span class="ltx_tag ltx_tag_bibitem">[64]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.1.1" style="font-size:90%;">
W. Sun et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.2.1" style="font-size:90%;">Specific comic character detection using local feature matching.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib64.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib64.4.2" style="font-size:90%;">2013 12th International Conference on Document Analysis and Recognition</span><span class="ltx_text" id="bib.bib64.5.3" style="font-size:90%;">, pp. 275–279, August 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib65">
<span class="ltx_tag ltx_tag_bibitem">[65]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.1.1" style="font-size:90%;">
A. Dunst et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.2.1" style="font-size:90%;">The Graphic Narrative Corpus (GNC): Design, Annotation, and Analysis for the Digital Humanities.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib65.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib65.4.2" style="font-size:90%;">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</span><span class="ltx_text" id="bib.bib65.5.3" style="font-size:90%;">, volume 03, pp. 15–20, November 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib66">
<span class="ltx_tag ltx_tag_bibitem">[66]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.1.1" style="font-size:90%;">
T. N. Le et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib66.2.1" style="font-size:90%;">Subgraph spotting in graph representations of comic book images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib66.3.1" style="font-size:90%;">Pattern Recognition Letters</span><span class="ltx_text" id="bib.bib66.4.2" style="font-size:90%;">, 112:118–124, September 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib67">
<span class="ltx_tag ltx_tag_bibitem">[67]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.1.1" style="font-size:90%;">
Z. He et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.2.1" style="font-size:90%;">An End-to-End Quadrilateral Regression Network for Comic Panel Extraction.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib67.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib67.4.2" style="font-size:90%;">Proceedings of the 26th ACM International Conference on Multimedia</span><span class="ltx_text" id="bib.bib67.5.3" style="font-size:90%;">, MM ’18, pp. 887–895. Association for Computing Machinery.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib68">
<span class="ltx_tag ltx_tag_bibitem">[68]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.1.1" style="font-size:90%;">
N. Inoue et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib68.2.1" style="font-size:90%;">Cross-Domain Weakly-Supervised Object Detection through Progressive Domain Adaptation.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib69">
<span class="ltx_tag ltx_tag_bibitem">[69]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.1.1" style="font-size:90%;">
A. Dutta et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib69.2.1" style="font-size:90%;">BCBId: First Bangla comic dataset and its applications.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib69.3.1" style="font-size:90%;">International Journal on Document Analysis and Recognition (IJDAR)</span><span class="ltx_text" id="bib.bib69.4.2" style="font-size:90%;">, 25(4):265–279, December 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib70">
<span class="ltx_tag ltx_tag_bibitem">[70]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.1.1" style="font-size:90%;">
G. Soykan et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib70.2.1" style="font-size:90%;">A Comprehensive Gold Standard and Benchmark for Comics Text Detection and Recognition.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib71">
<span class="ltx_tag ltx_tag_bibitem">[71]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.1.1" style="font-size:90%;">
H. N. Ho et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.2.1" style="font-size:90%;">Redundant structure detection in attributed adjacency graphs for character detection in comics books.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib71.3.1" style="font-size:90%;">08 2013.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib72">
<span class="ltx_tag ltx_tag_bibitem">[72]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.1.1" style="font-size:90%;">
E. Vivoli et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib72.2.1" style="font-size:90%;">Multimodal transformer for comics text-cloze, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib73">
<span class="ltx_tag ltx_tag_bibitem">[73]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.1.1" style="font-size:90%;">
Y. Li et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.2.1" style="font-size:90%;">Manga109Dialog A Large-scale Dialogue Dataset for Comics Speaker Detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib73.3.1" style="font-size:90%;">arXiv.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib74">
<span class="ltx_tag ltx_tag_bibitem">[74]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.1.1" style="font-size:90%;">
OpenAI et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib74.2.1" style="font-size:90%;">Gpt-4 technical report, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib75">
<span class="ltx_tag ltx_tag_bibitem">[75]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.1.1" style="font-size:90%;">
Z. He et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.2.1" style="font-size:90%;">SReN: Shape Regression Network for Comic Storyboard Extraction.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib75.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib75.4.2" style="font-size:90%;">Proceedings of the AAAI Conference on Artificial Intelligence</span><span class="ltx_text" id="bib.bib75.5.3" style="font-size:90%;">, volume 31.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib76">
<span class="ltx_tag ltx_tag_bibitem">[76]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.1.1" style="font-size:90%;">
C. Rigaud.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.2.1" style="font-size:90%;">Segmentation and indexation of complex objects in comic book.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib76.3.1" style="font-size:90%;">14(3).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib77">
<span class="ltx_tag ltx_tag_bibitem">[77]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.1.1" style="font-size:90%;">
H. Agrawal et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.2.1" style="font-size:90%;">Multimodal Persona Based Generation of Comic Dialogs.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib77.3.1" style="font-size:90%;">In A. Rogers et al., editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib77.4.2" style="font-size:90%;">Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</span><span class="ltx_text" id="bib.bib77.5.3" style="font-size:90%;">, pp. 14150–14164. Association for Computational Linguistics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib78">
<span class="ltx_tag ltx_tag_bibitem">[78]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.1.1" style="font-size:90%;">
X. Yue et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib78.2.1" style="font-size:90%;">MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib79">
<span class="ltx_tag ltx_tag_bibitem">[79]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.1.1" style="font-size:90%;">
F. Fan et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib79.2.1" style="font-size:90%;">AIGCBench: Comprehensive Evaluation of Image-to-Video Content Generated by AI.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib80">
<span class="ltx_tag ltx_tag_bibitem">[80]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.1.1" style="font-size:90%;">
C. Fu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib80.2.1" style="font-size:90%;">MME: A Comprehensive Evaluation Benchmark for Multimodal Large Language Models.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib81">
<span class="ltx_tag ltx_tag_bibitem">[81]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.1.1" style="font-size:90%;">
T. Baltrušaitis et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib81.2.1" style="font-size:90%;">Multimodal Machine Learning: A Survey and Taxonomy.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib82">
<span class="ltx_tag ltx_tag_bibitem">[82]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.1.1" style="font-size:90%;">
F. Li et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib82.2.1" style="font-size:90%;">Vision-Language Intelligence: Tasks, Representation Learning, and Large Models.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib83">
<span class="ltx_tag ltx_tag_bibitem">[83]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.1.1" style="font-size:90%;">
S. Hiroe and S. Hotta.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.2.1" style="font-size:90%;">Histogram of Exclamation Marks and Its Application for Comics Analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib83.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib83.4.2" style="font-size:90%;">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</span><span class="ltx_text" id="bib.bib83.5.3" style="font-size:90%;">, pp. 66–71, November 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib84">
<span class="ltx_tag ltx_tag_bibitem">[84]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.1.1" style="font-size:90%;">
Y. Daiku et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.2.1" style="font-size:90%;">Comic Story Analysis Based on Genre Classification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib84.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib84.4.2" style="font-size:90%;">Proc. Int. Conf. Doc. Anal. Recognit.</span><span class="ltx_text" id="bib.bib84.5.3" style="font-size:90%;">, volume 3, pp. 60–65. IEEE Computer Society.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib85">
<span class="ltx_tag ltx_tag_bibitem">[85]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.1.1" style="font-size:90%;">
S. Jiang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.2.1" style="font-size:90%;">Learning Consensus Representation for Weak Style Classification.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib85.3.1" style="font-size:90%;">40(12):2906–2919.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib86">
<span class="ltx_tag ltx_tag_bibitem">[86]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.1.1" style="font-size:90%;">
A. Terauchi et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.2.1" style="font-size:90%;">Analysis based on distributed representations of various parts images in four-scene comics story dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib86.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib86.4.2" style="font-size:90%;">Int. Conf. Doc. Anal. Recog. Workshops, ICDARW</span><span class="ltx_text" id="bib.bib86.5.3" style="font-size:90%;">, volume 1, pp. 50–55. Institute of Electrical and Electronics Engineers Inc.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib87">
<span class="ltx_tag ltx_tag_bibitem">[87]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.1.1" style="font-size:90%;">
C. Xu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.2.1" style="font-size:90%;">Panel-Page-Aware Comic Genre Understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib87.3.1" style="font-size:90%;">32:2636–2648.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib88">
<span class="ltx_tag ltx_tag_bibitem">[88]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.1.1" style="font-size:90%;">
H. Tanaka et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.2.1" style="font-size:90%;">Relation Analysis between Speech Balloon Shapes and their Serif Descriptions in Comic.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib88.3.1" style="font-size:90%;">In Hirokawa S. et al., editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib88.4.2" style="font-size:90%;">Proc. - IIAI Int. Congr. Adv. Appl. Inform., IIAI-AAI</span><span class="ltx_text" id="bib.bib88.5.3" style="font-size:90%;">, pp. 229–233. Institute of Electrical and Electronics Engineers Inc.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib89">
<span class="ltx_tag ltx_tag_bibitem">[89]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.1.1" style="font-size:90%;">
M. Tan and Q. V. Le.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib89.2.1" style="font-size:90%;">EfficientNet: Rethinking Model Scaling for Convolutional Neural Networks.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib90">
<span class="ltx_tag ltx_tag_bibitem">[90]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.1.1" style="font-size:90%;">
Y. Liu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib90.2.1" style="font-size:90%;">RoBERTa: A Robustly Optimized BERT Pretraining Approach.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib91">
<span class="ltx_tag ltx_tag_bibitem">[91]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.1.1" style="font-size:90%;">
C. L. Sanches et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.2.1" style="font-size:90%;">Manga content analysis using physiological signals.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib91.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib91.4.2" style="font-size:90%;">Proceedings of the 1st International Workshop on CoMics ANalysis, Processing and Understanding</span><span class="ltx_text" id="bib.bib91.5.3" style="font-size:90%;">, MANPU ’16, New York, NY, USA, 2016. Association for Computing Machinery.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib92">
<span class="ltx_tag ltx_tag_bibitem">[92]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.1.1" style="font-size:90%;">
YT. Yang and WT. Chu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.2.1" style="font-size:90%;">Manga Text Detection with Manga-Specific Data Augmentation and Its Applications on Emotion Analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib92.3.1" style="font-size:90%;">In DT. Dang-Nguyen et al., editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib92.4.2" style="font-size:90%;">National Cheng Kung University</span><span class="ltx_text" id="bib.bib92.5.3" style="font-size:90%;">, volume 13834, pp. 29–40, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib93">
<span class="ltx_tag ltx_tag_bibitem">[93]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib93.1.1" style="font-size:90%;">
Y. Zhu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib93.2.1" style="font-size:90%;">A Comprehensive Study of Deep Video Action Recognition.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib94">
<span class="ltx_tag ltx_tag_bibitem">[94]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.1.1" style="font-size:90%;">
A. Gordo et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.2.1" style="font-size:90%;">Document Classification and Page Stream Segmentation for Digital Mailroom Applications.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib94.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib94.4.2" style="font-size:90%;">2013 12th International Conference on Document Analysis and Recognition</span><span class="ltx_text" id="bib.bib94.5.3" style="font-size:90%;">, pp. 621–625. IEEE.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib95">
<span class="ltx_tag ltx_tag_bibitem">[95]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.1.1" style="font-size:90%;">
G. Wiedemann and G. Heyer.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.2.1" style="font-size:90%;">Page Stream Segmentation with Convolutional Neural Nets Combining Textual and Visual Features.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib95.3.1" style="font-size:90%;">In N. Calzolari et al., editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib95.4.2" style="font-size:90%;">Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018)</span><span class="ltx_text" id="bib.bib95.5.3" style="font-size:90%;">. European Language Resources Association (ELRA).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib96">
<span class="ltx_tag ltx_tag_bibitem">[96]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib96.1.1" style="font-size:90%;">
M. A. Demirtaş et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib96.2.1" style="font-size:90%;">Semantic Parsing of Interpage Relations.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib97">
<span class="ltx_tag ltx_tag_bibitem">[97]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib97.1.1" style="font-size:90%;">
C.-Y. Yao et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib97.2.1" style="font-size:90%;">Screentone-aware manga super-resolution using deeplearning, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib98">
<span class="ltx_tag ltx_tag_bibitem">[98]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.1.1" style="font-size:90%;">
Y. Dai et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib98.2.1" style="font-size:90%;">Structured Fusion Attention Network for Image Super-Resolution Reconstruction.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib98.3.1" style="font-size:90%;">IEEE Access</span><span class="ltx_text" id="bib.bib98.4.2" style="font-size:90%;">, 10:31896–31906, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib99">
<span class="ltx_tag ltx_tag_bibitem">[99]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib99.1.1" style="font-size:90%;">
L. Zhang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib99.2.1" style="font-size:90%;">Generating manga from illustrations via mimicking manga creation workflow.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib99.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib99.4.2" style="font-size:90%;">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib99.5.3" style="font-size:90%;">, pp. 5638–5647, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib100">
<span class="ltx_tag ltx_tag_bibitem">[100]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib100.1.1" style="font-size:90%;">
B. B. Topal et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib100.2.1" style="font-size:90%;">DASS-Detector: Domain-Adaptive Self-Supervised Pre-Training for Face &amp; Body Detection in Drawings.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib101">
<span class="ltx_tag ltx_tag_bibitem">[101]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib101.1.1" style="font-size:90%;">
J. Deng et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib101.2.1" style="font-size:90%;">Unbiased Mean Teacher for Cross-domain Object Detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib101.3.1" style="font-size:90%;">2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib101.4.2" style="font-size:90%;">, pp. 4089–4099, June 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib102">
<span class="ltx_tag ltx_tag_bibitem">[102]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib102.1.1" style="font-size:90%;">
C. Furusawa et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib102.2.1" style="font-size:90%;">Comicolorization: Semi-automatic manga colorization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib102.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib102.4.2" style="font-size:90%;">SIGGRAPH Asia Tech. Briefs, SA</span><span class="ltx_text" id="bib.bib102.5.3" style="font-size:90%;">. Association for Computing Machinery, Inc.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib103">
<span class="ltx_tag ltx_tag_bibitem">[103]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib103.1.1" style="font-size:90%;">
P. Hensman and K. Aizawa.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib103.2.1" style="font-size:90%;">CGAN-Based Manga Colorization Using a Single Training Image.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib103.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib103.4.2" style="font-size:90%;">Proc. Int. Conf. Doc. Anal. Recognit.</span><span class="ltx_text" id="bib.bib103.5.3" style="font-size:90%;">, volume 3, pp. 72–77. IEEE Computer Society.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib104">
<span class="ltx_tag ltx_tag_bibitem">[104]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib104.1.1" style="font-size:90%;">
K. Tsubota et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib104.2.1" style="font-size:90%;">Synthesis of Screentone Patterns of Manga Characters.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib104.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib104.4.2" style="font-size:90%;">Proc. - IEEE Int. Symp. Multimed., ISM</span><span class="ltx_text" id="bib.bib104.5.3" style="font-size:90%;">, pp. 212–215. Institute of Electrical and Electronics Engineers Inc., 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib105">
<span class="ltx_tag ltx_tag_bibitem">[105]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib105.1.1" style="font-size:90%;">
P. Isola et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib105.2.1" style="font-size:90%;">Image-to-Image Translation with Conditional Adversarial Networks.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib106">
<span class="ltx_tag ltx_tag_bibitem">[106]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib106.1.1" style="font-size:90%;">
C. Li et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib106.2.1" style="font-size:90%;">Deep extraction of manga structural lines.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib106.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib106.4.2" style="font-size:90%;">ACM Trans Graphics</span><span class="ltx_text" id="bib.bib106.5.3" style="font-size:90%;">, volume 36. Association for Computing Machinery.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib107">
<span class="ltx_tag ltx_tag_bibitem">[107]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib107.1.1" style="font-size:90%;">
M. Golyadkin and I. Makarov.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib107.2.1" style="font-size:90%;">Robust manga page colorization via coloring latent space.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib108">
<span class="ltx_tag ltx_tag_bibitem">[108]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib108.1.1" style="font-size:90%;">
E. Richardson et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib108.2.1" style="font-size:90%;">Encoding in Style: A StyleGAN Encoder for Image-to-Image Translation.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib109">
<span class="ltx_tag ltx_tag_bibitem">[109]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib109.1.1" style="font-size:90%;">
T. Jiramahapokee.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib109.2.1" style="font-size:90%;">Inkn’hue: Enhancing manga colorization from multiple priors with alignment multi-encoder VAE.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib110">
<span class="ltx_tag ltx_tag_bibitem">[110]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib110.1.1" style="font-size:90%;">
M. Zeyen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib110.2.1" style="font-size:90%;">Color Interpolation for Non-Euclidean Color Spaces.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib110.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib110.4.2" style="font-size:90%;">2018 IEEE Scientific Visualization Conference (SciVis)</span><span class="ltx_text" id="bib.bib110.5.3" style="font-size:90%;">, pp. 11–15.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib111">
<span class="ltx_tag ltx_tag_bibitem">[111]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib111.1.1" style="font-size:90%;">
H. Wu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib111.2.1" style="font-size:90%;">Shading-guided Manga Screening from Reference.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib111.3.1" style="font-size:90%;">IEEE Transactions on Visualization and Computer Graphics</span><span class="ltx_text" id="bib.bib111.4.2" style="font-size:90%;">, pp. 1–15, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib112">
<span class="ltx_tag ltx_tag_bibitem">[112]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib112.1.1" style="font-size:90%;">
Z. Liu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib112.2.1" style="font-size:90%;">Swin Transformer: Hierarchical Vision Transformer using Shifted Windows.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib113">
<span class="ltx_tag ltx_tag_bibitem">[113]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib113.1.1" style="font-size:90%;">
J. Kopf and D. Lischinski.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib113.2.1" style="font-size:90%;">Depixelizing pixel art.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib113.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib113.4.2" style="font-size:90%;">ACM SIGGRAPH 2011 Papers</span><span class="ltx_text" id="bib.bib113.5.3" style="font-size:90%;">, SIGGRAPH ’11, pp. 1–8. Association for Computing Machinery.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib114">
<span class="ltx_tag ltx_tag_bibitem">[114]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib114.1.1" style="font-size:90%;">
Adobe Illustrator - Industry-leading vector graphics software.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib115">
<span class="ltx_tag ltx_tag_bibitem">[115]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib115.1.1" style="font-size:90%;">
P. Selinger.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib115.2.1" style="font-size:90%;">Potrace : A polygon-based tracing algorithm.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib116">
<span class="ltx_tag ltx_tag_bibitem">[116]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib116.1.1" style="font-size:90%;">
X. Ma et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib116.2.1" style="font-size:90%;">Towards Layer-wise Image Vectorization.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib117">
<span class="ltx_tag ltx_tag_bibitem">[117]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib117.1.1" style="font-size:90%;">
C.-Y. Yao et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib117.2.1" style="font-size:90%;">Manga Vectorization and Manipulation with Procedural Simple Screentone.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib117.3.1" style="font-size:90%;">23(2):1070–1084.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib118">
<span class="ltx_tag ltx_tag_bibitem">[118]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib118.1.1" style="font-size:90%;">
H. Su et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib118.2.1" style="font-size:90%;">MARVEL: Raster Gray-level Manga Vectorization via Primitive-wise Deep Reinforcement Learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib118.3.1" style="font-size:90%;">pp. 1–1.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib119">
<span class="ltx_tag ltx_tag_bibitem">[119]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib119.1.1" style="font-size:90%;">
D. Bhattacharjee et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib119.2.1" style="font-size:90%;">Dense Multitask Learning to Reconfigure Comics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib119.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib119.4.2" style="font-size:90%;">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</span><span class="ltx_text" id="bib.bib119.5.3" style="font-size:90%;">, pp. 5646–5655.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib120">
<span class="ltx_tag ltx_tag_bibitem">[120]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib120.1.1" style="font-size:90%;">
D. Bhattacharjee et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib120.2.1" style="font-size:90%;">MulT: An End-to-End Multitask Learning Transformer.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib121">
<span class="ltx_tag ltx_tag_bibitem">[121]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib121.1.1" style="font-size:90%;">
A. Garai et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib121.2.1" style="font-size:90%;">Automatic dewarping of camera-captured comic document images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib121.3.1" style="font-size:90%;">Multimedia Tools and Applications</span><span class="ltx_text" id="bib.bib121.4.2" style="font-size:90%;">, 82(1):1537–1552, January 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib122">
<span class="ltx_tag ltx_tag_bibitem">[122]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib122.1.1" style="font-size:90%;">
C. Ponsard.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib122.2.1" style="font-size:90%;">Enhancing the Accessibility for All of Digital Comic Books.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib123">
<span class="ltx_tag ltx_tag_bibitem">[123]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib123.1.1" style="font-size:90%;">
K. Arai and T. Herman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib123.2.1" style="font-size:90%;">Method for automatic e-comic scene frame extraction for reading comic on mobile devices.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib124">
<span class="ltx_tag ltx_tag_bibitem">[124]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib124.1.1" style="font-size:90%;">
K. Arai and H. Tolle.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib124.2.1" style="font-size:90%;">Method for Real Time Text Extraction of Digital Manga Comic.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib125">
<span class="ltx_tag ltx_tag_bibitem">[125]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib125.1.1" style="font-size:90%;">
C. Y. Su et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib125.2.1" style="font-size:90%;">Recognizing Text Elements for SVG Comic Compression and Its Novel Applications.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib125.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib125.4.2" style="font-size:90%;">2011 International Conference on Document Analysis and Recognition</span><span class="ltx_text" id="bib.bib125.5.3" style="font-size:90%;">, pp. 1329–1333.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib126">
<span class="ltx_tag ltx_tag_bibitem">[126]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib126.1.1" style="font-size:90%;">
A. K. N. Ho et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib126.2.1" style="font-size:90%;">Panel and Speech Balloon Extraction from Comic Books.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib126.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib126.4.2" style="font-size:90%;">2012 10th IAPR International Workshop on Document Analysis Systems</span><span class="ltx_text" id="bib.bib126.5.3" style="font-size:90%;">, pp. 424–428, March 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib127">
<span class="ltx_tag ltx_tag_bibitem">[127]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib127.1.1" style="font-size:90%;">
H. S. Jomaa et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib127.2.1" style="font-size:90%;">Panel tracking for the extraction and the classification of speech balloons.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib127.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib127.4.2" style="font-size:90%;">Image Analysis and Processing — ICIAP 2015</span><span class="ltx_text" id="bib.bib127.5.3" style="font-size:90%;">, pp. 394–405, Berlin, Heidelberg, 2022. Springer-Verlag.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib128">
<span class="ltx_tag ltx_tag_bibitem">[128]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib128.1.1" style="font-size:90%;">
X. Liu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib128.2.1" style="font-size:90%;">Text-aware balloon extraction from manga.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib128.3.1" style="font-size:90%;">32(4):501–511.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib129">
<span class="ltx_tag ltx_tag_bibitem">[129]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib129.1.1" style="font-size:90%;">
S. Pal et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib129.2.1" style="font-size:90%;">Line-wise text identification in comic books: A support vector machine-based approach.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib129.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib129.4.2" style="font-size:90%;">Proc Int Jt Conf Neural Networks</span><span class="ltx_text" id="bib.bib129.5.3" style="font-size:90%;">, volume 2016-October, pp. 3995–4000. Institute of Electrical and Electronics Engineers Inc.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib130">
<span class="ltx_tag ltx_tag_bibitem">[130]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib130.1.1" style="font-size:90%;">
Y. Aramaki et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib130.2.1" style="font-size:90%;">Text detection in manga by combining connected-component-based and region-based classifications.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib130.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib130.4.2" style="font-size:90%;">2016 IEEE International Conference on Image Processing (ICIP)</span><span class="ltx_text" id="bib.bib130.5.3" style="font-size:90%;">, pp. 2901–2905, September 2016.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib131">
<span class="ltx_tag ltx_tag_bibitem">[131]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib131.1.1" style="font-size:90%;">
W.-T. Chu and C.-C. Yu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib131.2.1" style="font-size:90%;">Text Detection in Manga by Deep Region Proposal, Classification, and Regression.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib131.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib131.4.2" style="font-size:90%;">VCIP - IEEE Int. Conf. Visual Commun. Image Process.</span><span class="ltx_text" id="bib.bib131.5.3" style="font-size:90%;"> Institute of Electrical and Electronics Engineers Inc., 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib132">
<span class="ltx_tag ltx_tag_bibitem">[132]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib132.1.1" style="font-size:90%;">
H. Yanagisawa et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib132.2.1" style="font-size:90%;">A study on object detection method from manga images using CNN.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib132.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib132.4.2" style="font-size:90%;">2018 International Workshop on Advanced Image Technology (IWAIT)</span><span class="ltx_text" id="bib.bib132.5.3" style="font-size:90%;">, pp. 1–4.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib133">
<span class="ltx_tag ltx_tag_bibitem">[133]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib133.1.1" style="font-size:90%;">
V. Nguyen Nhu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib133.2.1" style="font-size:90%;">What do We Expect from Comic Panel Extraction?
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib133.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib133.4.2" style="font-size:90%;">2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)</span><span class="ltx_text" id="bib.bib133.5.3" style="font-size:90%;">, volume 1, pp. 44–49, September 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib134">
<span class="ltx_tag ltx_tag_bibitem">[134]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib134.1.1" style="font-size:90%;">
R. Hu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib134.2.1" style="font-size:90%;">Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib135">
<span class="ltx_tag ltx_tag_bibitem">[135]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib135.1.1" style="font-size:90%;">
R. Sharma and V. Kukreja.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib135.2.1" style="font-size:90%;">CPD: Faster RCNN-based DragonBall Comic Panel Detection.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib135.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib135.4.2" style="font-size:90%;">2023 IEEE 12th International Conference on Communication Systems and Network Technologies (CSNT)</span><span class="ltx_text" id="bib.bib135.5.3" style="font-size:90%;">, pp. 786–790.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib136">
<span class="ltx_tag ltx_tag_bibitem">[136]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib136.1.1" style="font-size:90%;">
A. Dutta and S. Biswas.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib136.2.1" style="font-size:90%;">CNN Based Extraction of Panels/Characters from Bengali Comic Book Page Images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib136.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib136.4.2" style="font-size:90%;">2019 International Conference on Document Analysis and Recognition Workshops (ICDARW)</span><span class="ltx_text" id="bib.bib136.5.3" style="font-size:90%;">, pp. 38–43.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib137">
<span class="ltx_tag ltx_tag_bibitem">[137]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib137.1.1" style="font-size:90%;">
M. Iwata et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib137.2.1" style="font-size:90%;">A study to achieve Manga character retrieval method for Manga images.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib137.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib137.4.2" style="font-size:90%;">Proc. - IAPR Int. Workshop Doc. Anal. Syst., DAS</span><span class="ltx_text" id="bib.bib137.5.3" style="font-size:90%;">, pp. 309–313. IEEE Computer Society.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib138">
<span class="ltx_tag ltx_tag_bibitem">[138]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib138.1.1" style="font-size:90%;">
W. Sun and K. Kise.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib138.2.1" style="font-size:90%;">Similar manga retrieval using visual vocabulary based on regions of interest.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib138.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib138.4.2" style="font-size:90%;">Proc. Int. Conf. Doc. Anal. Recognit.</span><span class="ltx_text" id="bib.bib138.5.3" style="font-size:90%;">, pp. 1075–1079, 2011.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib139">
<span class="ltx_tag ltx_tag_bibitem">[139]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib139.1.1" style="font-size:90%;">
M. Iwata et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib139.2.1" style="font-size:90%;">Similarity learning based on pool-based active learning for manga character retrieval.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib139.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib139.4.2" style="font-size:90%;">2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)</span><span class="ltx_text" id="bib.bib139.5.3" style="font-size:90%;">, pp. 437–442.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib140">
<span class="ltx_tag ltx_tag_bibitem">[140]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib140.1.1" style="font-size:90%;">
O. Nir et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib140.2.1" style="font-size:90%;">CAST: Character labeling in Animation using Self‐supervision by Tracking.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib140.3.1" style="font-size:90%;">41(2):135–145.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib141">
<span class="ltx_tag ltx_tag_bibitem">[141]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib141.1.1" style="font-size:90%;">
Z. Zhang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib141.2.1" style="font-size:90%;">Unsupervised manga character re-identification via face-body and spatial-temporal associated clustering.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib142">
<span class="ltx_tag ltx_tag_bibitem">[142]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib142.1.1" style="font-size:90%;">
C.-Y. Zhang and W.-T. Chu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib142.2.1" style="font-size:90%;">Occlusion-aware manga character re-identification with self-paced contrastive learning.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib142.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib142.4.2" style="font-size:90%;">Proceedings of the 5th ACM International Conference on Multimedia in Asia</span><span class="ltx_text" id="bib.bib142.5.3" style="font-size:90%;">, MMAsia ’23, New York, NY, USA, 2024. Association for Computing Machinery.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib143">
<span class="ltx_tag ltx_tag_bibitem">[143]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib143.1.1" style="font-size:90%;">
G. Soykan et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib143.2.1" style="font-size:90%;">Identity-aware semi-supervised learning for comic character re-identification.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib144">
<span class="ltx_tag ltx_tag_bibitem">[144]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib144.1.1" style="font-size:90%;">
T. Chen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib144.2.1" style="font-size:90%;">A Simple Framework for Contrastive Learning of Visual Representations.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib145">
<span class="ltx_tag ltx_tag_bibitem">[145]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib145.1.1" style="font-size:90%;">
S. Shit et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib145.2.1" style="font-size:90%;">Relationformer: A Unified Framework for Image-to-Graph Generation.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib146">
<span class="ltx_tag ltx_tag_bibitem">[146]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib146.1.1" style="font-size:90%;">
T. Ahmad and M. Schich.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib146.2.1" style="font-size:90%;">Toward cross-domain object detection in artwork images using improved YoloV5 and XGBoosting.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib146.3.1" style="font-size:90%;">17(8):2437–2449.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib147">
<span class="ltx_tag ltx_tag_bibitem">[147]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib147.1.1" style="font-size:90%;">
Z. Li et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib147.2.1" style="font-size:90%;">Zero-shot object detection with textual descriptions.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib147.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib147.4.2" style="font-size:90%;">AAAI Conference on Artificial Intelligence</span><span class="ltx_text" id="bib.bib147.5.3" style="font-size:90%;">, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib148">
<span class="ltx_tag ltx_tag_bibitem">[148]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib148.1.1" style="font-size:90%;">
L. Parcalabescu and A. Frank.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib148.2.1" style="font-size:90%;">Exploring phrase grounding without training: Contextualisation and extension to text-based image retrieval.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib148.3.1" style="font-size:90%;">2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)</span><span class="ltx_text" id="bib.bib148.4.2" style="font-size:90%;">, pp. 4137–4146, 2020.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib149">
<span class="ltx_tag ltx_tag_bibitem">[149]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib149.1.1" style="font-size:90%;">
H. Shen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib149.2.1" style="font-size:90%;">Groundvlp: Harnessing zero-shot visual grounding from vision-language pre-training and open-vocabulary object detection, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib150">
<span class="ltx_tag ltx_tag_bibitem">[150]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib150.1.1" style="font-size:90%;">
C. Rigaud et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib150.2.1" style="font-size:90%;">Speech balloon and speaker association for comics and manga understanding.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib150.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib150.4.2" style="font-size:90%;">2015 13th International Conference on Document Analysis and Recognition (ICDAR)</span><span class="ltx_text" id="bib.bib150.5.3" style="font-size:90%;">, pp. 351–355.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib151">
<span class="ltx_tag ltx_tag_bibitem">[151]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib151.1.1" style="font-size:90%;">
N.-V. Nguyen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib151.2.1" style="font-size:90%;">Comic MTL: Optimized multi-task learning for comic book image analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib151.3.1" style="font-size:90%;">International Journal on Document Analysis and Recognition (IJDAR)</span><span class="ltx_text" id="bib.bib151.4.2" style="font-size:90%;">, 22(3):265–284, September 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib152">
<span class="ltx_tag ltx_tag_bibitem">[152]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib152.1.1" style="font-size:90%;">
Y. Omori et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib152.2.1" style="font-size:90%;">Algorithms for estimation of comic speakers considering reading order of frames and texts.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib152.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib152.4.2" style="font-size:90%;">2022 12th International Congress on Advanced Applied Informatics (IIAI-AAI)</span><span class="ltx_text" id="bib.bib152.5.3" style="font-size:90%;">, pp. 367–372.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib153">
<span class="ltx_tag ltx_tag_bibitem">[153]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib153.1.1" style="font-size:90%;">
M. Ueno and H. Isahara.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib153.2.1" style="font-size:90%;">Story Pattern Analysis Based on Scene Order Information in Four-Scene Comics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib153.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib153.4.2" style="font-size:90%;">Proc. Int. Conf. Doc. Anal. Recognit.</span><span class="ltx_text" id="bib.bib153.5.3" style="font-size:90%;">, volume 3, pp. 78–83. IEEE Computer Society.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib154">
<span class="ltx_tag ltx_tag_bibitem">[154]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib154.1.1" style="font-size:90%;">
Y. Wu.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib154.2.1" style="font-size:90%;">Searching Digital Political Cartoons.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib154.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib154.4.2" style="font-size:90%;">2010 IEEE International Conference on Granular Computing</span><span class="ltx_text" id="bib.bib154.5.3" style="font-size:90%;">, pp. 541–545.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib155">
<span class="ltx_tag ltx_tag_bibitem">[155]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib155.1.1" style="font-size:90%;">
Y. Matsui et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib155.2.1" style="font-size:90%;">Sketch-based manga retrieval using manga109 dataset.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib155.3.1" style="font-size:90%;">Multimedia Tools and Applications</span><span class="ltx_text" id="bib.bib155.4.2" style="font-size:90%;">, 76(20):21811–21838, 2017.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib156">
<span class="ltx_tag ltx_tag_bibitem">[156]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib156.1.1" style="font-size:90%;">
R. Narita et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib156.2.1" style="font-size:90%;">Sketch-Based Manga Retrieval Using Deep Features.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib156.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib156.4.2" style="font-size:90%;">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</span><span class="ltx_text" id="bib.bib156.5.3" style="font-size:90%;">, pp. 49–53, Kyoto, November 2017. IEEE.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib157">
<span class="ltx_tag ltx_tag_bibitem">[157]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib157.1.1" style="font-size:90%;">
NV. Nguyen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib157.2.1" style="font-size:90%;">Manga-MMTL: Multimodal Multitask Transfer Learning for Manga Character Analysis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib157.3.1" style="font-size:90%;">In J. Llados et al., editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib157.4.2" style="font-size:90%;">DOCUMENT ANALYSIS AND RECOGNITION - ICDAR 2021, PT II</span><span class="ltx_text" id="bib.bib157.5.3" style="font-size:90%;">, volume 12822, pp. 410–425, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib158">
<span class="ltx_tag ltx_tag_bibitem">[158]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib158.1.1" style="font-size:90%;">
X. Wei et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib158.2.1" style="font-size:90%;">ComicLib: A new large-scale comic dataset for sketch understanding.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib159">
<span class="ltx_tag ltx_tag_bibitem">[159]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib159.1.1" style="font-size:90%;">
D. Ha and D. Eck.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib159.2.1" style="font-size:90%;">A Neural Representation of Sketch Drawings.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib160">
<span class="ltx_tag ltx_tag_bibitem">[160]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib160.1.1" style="font-size:90%;">
L. Zhang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib160.2.1" style="font-size:90%;">Style Transfer for Anime Sketches with Enhanced Residual U-net and Auxiliary Classifier GAN.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib161">
<span class="ltx_tag ltx_tag_bibitem">[161]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib161.1.1" style="font-size:90%;">
A. Radford et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib161.2.1" style="font-size:90%;">Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib162">
<span class="ltx_tag ltx_tag_bibitem">[162]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib162.1.1" style="font-size:90%;">
S. Ren et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib162.2.1" style="font-size:90%;">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib163">
<span class="ltx_tag ltx_tag_bibitem">[163]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib163.1.1" style="font-size:90%;">
J. Redmon et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib163.2.1" style="font-size:90%;">YOLO: You Only Look Once: Unified, Real-Time Object Detection.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib164">
<span class="ltx_tag ltx_tag_bibitem">[164]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib164.1.1" style="font-size:90%;">
W. Liu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib164.2.1" style="font-size:90%;">SSD: Single Shot MultiBox Detector.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib164.3.1" style="font-size:90%;">volume 9905, pp. 21–37.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib165">
<span class="ltx_tag ltx_tag_bibitem">[165]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib165.1.1" style="font-size:90%;">
C. T. Shen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib165.2.1" style="font-size:90%;">Maru: A manga retrieval and understanding system connecting vision and language, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib166">
<span class="ltx_tag ltx_tag_bibitem">[166]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib166.1.1" style="font-size:90%;">
N. Carion et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib166.2.1" style="font-size:90%;">End-to-End Object Detection with Transformers.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib167">
<span class="ltx_tag ltx_tag_bibitem">[167]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib167.1.1" style="font-size:90%;">
H. Touvron et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib167.2.1" style="font-size:90%;">Training data-efficient image transformers &amp; distillation through attention.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib167.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib167.4.2" style="font-size:90%;">arXiv.Org</span><span class="ltx_text" id="bib.bib167.5.3" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib168">
<span class="ltx_tag ltx_tag_bibitem">[168]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib168.1.1" style="font-size:90%;">
V. Sanh et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib168.2.1" style="font-size:90%;">DistilBERT, a distilled version of BERT: Smaller, faster, cheaper and lighter.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib169">
<span class="ltx_tag ltx_tag_bibitem">[169]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib169.1.1" style="font-size:90%;">
S. Suh et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib169.2.1" style="font-size:90%;">CodeToon: Story Ideation, Auto Comic Generation, and Structure Mapping for Code-Driven Storytelling.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib169.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib169.4.2" style="font-size:90%;">The 35th Annual ACM Symposium on User Interface Software and Technology</span><span class="ltx_text" id="bib.bib169.5.3" style="font-size:90%;">, pp. 1–16.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib170">
<span class="ltx_tag ltx_tag_bibitem">[170]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib170.1.1" style="font-size:90%;">
Z. Wang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib170.2.1" style="font-size:90%;">Interactive Data Comics.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib170.3.1" style="font-size:90%;">IEEE Transactions on Visualization and Computer Graphics</span><span class="ltx_text" id="bib.bib170.4.2" style="font-size:90%;">, 28(1):944–954, January 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib171">
<span class="ltx_tag ltx_tag_bibitem">[171]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib171.1.1" style="font-size:90%;">
B. Korbar and A. Zisserman.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib171.2.1" style="font-size:90%;">Personalised CLIP or: How to find your vacation videos.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib172">
<span class="ltx_tag ltx_tag_bibitem">[172]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib172.1.1" style="font-size:90%;">
Y. Moriyama et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib172.2.1" style="font-size:90%;">Designing a question-answering system for comic contents.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib172.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib172.4.2" style="font-size:90%;">Proceedings of the 1st International Workshop on coMics ANalysis, Processing and Understanding MANPU</span><span class="ltx_text" id="bib.bib172.5.3" style="font-size:90%;">, MANPU ’16, pp. 1–6. Association for Computing Machinery.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib173">
<span class="ltx_tag ltx_tag_bibitem">[173]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib173.1.1" style="font-size:90%;">
Y. Sumi.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib173.2.1" style="font-size:90%;">ComicQA: Contextual navigation aid by hyper-comic representation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib173.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib173.4.2" style="font-size:90%;">Proceedings of the 19th International Conference on Information Integration and Web-based Applications &amp; Services</span><span class="ltx_text" id="bib.bib173.5.3" style="font-size:90%;">, pp. 76–84.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib174">
<span class="ltx_tag ltx_tag_bibitem">[174]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib174.1.1" style="font-size:90%;">
C. Zeng et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib174.2.1" style="font-size:90%;">A Survey on Machine Reading Comprehension—Tasks, Evaluation Metrics and Benchmark Datasets.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib174.3.1" style="font-size:90%;">10(21):7640.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib175">
<span class="ltx_tag ltx_tag_bibitem">[175]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib175.1.1" style="font-size:90%;">
P. Sahu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib175.2.1" style="font-size:90%;">Towards Solving Multimodal Comprehension.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib176">
<span class="ltx_tag ltx_tag_bibitem">[176]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib176.1.1" style="font-size:90%;">
P. Sahu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib176.2.1" style="font-size:90%;">Challenges in Procedural Multimodal Machine Comprehension: A Novel Way To Benchmark.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib176.3.1" style="font-size:90%;">2022 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV)</span><span class="ltx_text" id="bib.bib176.4.2" style="font-size:90%;">, pp. 526–535, January 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib177">
<span class="ltx_tag ltx_tag_bibitem">[177]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib177.1.1" style="font-size:90%;">
A. Singh et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib177.2.1" style="font-size:90%;">Towards vqa models that can read, 2019.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib178">
<span class="ltx_tag ltx_tag_bibitem">[178]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib178.1.1" style="font-size:90%;">
A. F. Biten et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib178.2.1" style="font-size:90%;">Scene Text Visual Question Answering.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib179">
<span class="ltx_tag ltx_tag_bibitem">[179]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib179.1.1" style="font-size:90%;">
M. Mathew et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib179.2.1" style="font-size:90%;">InfographicVQA.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib180">
<span class="ltx_tag ltx_tag_bibitem">[180]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib180.1.1" style="font-size:90%;">
E. Vivoli et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib180.2.1" style="font-size:90%;">MUST-VQA: MUltilingual Scene-text VQA.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib181">
<span class="ltx_tag ltx_tag_bibitem">[181]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib181.1.1" style="font-size:90%;">
R. Tanaka et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib181.2.1" style="font-size:90%;">SlideVQA: A Dataset for Document Visual Question Answering on Multiple Images.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib182">
<span class="ltx_tag ltx_tag_bibitem">[182]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib182.1.1" style="font-size:90%;">
M. Matsuda et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib182.2.1" style="font-size:90%;">Comic live chat communication tool based on concept of downgrading.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib183">
<span class="ltx_tag ltx_tag_bibitem">[183]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib183.1.1" style="font-size:90%;">
P. Tanapichet et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib183.2.1" style="font-size:90%;">Automatic comic strip generation using extracted keyframes from cartoon animation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib183.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib183.4.2" style="font-size:90%;">Int. Symp. Intelligent Signal Process. Commun. Syst.: ”Decade Intelligent Green Signal Process. Commun.”, ISPACS</span><span class="ltx_text" id="bib.bib183.5.3" style="font-size:90%;">.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib184">
<span class="ltx_tag ltx_tag_bibitem">[184]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib184.1.1" style="font-size:90%;">
K. Hoashi et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib184.2.1" style="font-size:90%;">Automatic preview generation of comic episodes for digitized comic search.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib185">
<span class="ltx_tag ltx_tag_bibitem">[185]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib185.1.1" style="font-size:90%;">
X. Deng et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib185.2.1" style="font-size:90%;">Research and development of the generation in Japanese manga based on frontal face image.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib186">
<span class="ltx_tag ltx_tag_bibitem">[186]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib186.1.1" style="font-size:90%;">
I. C. Chang and R. M. Cheng.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib186.2.1" style="font-size:90%;">Caricaturation for human face pictures.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib186.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib186.4.2" style="font-size:90%;">2011 International Conference on Machine Learning and Cybernetics</span><span class="ltx_text" id="bib.bib186.5.3" style="font-size:90%;">, volume 4, pp. 1702–1707.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib187">
<span class="ltx_tag ltx_tag_bibitem">[187]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib187.1.1" style="font-size:90%;">
Y. Cao et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib187.2.1" style="font-size:90%;">Automatic stylistic manga layout.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib187.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib187.4.2" style="font-size:90%;">ACM Trans Graphics</span><span class="ltx_text" id="bib.bib187.5.3" style="font-size:90%;">, volume 31.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib188">
<span class="ltx_tag ltx_tag_bibitem">[188]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib188.1.1" style="font-size:90%;">
L. Herranz et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib188.2.1" style="font-size:90%;">Scalable comic-like video summaries and layout disturbance.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib188.3.1" style="font-size:90%;">IEEE Transactions on Multimedia</span><span class="ltx_text" id="bib.bib188.4.2" style="font-size:90%;">, 14(4 PART 2):1290–1297, 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib189">
<span class="ltx_tag ltx_tag_bibitem">[189]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib189.1.1" style="font-size:90%;">
G. Jing et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib189.2.1" style="font-size:90%;">Content-Aware Video2Comics With Manga-Style Layout.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib189.3.1" style="font-size:90%;">IEEE Transactions on Multimedia</span><span class="ltx_text" id="bib.bib189.4.2" style="font-size:90%;">, 17(12):2122–2133, December 2015.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib190">
<span class="ltx_tag ltx_tag_bibitem">[190]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib190.1.1" style="font-size:90%;">
M. Wang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib190.2.1" style="font-size:90%;">Movie2Comics: Towards a Lively Video Content Presentation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib190.3.1" style="font-size:90%;">IEEE Transactions on Multimedia - TMM</span><span class="ltx_text" id="bib.bib190.4.2" style="font-size:90%;">, 14:858–870, June 2012.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib191">
<span class="ltx_tag ltx_tag_bibitem">[191]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib191.1.1" style="font-size:90%;">
M. Pesko et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib191.2.1" style="font-size:90%;">Comixify: Transform video into a comics.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib192">
<span class="ltx_tag ltx_tag_bibitem">[192]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib192.1.1" style="font-size:90%;">
X. Yang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib192.2.1" style="font-size:90%;">Automatic Comic Generation with Stylistic Multi-page Layouts and Emotion-driven Text Balloon Generation.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib193">
<span class="ltx_tag ltx_tag_bibitem">[193]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib193.1.1" style="font-size:90%;">
A. Oliva and A. Torralba.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib193.2.1" style="font-size:90%;">Modeling the Shape of the Scene: A Holistic Representation of the Spatial Envelope.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib193.3.1" style="font-size:90%;">42(3):145–175.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib194">
<span class="ltx_tag ltx_tag_bibitem">[194]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib194.1.1" style="font-size:90%;">
H. Zhang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib194.2.1" style="font-size:90%;">Augmenting Conversations With Comic-Style Word Balloons.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib194.3.1" style="font-size:90%;">IEEE Transactions on Human-Machine Systems</span><span class="ltx_text" id="bib.bib194.4.2" style="font-size:90%;">, 53(2):367–377, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib195">
<span class="ltx_tag ltx_tag_bibitem">[195]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib195.1.1" style="font-size:90%;">
S. Suh et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib195.2.1" style="font-size:90%;">Using Comics to Introduce and Reinforce Programming Concepts in CS1.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib195.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib195.4.2" style="font-size:90%;">Proceedings of the 52nd ACM Technical Symposium on Computer Science Education</span><span class="ltx_text" id="bib.bib195.5.3" style="font-size:90%;">, pp. 369–375.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib196">
<span class="ltx_tag ltx_tag_bibitem">[196]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib196.1.1" style="font-size:90%;">
A. A. Lima et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib196.2.1" style="font-size:90%;">Comics as a Pedagogical Tool for Teaching.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib196.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib196.4.2" style="font-size:90%;">2022 XVII Latin American Conference on Learning Technologies (LACLO)</span><span class="ltx_text" id="bib.bib196.5.3" style="font-size:90%;">, pp. 1–7.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib197">
<span class="ltx_tag ltx_tag_bibitem">[197]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib197.1.1" style="font-size:90%;">
F. Castro et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib197.2.1" style="font-size:90%;">Developing Comic-based Learning Toolkits for Teaching Computing to Elementary School Learners.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib197.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib197.4.2" style="font-size:90%;">SIGCSE - Proc. ACM Tech. Symp. Comput. Sci. Educ.</span><span class="ltx_text" id="bib.bib197.5.3" style="font-size:90%;">, volume 2, pp. 1325. Association for Computing Machinery, Inc.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib198">
<span class="ltx_tag ltx_tag_bibitem">[198]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib198.1.1" style="font-size:90%;">
J. Zhao et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib198.2.1" style="font-size:90%;">ChartStory: Automated Partitioning, Layout, and Captioning of Charts into Comic-Style Narratives.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib198.3.1" style="font-size:90%;">29(2):1384–1399.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib199">
<span class="ltx_tag ltx_tag_bibitem">[199]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib199.1.1" style="font-size:90%;">
R. Ramaprasad.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib199.2.1" style="font-size:90%;">Comics for everyone: Generating accessible text descriptions for comic strips.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib200">
<span class="ltx_tag ltx_tag_bibitem">[200]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib200.1.1" style="font-size:90%;">
H. Liu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib200.2.1" style="font-size:90%;">Visual Instruction Tuning.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib201">
<span class="ltx_tag ltx_tag_bibitem">[201]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib201.1.1" style="font-size:90%;">
H. Guo et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib201.2.1" style="font-size:90%;">M2C: Towards automatic multimodal manga complement.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib202">
<span class="ltx_tag ltx_tag_bibitem">[202]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib202.1.1" style="font-size:90%;">
Y. Jin et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib202.2.1" style="font-size:90%;">Towards the Automatic Anime Characters Creation with Generative Adversarial Networks.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib203">
<span class="ltx_tag ltx_tag_bibitem">[203]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib203.1.1" style="font-size:90%;">
Y. Li et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib203.2.1" style="font-size:90%;">Storygan: A sequential conditional GAN for story visualization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib203.3.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib203.4.2" style="font-size:90%;">, abs/1812.02784, 2018.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib204">
<span class="ltx_tag ltx_tag_bibitem">[204]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib204.1.1" style="font-size:90%;">
A. Maharana et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib204.2.1" style="font-size:90%;">Improving generation and evaluation of visual stories via semantic consistency.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib204.3.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib204.4.2" style="font-size:90%;">, abs/2105.10026, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib205">
<span class="ltx_tag ltx_tag_bibitem">[205]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib205.1.1" style="font-size:90%;">
A. Maharana and M. Bansal.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib205.2.1" style="font-size:90%;">Integrating visuospatial, linguistic and commonsense structure into story visualization.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib205.3.1" style="font-size:90%;">CoRR</span><span class="ltx_text" id="bib.bib205.4.2" style="font-size:90%;">, abs/2110.10834, 2021.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib206">
<span class="ltx_tag ltx_tag_bibitem">[206]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib206.1.1" style="font-size:90%;">
T. Melistas et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib206.2.1" style="font-size:90%;">A Deep Learning Pipeline for the Synthesis of Graphic Novels.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib207">
<span class="ltx_tag ltx_tag_bibitem">[207]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib207.1.1" style="font-size:90%;">
B. Proven-Bessel et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib207.2.1" style="font-size:90%;">ComicGAN: Text-to-Comic Generative Adversarial Network.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib208">
<span class="ltx_tag ltx_tag_bibitem">[208]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib208.1.1" style="font-size:90%;">
B. Liu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib208.2.1" style="font-size:90%;">Towards Faster and Stabilized GAN Training for High-fidelity Few-shot Image Synthesis.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib209">
<span class="ltx_tag ltx_tag_bibitem">[209]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib209.1.1" style="font-size:90%;">
K. Hiruta et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib209.2.1" style="font-size:90%;">Conditional GAN for Small Datasets.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib209.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib209.4.2" style="font-size:90%;">Proc. - IEEE Int. Symp. Multimed., ISM</span><span class="ltx_text" id="bib.bib209.5.3" style="font-size:90%;">, pp. 278–281. Institute of Electrical and Electronics Engineers Inc.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib210">
<span class="ltx_tag ltx_tag_bibitem">[210]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib210.1.1" style="font-size:90%;">
K. H. Yu et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib210.2.1" style="font-size:90%;">A study on generating webtoons using multilingual text-to-image models.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text ltx_font_italic" id="bib.bib210.3.1" style="font-size:90%;">Applied Sciences</span><span class="ltx_text" id="bib.bib210.4.2" style="font-size:90%;">, 2023.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib211">
<span class="ltx_tag ltx_tag_bibitem">[211]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib211.1.1" style="font-size:90%;">
A. Maharana et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib211.2.1" style="font-size:90%;">Storydall-e: Adapting pretrained text-to-image transformers for story continuation, 2022.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib212">
<span class="ltx_tag ltx_tag_bibitem">[212]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib212.1.1" style="font-size:90%;">
M. N. Everaert et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib212.2.1" style="font-size:90%;">Diffusion in Style.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib212.3.1" style="font-size:90%;">pp. 2251–2261.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib213">
<span class="ltx_tag ltx_tag_bibitem">[213]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib213.1.1" style="font-size:90%;">
H. He et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib213.2.1" style="font-size:90%;">Dreamstory: Open-domain story visualization by llm-guided multi-subject consistent diffusion, 2024.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib214">
<span class="ltx_tag ltx_tag_bibitem">[214]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib214.1.1" style="font-size:90%;">
Z. Jin and Z. Song.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib214.2.1" style="font-size:90%;">Generating coherent comic with rich story using ChatGPT and Stable Diffusion.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib215">
<span class="ltx_tag ltx_tag_bibitem">[215]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib215.1.1" style="font-size:90%;">
Y. Wang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib215.2.1" style="font-size:90%;">Comic-guided speech synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib215.3.1" style="font-size:90%;">38(6).
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib216">
<span class="ltx_tag ltx_tag_bibitem">[216]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib216.1.1" style="font-size:90%;">
Y. J. Lee et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib216.2.1" style="font-size:90%;">AccessComics2: Understanding the User Experience of an Accessible Comic Book Reader for Blind People with Textual Sound Effects.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib216.3.1" style="font-size:90%;">16(1):2:1–2:25.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib217">
<span class="ltx_tag ltx_tag_bibitem">[217]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib217.1.1" style="font-size:90%;">
J. Kim et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib217.2.1" style="font-size:90%;">U-GAT-IT: Unsupervised Generative Attentional Networks with Adaptive Layer-Instance Normalization for Image-to-Image Translation.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib218">
<span class="ltx_tag ltx_tag_bibitem">[218]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib218.1.1" style="font-size:90%;">
P. Khungurn.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib218.2.1" style="font-size:90%;">Pkhungurn/talking-head-anime-2-demo.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib219">
<span class="ltx_tag ltx_tag_bibitem">[219]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib219.1.1" style="font-size:90%;">
K. Kim et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib219.2.1" style="font-size:90%;">AnimeCeleb: Large-Scale Animation CelebHeads Dataset for Head Reenactment.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib220">
<span class="ltx_tag ltx_tag_bibitem">[220]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib220.1.1" style="font-size:90%;">
Z. Lin et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib220.2.1" style="font-size:90%;">Collaborative Neural Rendering using Anime Character Sheets.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib221">
<span class="ltx_tag ltx_tag_bibitem">[221]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib221.1.1" style="font-size:90%;">
S. Chen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib221.2.1" style="font-size:90%;">PAniC-3D: Stylized Single-view 3D Reconstruction from Portraits of Anime Characters.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib221.3.1" style="font-size:90%;">In </span><span class="ltx_text ltx_font_italic" id="bib.bib221.4.2" style="font-size:90%;">2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</span><span class="ltx_text" id="bib.bib221.5.3" style="font-size:90%;">, pp. 21068–21077. IEEE.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib222">
<span class="ltx_tag ltx_tag_bibitem">[222]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib222.1.1" style="font-size:90%;">
Z. Lun et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib222.2.1" style="font-size:90%;">3D Shape Reconstruction from Sketches via Multi-view Convolutional Networks.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib222.3.1" style="font-size:90%;">pp. 67–77.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib223">
<span class="ltx_tag ltx_tag_bibitem">[223]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib223.1.1" style="font-size:90%;">
A. Sanghi et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib223.2.1" style="font-size:90%;">Sketch-A-Shape: Zero-Shot Sketch-to-3D Shape Generation.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib224">
<span class="ltx_tag ltx_tag_bibitem">[224]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib224.1.1" style="font-size:90%;">
W. Yan et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib224.2.1" style="font-size:90%;">Motion-Conditioned Image Animation for Video Editing.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib225">
<span class="ltx_tag ltx_tag_bibitem">[225]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib225.1.1" style="font-size:90%;">
C. Wang et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib225.2.1" style="font-size:90%;">DreamVideo: High-Fidelity Image-to-Video Generation with Image Retention and Text Guidance.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib226">
<span class="ltx_tag ltx_tag_bibitem">[226]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib226.1.1" style="font-size:90%;">
A. Gupta et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib226.2.1" style="font-size:90%;">Photorealistic Video Generation with Diffusion Models.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib227">
<span class="ltx_tag ltx_tag_bibitem">[227]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib227.1.1" style="font-size:90%;">
Y. Cao et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib227.2.1" style="font-size:90%;">Dynamic Manga: Animating Still Manga via Camera Movement.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib227.3.1" style="font-size:90%;">19(1):160–172.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib228">
<span class="ltx_tag ltx_tag_bibitem">[228]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib228.1.1" style="font-size:90%;">
V. Gupta et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib228.2.1" style="font-size:90%;">C2VNet: A Deep Learning Framework Towards Comic Strip to Audio-Visual Scene Synthesis.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib228.3.1" style="font-size:90%;">In J. Lladós et al., editors, </span><span class="ltx_text ltx_font_italic" id="bib.bib228.4.2" style="font-size:90%;">Document Analysis and Recognition – ICDAR 2021</span><span class="ltx_text" id="bib.bib228.5.3" style="font-size:90%;">, Lecture Notes in Computer Science, pp. 160–175, Cham, 2021. Springer International Publishing.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib229">
<span class="ltx_tag ltx_tag_bibitem">[229]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib229.1.1" style="font-size:90%;">
Y. Li et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib229.2.1" style="font-size:90%;">StoryGAN: A Sequential Conditional GAN for Story Visualization.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib230">
<span class="ltx_tag ltx_tag_bibitem">[230]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib230.1.1" style="font-size:90%;">
A. Maharana et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib230.2.1" style="font-size:90%;">Improving Generation and Evaluation of Visual Stories via Semantic Consistency.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib231">
<span class="ltx_tag ltx_tag_bibitem">[231]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib231.1.1" style="font-size:90%;">
H. Chen et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib231.2.1" style="font-size:90%;">Character-Centric Story Visualization via Visual Planning and Token Alignment.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib232">
<span class="ltx_tag ltx_tag_bibitem">[232]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib232.1.1" style="font-size:90%;">
X. Pan et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib232.2.1" style="font-size:90%;">Synthesizing Coherent Story with Auto-Regressive Latent Diffusion Models.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib233">
<span class="ltx_tag ltx_tag_bibitem">[233]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib233.1.1" style="font-size:90%;">
T. Rahman et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib233.2.1" style="font-size:90%;">Make-A-Story: Visual Memory Conditioned Consistent Story Generation.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib233.3.1" style="font-size:90%;">pp. 2493–2502.
</span>
</span>
</li>
<li class="ltx_bibitem" id="bib.bib234">
<span class="ltx_tag ltx_tag_bibitem">[234]</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib234.1.1" style="font-size:90%;">
W. Peng et al.
</span>
</span>
<span class="ltx_bibblock"><span class="ltx_text" id="bib.bib234.2.1" style="font-size:90%;">Personalized Comic Story Generation.
</span>
</span>
</li>
</ul>
</section>
<div class="ltx_pagination ltx_role_newpage"></div>
<section class="ltx_appendix" id="Ax1">
<h2 class="ltx_title ltx_title_appendix">A. Methodology details</h2>
<div class="ltx_para" id="Ax1.p1">
<p class="ltx_p" id="Ax1.p1.1">In this section, we will explore the process of finding the state-of-the-art in Comics and Manga. We utilized the software “Publish or Perish”<span class="ltx_note ltx_role_footnote" id="footnote13"><sup class="ltx_note_mark">13</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">13</sup><span class="ltx_tag ltx_tag_note">13</span>Available at: <a class="ltx_ref ltx_href" href="https://harzing.com/resources/publish-or-perish" title="">https://harzing.com/resources/publish-or-perish</a></span></span></span>, which comprehends the following Search sources: “Semantic Scholar”, “Google Scholar”, “Web of Science” and “Scopus”. As every source possesses its own interface and query structure, we designed a “SQL-like” query:</p>
<ul class="ltx_itemize" id="Ax1.I1">
<li class="ltx_item" id="Ax1.I1.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax1.I1.ix1.p1">
<p class="ltx_p" id="Ax1.I1.ix1.p1.1">Title: <span class="ltx_text ltx_font_italic" id="Ax1.I1.ix1.p1.1.1">“comic*”, “manga”</span></p>
</div>
</li>
<li class="ltx_item" id="Ax1.I1.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item"></span>
<div class="ltx_para" id="Ax1.I1.ix2.p1">
<p class="ltx_p" id="Ax1.I1.ix2.p1.1">Keywords: <span class="ltx_text ltx_font_italic" id="Ax1.I1.ix2.p1.1.1">(“comic*” OR “manga”) AND (“machine*” OR “learning*” OR “ml” OR “computer*” OR “science*” OR “artificial*”)</span></p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ax1.p2">
<p class="ltx_p" id="Ax1.p2.1">For every source, we limited the results to the last 1000 from 2010 to 2024. For every paper (out of the 3500 obtained), we used the “Semantic Scholar API”<span class="ltx_note ltx_role_footnote" id="footnote14"><sup class="ltx_note_mark">14</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">14</sup><span class="ltx_tag ltx_tag_note">14</span>API at link: <a class="ltx_ref ltx_href" href="https://www.semanticscholar.org/product/api" title="">https://www.semanticscholar.org/product/api</a></span></span></span> to query the title and obtain the unique DOI. We finally filtered unique papers, automatically removing duplicated copies (DOI) copies, obtaining 2276 papers. This duplicate-removal process has been further implemented also in “zotero”<span class="ltx_note ltx_role_footnote" id="footnote15"><sup class="ltx_note_mark">15</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">15</sup><span class="ltx_tag ltx_tag_note">15</span>Zotero software available at: <a class="ltx_ref ltx_href" href="https://forums.zotero.org" title="">https://forums.zotero.org</a></span></span></span>, fetching data from the websites and using javascript code<span class="ltx_note ltx_role_footnote" id="footnote16"><sup class="ltx_note_mark">16</sup><span class="ltx_note_outer"><span class="ltx_note_content"><sup class="ltx_note_mark">16</sup><span class="ltx_tag ltx_tag_note">16</span>Zotero <a class="ltx_ref ltx_href" href="https://forums.zotero.org/discussion/40457/merge-all-duplicates" title="">deduplication script</a></span></span></span>. Finally, we define our inclusion/exclusion criteria as follows:</p>
</div>
<div class="ltx_para" id="Ax1.p3">
<p class="ltx_p" id="Ax1.p3.1"><span class="ltx_text ltx_font_bold" id="Ax1.p3.1.1">Inclusion.</span> The study regards:</p>
<ul class="ltx_itemize" id="Ax1.I2">
<li class="ltx_item" id="Ax1.I2.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(I1)</span>
<div class="ltx_para" id="Ax1.I2.ix1.p1">
<p class="ltx_p" id="Ax1.I2.ix1.p1.1">Comics and Manga;</p>
</div>
</li>
<li class="ltx_item" id="Ax1.I2.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(I2)</span>
<div class="ltx_para" id="Ax1.I2.ix2.p1">
<p class="ltx_p" id="Ax1.I2.ix2.p1.1">Computer science;</p>
</div>
</li>
<li class="ltx_item" id="Ax1.I2.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(I3)</span>
<div class="ltx_para" id="Ax1.I2.ix3.p1">
<p class="ltx_p" id="Ax1.I2.ix3.p1.1">Deep Learning;</p>
</div>
</li>
<li class="ltx_item" id="Ax1.I2.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(I4)</span>
<div class="ltx_para" id="Ax1.I2.ix4.p1">
<p class="ltx_p" id="Ax1.I2.ix4.p1.1">new Dataset and/or Task.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ax1.p4">
<p class="ltx_p" id="Ax1.p4.1"><span class="ltx_text ltx_font_bold" id="Ax1.p4.1.1">Exclusion.</span> The study is:</p>
<ul class="ltx_itemize" id="Ax1.I3">
<li class="ltx_item" id="Ax1.I3.ix1" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(E1)</span>
<div class="ltx_para" id="Ax1.I3.ix1.p1">
<p class="ltx_p" id="Ax1.I3.ix1.p1.1">not written in English;</p>
</div>
</li>
<li class="ltx_item" id="Ax1.I3.ix2" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(E2)</span>
<div class="ltx_para" id="Ax1.I3.ix2.p1">
<p class="ltx_p" id="Ax1.I3.ix2.p1.1">not available;</p>
</div>
</li>
<li class="ltx_item" id="Ax1.I3.ix3" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(E3)</span>
<div class="ltx_para" id="Ax1.I3.ix3.p1">
<p class="ltx_p" id="Ax1.I3.ix3.p1.1">a duplicate or extension of an already included study;</p>
</div>
</li>
<li class="ltx_item" id="Ax1.I3.ix4" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(E4)</span>
<div class="ltx_para" id="Ax1.I3.ix4.p1">
<p class="ltx_p" id="Ax1.I3.ix4.p1.1">a secondary or tertiary study;</p>
</div>
</li>
<li class="ltx_item" id="Ax1.I3.ix5" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(E5)</span>
<div class="ltx_para" id="Ax1.I3.ix5.p1">
<p class="ltx_p" id="Ax1.I3.ix5.p1.1">in the form of editorials, tutorials, books, extended abstracts, etc.;</p>
</div>
</li>
<li class="ltx_item" id="Ax1.I3.ix6" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(E6)</span>
<div class="ltx_para" id="Ax1.I3.ix6.p1">
<p class="ltx_p" id="Ax1.I3.ix6.p1.1">a non-scientific publication or grey literature;</p>
</div>
</li>
<li class="ltx_item" id="Ax1.I3.ix7" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(E7)</span>
<div class="ltx_para" id="Ax1.I3.ix7.p1">
<p class="ltx_p" id="Ax1.I3.ix7.p1.1">cited at least 2 times (if previous 2022);</p>
</div>
</li>
<li class="ltx_item" id="Ax1.I3.ix8" style="list-style-type:none;">
<span class="ltx_tag ltx_tag_item">(E7)</span>
<div class="ltx_para" id="Ax1.I3.ix8.p1">
<p class="ltx_p" id="Ax1.I3.ix8.p1.1">not published in AI conferences.</p>
</div>
</li>
</ul>
</div>
<div class="ltx_para" id="Ax1.p5">
<p class="ltx_p" id="Ax1.p5.1">After this process, we ended with 216 papers. The last phase of “snowballing” is applied at this point: whi le reading and analyzing the papers we discovered important references that were not in the collection because they were not selected by our query or were discarded by I/E criteria. However, this process is needed to include important papers that are highly cited from our collection. This step enlarged the collection to the final number of 353 papers. Finally, we manually check all the references and include the one that suits the best our taxonomy and scope.</p>
</div>
</section>
<section class="ltx_appendix" id="Ax2">
<h2 class="ltx_title ltx_title_appendix">B. Taxonomy</h2>
<div class="ltx_para" id="Ax2.p1">
<p class="ltx_p" id="Ax2.p1.1">Task classification within the <span class="ltx_text ltx_font_italic" id="Ax2.p1.1.1">LoCU</span> is informed by both the modalities and dimensions of task inputs and outputs. We define modalities as <span class="ltx_text ltx_font_bold" id="Ax2.p1.1.2">Images</span> (ranging from no image to multiple pages), <span class="ltx_text ltx_font_bold" id="Ax2.p1.1.3">Text</span> (from no text to complex multi-sentence structures), <span class="ltx_text ltx_font_bold" id="Ax2.p1.1.4">Audio</span> (from no audio to complex sound mixes), and <span class="ltx_text ltx_font_bold" id="Ax2.p1.1.5">Geometry</span> (encompassing points, lines, bounding boxes, segmentations, and 3D geometries). Dimensional analysis considers <span class="ltx_text ltx_font_bold" id="Ax2.p1.1.6">Space</span> (from non-spatial to tridimensional) and <span class="ltx_text ltx_font_bold" id="Ax2.p1.1.7">Time</span> (whether time is a factor). Each task is scored based on its input and output characteristics in these modalities and dimensions (see Tables <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#Ax2.T4" title="TABLE IV ‣ B. Taxonomy ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">IV</span></a> and <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#Ax2.T5" title="TABLE V ‣ B. Taxonomy ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">V</span></a>), with the cumulative score determining its placement within the <span class="ltx_text ltx_font_italic" id="Ax2.p1.1.8">LoCU</span> layers.</p>
</div>
<figure class="ltx_table" id="Ax2.T4">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ax2.T4.3.1.1" style="font-size:90%;">TABLE IV</span>: </span><span class="ltx_text" id="Ax2.T4.4.2" style="font-size:90%;">Modality Scoring for <span class="ltx_text ltx_font_italic" id="Ax2.T4.4.2.1">LoCU</span> Tasks</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ax2.T4.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ax2.T4.5.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="Ax2.T4.5.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T4.5.1.1.1.1">Modality</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Ax2.T4.5.1.1.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T4.5.1.1.2.1">Category</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Ax2.T4.5.1.1.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T4.5.1.1.3.1">Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ax2.T4.5.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Ax2.T4.5.2.1.1" rowspan="4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T4.5.2.1.1.1">Images</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T4.5.2.1.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">No image</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T4.5.2.1.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">0</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.3.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Single panel</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.3.2.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.4.3.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Single page (multiple panels)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.4.3.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.5.4">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.5.4.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Multiple pages</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.5.4.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.6.5">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Ax2.T4.5.6.5.1" rowspan="6" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T4.5.6.5.1.1">Text</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T4.5.6.5.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">No text</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T4.5.6.5.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">0</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.7.6">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.7.6.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Single word (or tag)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.7.6.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.8.7">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.8.7.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Single sentence</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.8.7.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.9.8">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.9.8.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Multi sentences</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.9.8.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.10.9">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.10.9.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Instruct multi-sentences</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.10.9.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">4</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.11.10">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.11.10.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Complex multi-sentence</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.11.10.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">5</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.12.11">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Ax2.T4.5.12.11.1" rowspan="4" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T4.5.12.11.1.1">Audio</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T4.5.12.11.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">No audio</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T4.5.12.11.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">0</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.13.12">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.13.12.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Single sound</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.13.12.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.14.13">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.14.13.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Multi-elements sound</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.14.13.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.15.14">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.15.14.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Complex mix of sounds</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.15.14.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">4</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.16.15">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="Ax2.T4.5.16.15.1" rowspan="7" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T4.5.16.15.1.1">Geometry</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T4.5.16.15.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">No</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T4.5.16.15.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">0</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.17.16">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.17.16.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Point</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.17.16.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.18.17">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.18.17.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Line</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.18.17.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.19.18">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.19.18.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Bounding box</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.19.18.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.20.19">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.20.19.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Instance segmentation</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.20.19.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">4</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.21.20">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.21.20.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Semantic segmentation (multiple masks)</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T4.5.21.20.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">5</td>
</tr>
<tr class="ltx_tr" id="Ax2.T4.5.22.21">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="Ax2.T4.5.22.21.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3D object geometry</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="Ax2.T4.5.22.21.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">6</td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_table" id="Ax2.T5">
<figcaption class="ltx_caption ltx_centering"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ax2.T5.3.1.1" style="font-size:90%;">TABLE V</span>: </span><span class="ltx_text" id="Ax2.T5.4.2" style="font-size:90%;">Dimensions Scoring for <span class="ltx_text ltx_font_italic" id="Ax2.T5.4.2.1">LoCU</span> Tasks</span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ax2.T5.5">
<thead class="ltx_thead">
<tr class="ltx_tr" id="Ax2.T5.5.1.1">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_l ltx_border_r ltx_border_t" id="Ax2.T5.5.1.1.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T5.5.1.1.1.1">Dimensions</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Ax2.T5.5.1.1.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T5.5.1.1.2.1">Category</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_r ltx_border_t" id="Ax2.T5.5.1.1.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T5.5.1.1.3.1">Score</span></th>
</tr>
</thead>
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ax2.T5.5.2.1">
<td class="ltx_td ltx_align_center ltx_border_l ltx_border_r ltx_border_t" id="Ax2.T5.5.2.1.1" rowspan="3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T5.5.2.1.1.1">Space</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T5.5.2.1.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">No</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T5.5.2.1.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">0</td>
</tr>
<tr class="ltx_tr" id="Ax2.T5.5.3.2">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T5.5.3.2.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2D</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T5.5.3.2.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1</td>
</tr>
<tr class="ltx_tr" id="Ax2.T5.5.4.3">
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T5.5.4.3.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">3D</td>
<td class="ltx_td ltx_align_center ltx_border_r" id="Ax2.T5.5.4.3.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">2</td>
</tr>
<tr class="ltx_tr" id="Ax2.T5.5.5.4">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_l ltx_border_r ltx_border_t" id="Ax2.T5.5.5.4.1" rowspan="2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T5.5.5.4.1.1">Time</span></td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T5.5.5.4.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Not considered</td>
<td class="ltx_td ltx_align_center ltx_border_r ltx_border_t" id="Ax2.T5.5.5.4.3" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">0</td>
</tr>
<tr class="ltx_tr" id="Ax2.T5.5.6.5">
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="Ax2.T5.5.6.5.1" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">Considered</td>
<td class="ltx_td ltx_align_center ltx_border_b ltx_border_r" id="Ax2.T5.5.6.5.2" style="padding-top:-0.25pt;padding-bottom:-0.25pt;">1</td>
</tr>
</tbody>
</table>
</figure>
<div class="ltx_para" id="Ax2.p2">
<p class="ltx_p" id="Ax2.p2.1">In Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#Ax2.T6" title="TABLE VI ‣ B. Taxonomy ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">VI</span></a> are reported the values of modality and dimensions, together with the Layer of Comics Understanding Table <a class="ltx_ref" href="https://arxiv.org/html/2409.09502v1#S2.T1" title="TABLE I ‣ 2.1.3 Modern Foundational models ‣ 2.1 The Comics Research Epochs ‣ 2 Background ‣ One missing piece in Vision and Language: A Survey on Comics Understanding"><span class="ltx_text ltx_ref_tag">I</span></a>. This shows the modalities and dimensions value assigned to every task’s input and output, in order to obtain an objective complexity score that could sort each of the tasks. The acronyms “I, T, A, G” represent “Images, Text, Audio and Geometry”, respectively. Similarly, the acronyms “S, T” correspond to the Dimensions “Space, and Time”, respectively.</p>
</div>
<figure class="ltx_table" id="Ax2.T6">
<figcaption class="ltx_caption ltx_centering" style="font-size:50%;"><span class="ltx_tag ltx_tag_table"><span class="ltx_text" id="Ax2.T6.4.1.1" style="font-size:180%;">TABLE VI</span>: </span><span class="ltx_text" id="Ax2.T6.5.2" style="font-size:180%;">Detailed view of Modalities and Dimensions for both inputs and outputs. </span></figcaption>
<table class="ltx_tabular ltx_centering ltx_guessed_headers ltx_align_middle" id="Ax2.T6.6">
<tbody class="ltx_tbody">
<tr class="ltx_tr" id="Ax2.T6.6.1.1">
<td class="ltx_td" id="Ax2.T6.6.1.1.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<th class="ltx_td ltx_th ltx_th_column" id="Ax2.T6.6.1.1.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></th>
<th class="ltx_td ltx_th ltx_th_column" id="Ax2.T6.6.1.1.3" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></th>
<th class="ltx_td ltx_th ltx_th_column" id="Ax2.T6.6.1.1.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4" id="Ax2.T6.6.1.1.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.1.1.5.1" style="font-size:50%;">Mods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2" id="Ax2.T6.6.1.1.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.1.1.6.1" style="font-size:50%;">Dim</span></th>
<th class="ltx_td ltx_th ltx_th_column" id="Ax2.T6.6.1.1.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="4" id="Ax2.T6.6.1.1.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.1.1.8.1" style="font-size:50%;">Mods</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column" colspan="2" id="Ax2.T6.6.1.1.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.1.1.9.1" style="font-size:50%;">Dim</span></th>
<td class="ltx_td" id="Ax2.T6.6.1.1.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.2.2" style="background-color:#D9D9D9;">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.1.1" style="font-size:50%;background-color:#D9D9D9;">Layer</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.2.1" style="font-size:50%;background-color:#D9D9D9;">Category</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.3" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.3.1" style="font-size:50%;background-color:#D9D9D9;">Tasks</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.4.1" style="font-size:50%;background-color:#D9D9D9;">Input</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.5.1" style="font-size:50%;background-color:#D9D9D9;">I</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.6.1" style="font-size:50%;background-color:#D9D9D9;">T</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.7.1" style="font-size:50%;background-color:#D9D9D9;">A</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.8.1" style="font-size:50%;background-color:#D9D9D9;">G</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.9.1" style="font-size:50%;background-color:#D9D9D9;">S</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.10.1" style="font-size:50%;background-color:#D9D9D9;">T</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.11.1" style="font-size:50%;background-color:#D9D9D9;">Output</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.12.1" style="font-size:50%;background-color:#D9D9D9;">I</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.13.1" style="font-size:50%;background-color:#D9D9D9;">T</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.14.1" style="font-size:50%;background-color:#D9D9D9;">A</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.15.1" style="font-size:50%;background-color:#D9D9D9;">G</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.16.1" style="font-size:50%;background-color:#D9D9D9;">S</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.17.1" style="font-size:50%;background-color:#D9D9D9;">T</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_tt" id="Ax2.T6.6.2.2.18" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.2.2.18.1" style="font-size:50%;background-color:#D9D9D9;">Score</span></th>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.3.3">
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.1" style="background-color:#6AA84F;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.1.1" style="font-size:50%;color:#FFFFFF;background-color:#6AA84F;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.2" style="background-color:#6AA84F;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.2.1" style="font-size:50%;color:#FFFFFF;background-color:#6AA84F;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.3.3.2.1.1">View</span></span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.3" style="background-color:#B6D7A8;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.3.1" style="font-size:50%;background-color:#B6D7A8;">Basic Image Viewing (BIV)</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.4.1" style="font-size:50%;">Text Command</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.5.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.6.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.7.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.8.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.9.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.10.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.11.1" style="font-size:50%;">Image Display</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.12.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.13.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.14.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.15.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.16.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.17.1" style="font-size:50%;">0</span></th>
<th class="ltx_td ltx_align_center ltx_th ltx_th_column ltx_border_t" id="Ax2.T6.6.3.3.18" style="background-color:#57BB8A;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.3.3.18.1" style="font-size:50%;color:#FFFFFF;background-color:#57BB8A;">0</span></th>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.4.4">
<td class="ltx_td ltx_border_t" id="Ax2.T6.6.4.4.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_border_t" id="Ax2.T6.6.4.4.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.3" style="background-color:#D9EAD3;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.3.1" style="font-size:50%;background-color:#D9EAD3;">Image Classification (I-CLS)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.4.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.9.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.11.1" style="font-size:50%;">Tag</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.13.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.16.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.4.4.18" style="background-color:#8CC37F;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.4.4.18.1" style="font-size:50%;color:#FFFFFF;background-color:#8CC37F;"> 2</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.5.5">
<td class="ltx_td" id="Ax2.T6.6.5.5.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.5.5.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.3" style="background-color:#D9EAD3;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.3.1" style="font-size:50%;background-color:#D9EAD3;">Emotion Classification</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.4.1" style="font-size:50%;">Comic Panels/Images</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.9.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.11.1" style="font-size:50%;">Emotion Labels</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.13.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.16.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.5.5.18" style="background-color:#8CC37F;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.5.5.18.1" style="font-size:50%;color:#FFFFFF;background-color:#8CC37F;"> 2</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.6.6">
<td class="ltx_td" id="Ax2.T6.6.6.6.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.6.6.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.3" style="background-color:#D9EAD3;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.3.1" style="font-size:50%;background-color:#D9EAD3;">Action Detection</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.4.1" style="font-size:50%;">Multiple Panels</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.5.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.10.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.11.1" style="font-size:50%;">Tag</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.13.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.16.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.6.6.18" style="background-color:#DCD06E;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.6.6.18.1" style="font-size:50%;color:#FFFFFF;background-color:#DCD06E;">5</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.7.7">
<td class="ltx_td" id="Ax2.T6.6.7.7.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.2" style="background-color:#B6D7A8;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.2.1" style="font-size:50%;color:#FFFFFF;background-color:#B6D7A8;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.7.7.2.1.1">Tagging</span></span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.3" style="background-color:#D9EAD3;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.3.1" style="font-size:50%;background-color:#D9EAD3;">Page Stream Segmentation (PSS)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.4.1" style="font-size:50%;">Images</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.5.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.11.1" style="font-size:50%;">Tags sequence</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.13.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.16.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.7.7.18" style="background-color:#C1CC74;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.7.7.18.1" style="font-size:50%;color:#FFFFFF;background-color:#C1CC74;">4</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.8.8">
<td class="ltx_td" id="Ax2.T6.6.8.8.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.8.8.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.3" style="background-color:#FFF2CC;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.3.1" style="font-size:50%;background-color:#FFF2CC;">Image Super-Resolution (ISR)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.4.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.11.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.12.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.8.8.18" style="background-color:#C1CC74;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.8.8.18.1" style="font-size:50%;color:#FFFFFF;background-color:#C1CC74;"> 4</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.9.9">
<td class="ltx_td" id="Ax2.T6.6.9.9.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.9.9.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.3" style="background-color:#FFF2CC;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.3.1" style="font-size:50%;background-color:#FFF2CC;">Style Transfer (ST)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.4.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.11.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.12.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.9.9.18" style="background-color:#C1CC74;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.9.9.18.1" style="font-size:50%;color:#FFFFFF;background-color:#C1CC74;"> 4</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.10.10">
<td class="ltx_td" id="Ax2.T6.6.10.10.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.10.10.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.3" style="background-color:#FFF2CC;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.3.1" style="font-size:50%;background-color:#FFF2CC;">Vectorization</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.4.1" style="font-size:50%;">Comic Panels/Images</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.11.1" style="font-size:50%;">Vector image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.12.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.15.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.10.10.18" style="background-color:#DCD06E;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.10.10.18.1" style="font-size:50%;color:#FFFFFF;background-color:#DCD06E;">5</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.11.11">
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.1" style="background-color:#DED06E;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.1.1" style="font-size:50%;color:#FFFFFF;background-color:#DED06E;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.2" style="background-color:#DDD06E;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.2.1" style="font-size:50%;color:#FFFFFF;background-color:#DDD06E;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.11.11.2.1.1">Augmentation</span></span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.3" style="background-color:#FFF2CC;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.3.1" style="font-size:50%;background-color:#FFF2CC;">Depth Estimation</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.4.1" style="font-size:50%;">Comic Panels/Images</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.11.1" style="font-size:50%;">Depth Map</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.12.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.11.11.18" style="background-color:#C1CC74;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.11.11.18.1" style="font-size:50%;color:#FFFFFF;background-color:#C1CC74;"> 4</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.12.12">
<td class="ltx_td ltx_border_t" id="Ax2.T6.6.12.12.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_border_t" id="Ax2.T6.6.12.12.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.3" style="background-color:#FFE599;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.3.1" style="font-size:50%;background-color:#FFE599;">Object Detection</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.4.1" style="font-size:50%;">Tag/s + Image</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.6.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.11.1" style="font-size:50%;">Bounding Boxes</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.15.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.12.12.18" style="background-color:#F7D468;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.12.12.18.1" style="font-size:50%;color:#FFFFFF;background-color:#F7D468;"> 6</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.13.13">
<td class="ltx_td" id="Ax2.T6.6.13.13.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.13.13.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.3" style="background-color:#FFE599;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.3.1" style="font-size:50%;background-color:#FFE599;">Character Re-identification (Character ID)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.4.1" style="font-size:50%;">Multiple Panels</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.5.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.6.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.11.1" style="font-size:50%;">Text [Tagging]</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.13.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.15.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.16.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.13.13.18" style="background-color:#F8C85E;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.13.13.18.1" style="font-size:50%;color:#FFFFFF;background-color:#F8C85E;"> 7</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.14.14">
<td class="ltx_td" id="Ax2.T6.6.14.14.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.2" style="background-color:#FFD966;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.2.1" style="font-size:50%;color:#FFFFFF;background-color:#FFD966;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.14.14.2.1.1">Grounding</span></span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.3" style="background-color:#FFE599;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.3.1" style="font-size:50%;background-color:#FFE599;">Grounding (IG)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.4.1" style="font-size:50%;">[prompt] + Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.6.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.11.1" style="font-size:50%;">Bounding Boxes</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.15.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.14.14.18" style="background-color:#F8C85E;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.14.14.18.1" style="font-size:50%;color:#FFFFFF;background-color:#F8C85E;">7</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.15.15">
<td class="ltx_td" id="Ax2.T6.6.15.15.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.15.15.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.3" style="background-color:#F9CB9C;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.3.1" style="font-size:50%;background-color:#F9CB9C;">Character-Balloon Association (Speaker ID)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.4.1" style="font-size:50%;">Character + Balloons</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.5.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.8.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.11.1" style="font-size:50%;">Text [Tagging]</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.15.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.16.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.15.15.18" style="background-color:#F8C85E;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.15.15.18.1" style="font-size:50%;color:#FFFFFF;background-color:#F8C85E;">7</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.16.16">
<td class="ltx_td" id="Ax2.T6.6.16.16.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.16.16.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.3" style="background-color:#F9CB9C;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.3.1" style="font-size:50%;background-color:#F9CB9C;">Dialog transcription</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.4.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.10.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.11.1" style="font-size:50%;">Text</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.13.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.16.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.17.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.16.16.18" style="background-color:#F8C85E;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.16.16.18.1" style="font-size:50%;color:#FFFFFF;background-color:#F8C85E;"> 7</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.17.17">
<td class="ltx_td" id="Ax2.T6.6.17.17.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.2" style="background-color:#F6B26B;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.2.1" style="font-size:50%;color:#FFFFFF;background-color:#F6B26B;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.17.17.2.1.1">Analysis</span></span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.3" style="background-color:#F9CB9C;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.3.1" style="font-size:50%;background-color:#F9CB9C;">Translation</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.4.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.6.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.11.1" style="font-size:50%;">Text</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.13.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.16.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.17.17.18" style="background-color:#EEB253;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.17.17.18.1" style="font-size:50%;color:#FFFFFF;background-color:#EEB253;">8</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.18.18">
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.1" style="background-color:#E8A84D;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.1.1" style="font-size:50%;color:#FFFFFF;background-color:#E8A84D;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.2" style="background-color:#E69138;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.2.1" style="font-size:50%;color:#FFFFFF;background-color:#E69138;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.18.18.2.1.1">Segmentation</span></span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.3" style="background-color:#F6B26B;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.3.1" style="font-size:50%;background-color:#F6B26B;">Instance Segmentation (IS)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.4.1" style="font-size:50%;">[prompt] + Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.8.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.11.1" style="font-size:50%;">Segments</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.15.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.18.18.18" style="background-color:#EEB253;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.18.18.18.1" style="font-size:50%;color:#FFFFFF;background-color:#EEB253;">8</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.19.19">
<td class="ltx_td ltx_border_t" id="Ax2.T6.6.19.19.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_border_t" id="Ax2.T6.6.19.19.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.3" style="background-color:#EAD1DC;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.3.1" style="font-size:50%;background-color:#EAD1DC;">Image-Text Retrieval (IR)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.4.1" style="font-size:50%;">Text</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.6.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.11.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.13.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.19.19.18" style="background-color:#EEB253;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.19.19.18.1" style="font-size:50%;color:#FFFFFF;background-color:#EEB253;"> 8</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.20.20">
<td class="ltx_td" id="Ax2.T6.6.20.20.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.20.20.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.3" style="background-color:#EAD1DC;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.3.1" style="font-size:50%;background-color:#EAD1DC;">Text-Image Retrieval (TR)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.4.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.5.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.6.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.11.1" style="font-size:50%;">Text</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.12.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.20.20.18" style="background-color:#EEB253;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.20.20.18.1" style="font-size:50%;color:#FFFFFF;background-color:#EEB253;"> 8</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.21.21">
<td class="ltx_td" id="Ax2.T6.6.21.21.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.2" style="background-color:#E06666;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.2.1" style="font-size:50%;color:#FFFFFF;background-color:#E06666;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.21.21.2.1.1" style="color:#FFFFFF;background-color:#E06666;">Retrieval</span></span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.3" style="background-color:#EAD1DC;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.3.1" style="font-size:50%;background-color:#EAD1DC;">Composed Image Retrieval (CIR)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.4.1" style="font-size:50%;">Text + Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.5.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.6.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.11.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.12.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.16.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.21.21.18" style="background-color:#EEB253;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.21.21.18.1" style="font-size:50%;color:#FFFFFF;background-color:#EEB253;">8</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.22.22">
<td class="ltx_td" id="Ax2.T6.6.22.22.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.22.22.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.3" style="background-color:#EA9999;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.3.1" style="font-size:50%;background-color:#EA9999;">Image Inpainting (II)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.4.1" style="font-size:50%;">Text + [prompt] + Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.6.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.8.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.11.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.12.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.22.22.18" style="background-color:#E49D47;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.22.22.18.1" style="font-size:50%;color:#FFFFFF;background-color:#E49D47;">9</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.23.23">
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.1" style="background-color:#E06666;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.1.1" style="font-size:50%;color:#FFFFFF;background-color:#E06666;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.2" style="background-color:#CC4125;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.2.1" style="font-size:50%;color:#FFFFFF;background-color:#CC4125;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.23.23.2.1.1" style="color:#FFFFFF;background-color:#CC4125;">Modification</span></span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.3" style="background-color:#EA9999;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.3.1" style="font-size:50%;background-color:#EA9999;">Image Editing via Text (IET)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.4.1" style="font-size:50%;">Text + Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.6.1" style="font-size:50%;">4</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.11.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.12.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.23.23.18" style="background-color:#E49D47;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.23.23.18.1" style="font-size:50%;color:#FFFFFF;background-color:#E49D47;"> 9</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.24.24">
<td class="ltx_td ltx_border_t" id="Ax2.T6.6.24.24.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_border_t" id="Ax2.T6.6.24.24.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.3" style="background-color:#E06666;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.3.1" style="font-size:50%;background-color:#E06666;">Visual Dialog (VisDial)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.4.1" style="font-size:50%;">Image + Dialog + Text</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.5.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.6.1" style="font-size:50%;">4</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.10.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.11.1" style="font-size:50%;">Text</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.13.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.17.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.24.24.18" style="background-color:#BB4718;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.24.24.18.1" style="font-size:50%;color:#FFFFFF;background-color:#BB4718;"> 13</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.25.25">
<td class="ltx_td" id="Ax2.T6.6.25.25.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.25.25.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.3" style="background-color:#E06666;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.3.1" style="font-size:50%;background-color:#E06666;">Visual Reasoning (VR)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.4.1" style="font-size:50%;">Image + Text</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.5.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.6.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.10.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.11.1" style="font-size:50%;">Text</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.13.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.17.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.25.25.18" style="background-color:#C55D24;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.25.25.18.1" style="font-size:50%;color:#FFFFFF;background-color:#C55D24;"> 12</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.26.26">
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.1" style="background-color:#CC4125;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.1.1" style="font-size:50%;color:#FFFFFF;background-color:#CC4125;">4</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.2" style="background-color:#990000;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.2.1" style="font-size:50%;color:#FFFFFF;background-color:#990000;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.26.26.2.1.1">Understanding</span></span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.3" style="background-color:#E06666;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.3.1" style="font-size:50%;background-color:#E06666;">Visual Question Answering (VQA)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.4.1" style="font-size:50%;">Image + Text</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.5.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.6.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.10.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.11.1" style="font-size:50%;">Text</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.13.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.26.26.18" style="background-color:#DA873B;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.26.26.18.1" style="font-size:50%;color:#FFFFFF;background-color:#DA873B;">10</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.27.27">
<td class="ltx_td ltx_border_t" id="Ax2.T6.6.27.27.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_border_t" id="Ax2.T6.6.27.27.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.3" style="background-color:#DD7E6B;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.3.1" style="font-size:50%;background-color:#DD7E6B;">Image Captioning [img-2-text] (IC)</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.4.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.9.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.11.1" style="font-size:50%;">Text</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.13.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.16.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_t" id="Ax2.T6.6.27.27.18" style="background-color:#EEB253;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.27.27.18.1" style="font-size:50%;color:#FFFFFF;background-color:#EEB253;"> 8</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.28.28">
<td class="ltx_td" id="Ax2.T6.6.28.28.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.28.28.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.3" style="background-color:#DD7E6B;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.3.1" style="font-size:50%;background-color:#DD7E6B;">Image Generation [text-2-img] (IG)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.4.1" style="font-size:50%;">Text</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.5.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.6.1" style="font-size:50%;">5</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.11.1" style="font-size:50%;">Image</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.12.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.16.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.28.28.18" style="background-color:#E49D47;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.28.28.18.1" style="font-size:50%;color:#FFFFFF;background-color:#E49D47;"> 9</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.29.29">
<td class="ltx_td" id="Ax2.T6.6.29.29.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.29.29.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.3" style="background-color:#DD7E6B;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.3.1" style="font-size:50%;background-color:#DD7E6B;">Scene Graph Generation for Captioning</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.4.1" style="font-size:50%;">Comic Panel</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.9.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.11.1" style="font-size:50%;">Scene Graph</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.13.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.15.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.16.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.29.29.18" style="background-color:#CF722F;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.29.29.18.1" style="font-size:50%;color:#FFFFFF;background-color:#CF722F;"> 11</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.30.30">
<td class="ltx_td" id="Ax2.T6.6.30.30.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.2" style="background-color:#A61C00;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.2.1" style="font-size:50%;color:#FFFFFF;background-color:#A61C00;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.30.30.2.1.1" style="color:#FFFFFF;background-color:#A61C00;">Generation</span></span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.3" style="background-color:#DD7E6B;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.3.1" style="font-size:50%;background-color:#DD7E6B;">Sound Generation from Single Panel</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.4.1" style="font-size:50%;">Single Comic Panel</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.5.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.9.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.11.1" style="font-size:50%;">Sound/Audio</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.14.1" style="font-size:50%;">4</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.16.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.17.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.30.30.18" style="background-color:#CF722F;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.30.30.18.1" style="font-size:50%;color:#FFFFFF;background-color:#CF722F;">11</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.31.31">
<td class="ltx_td" id="Ax2.T6.6.31.31.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.31.31.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.3" style="background-color:#C27BA0;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.3.1" style="font-size:50%;background-color:#C27BA0;">3D Model Generation from Images (3DGI)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.4.1" style="font-size:50%;">Collection of Images</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.5.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.6.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.10.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.11.1" style="font-size:50%;">3D Model</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.12.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.13.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.15.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.16.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.17.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.31.31.18" style="background-color:#DA873B;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.31.31.18.1" style="font-size:50%;color:#FFFFFF;background-color:#DA873B;"> 10</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.32.32">
<td class="ltx_td" id="Ax2.T6.6.32.32.1" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td" id="Ax2.T6.6.32.32.2" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.3" style="background-color:#C27BA0;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.3.1" style="font-size:50%;background-color:#C27BA0;">Video Generation from Text (VGT)</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.4.1" style="font-size:50%;">Complex Long Text</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.5.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.6.1" style="font-size:50%;">5</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.10.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.11.1" style="font-size:50%;">Video</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.12.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.13.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.17.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center" id="Ax2.T6.6.32.32.18" style="background-color:#C55D24;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.32.32.18.1" style="font-size:50%;color:#FFFFFF;background-color:#C55D24;"> 12</span></td>
</tr>
<tr class="ltx_tr" id="Ax2.T6.6.33.33">
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.1" style="background-color:#A61C00;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.1.1" style="font-size:50%;color:#FFFFFF;background-color:#A61C00;">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.2" style="background-color:#741B47;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.2.1" style="font-size:50%;color:#FFFFFF;background-color:#741B47;"><span class="ltx_text ltx_font_bold" id="Ax2.T6.6.33.33.2.1.1">Synthesis</span></span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.3" style="background-color:#C27BA0;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.3.1" style="font-size:50%;background-color:#C27BA0;">Narrative-Based Complex Scene Generation (NCSG)</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.4" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.4.1" style="font-size:50%;">Detailed Narrative Text</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.5" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.5.1" style="font-size:50%;">2</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.6" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.6.1" style="font-size:50%;">5</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.7" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.7.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.8" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.8.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.9" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.9.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.10" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.10.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.11" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.11.1" style="font-size:50%;">Series of Images</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.12" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.12.1" style="font-size:50%;">3</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.13" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.13.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.14" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.14.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.15" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.15.1" style="font-size:50%;">0</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.16" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.16.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.17" style="padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.17.1" style="font-size:50%;">1</span></td>
<td class="ltx_td ltx_align_center ltx_border_bb" id="Ax2.T6.6.33.33.18" style="background-color:#A61C00;padding-top:-0.15pt;padding-bottom:-0.15pt;"><span class="ltx_text" id="Ax2.T6.6.33.33.18.1" style="font-size:50%;color:#FFFFFF;background-color:#A61C00;">15</span></td>
</tr>
</tbody>
</table>
</figure>
<figure class="ltx_float biography" id="Ax2.1">
<table class="ltx_tabular" id="Ax2.1.1">
<tr class="ltx_tr" id="Ax2.1.1.1">
<td class="ltx_td" id="Ax2.1.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_portrait" height="125" id="Ax2.1.1.1.1.g1" src="extracted/5855233/bios/vivoli.jpg" width="92"/></td>
<td class="ltx_td" id="Ax2.1.1.1.2">
<span class="ltx_inline-block" id="Ax2.1.1.1.2.1">
<span class="ltx_p" id="Ax2.1.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="Ax2.1.1.1.2.1.1.1">Emanuele Vivoli</span>  is pursuing its joint-PhD at the Computer Vision Center (Autonomous University of Barcelona) and at the MICC Lab (University of Florence). His research interests encompass multi-modal learning (vision and language integration), with a focus on comics. His work covers a diverse range of topics, including document and table analysis, understanding comics and manga, and creating datasets and frameworks. If you are interested, read his papers.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="Ax2.2">
<table class="ltx_tabular" id="Ax2.2.1">
<tr class="ltx_tr" id="Ax2.2.1.1">
<td class="ltx_td" id="Ax2.2.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="100" id="Ax2.2.1.1.1.g1" src="extracted/5855233/bios/andrey.png" width="100"/></td>
<td class="ltx_td" id="Ax2.2.1.1.2">
<span class="ltx_inline-block" id="Ax2.2.1.1.2.1">
<span class="ltx_p" id="Ax2.2.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="Ax2.2.1.1.2.1.1.1">Andrey Barsky</span>  is a postdoctoral researcher at the Computer Vision Center, Barcelona, Spain. He received his Ph.D. in 2015 from the University of Nottingham in the UK. His research focuses on computer vision and multimodal learning, as well as robustness and explainability in AI models.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="Ax2.3">
<table class="ltx_tabular" id="Ax2.3.1">
<tr class="ltx_tr" id="Ax2.3.1.1">
<td class="ltx_td" id="Ax2.3.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="100" id="Ax2.3.1.1.1.g1" src="extracted/5855233/bios/moha.jpg" width="100"/></td>
<td class="ltx_td" id="Ax2.3.1.1.2">
<span class="ltx_inline-block" id="Ax2.3.1.1.2.1">
<span class="ltx_p" id="Ax2.3.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="Ax2.3.1.1.2.1.1.1">Mohamed Ali Souibgui</span>  is a postdoctoral researcher at Computer Vision Center, Barcelona, Spain. He received the Ph.D. degree in 2022 from the Universitat Autònoma de Barcelona (UAB), Spain. His current research focuses on document image analysis using computer vision and machine learning tools, with the focus on privacy and explainability. He is an author of several journal and peer-reviewed conference papers.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="Ax2.4">
<table class="ltx_tabular" id="Ax2.4.1">
<tr class="ltx_tr" id="Ax2.4.1.1">
<td class="ltx_td" id="Ax2.4.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="100" id="Ax2.4.1.1.1.g1" src="extracted/5855233/bios/artemis.jpeg" width="100"/></td>
<td class="ltx_td" id="Ax2.4.1.1.2">
<span class="ltx_inline-block" id="Ax2.4.1.1.2.1">
<span class="ltx_p" id="Ax2.4.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="Ax2.4.1.1.2.1.1.1">Artemis Llabrés</span>  is a PhD student in the Computer Vision Center. She is part of the Vision, Language, and reading group where she is working on multimodal models for document understanding. In 2024 she was awarded best student paper by the 16th IAPR International Workshop on Document Analysis Systems, for her work titled “Image-text matching for large-scale book collections”.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="Ax2.5">
<table class="ltx_tabular" id="Ax2.5.1">
<tr class="ltx_tr" id="Ax2.5.1.1">
<td class="ltx_td" id="Ax2.5.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="110" id="Ax2.5.1.1.1.g1" src="extracted/5855233/bios/bertini.png" width="100"/></td>
<td class="ltx_td" id="Ax2.5.1.1.2">
<span class="ltx_inline-block" id="Ax2.5.1.1.2.1">
<span class="ltx_p" id="Ax2.5.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="Ax2.5.1.1.2.1.1.1">Marco Bertini</span>  is an Associate Professor in Computer Science at the University of Florence, Italy. He is the director of the Media Integration and Communication Center of the University of Florence. His interests are focused on multimedia and computer vision. On these subjects he has addressed semantic analysis, automatic content indexing, semantic retrieval and video quality improvement, applying these techniques to different domains among which cultural heritage. He is author of more than 30 journal papers and more than 140 peer-reviewed conference papers. He has been involved in 10 EU research projects as WP coordinator and researcher, among which IM3I, euTV, ORUSSI, UMETECH, AI4Media and ReInHerit. He has been general and program co-chair of several conferences on multimedia. He is co-founder of Small Pixels, an academic spin-off working on GenAI solutions to improve video quality and video compression.</span>
</span>
</td>
</tr>
</table>
</figure>
<figure class="ltx_float biography" id="Ax2.6">
<table class="ltx_tabular" id="Ax2.6.1">
<tr class="ltx_tr" id="Ax2.6.1.1">
<td class="ltx_td" id="Ax2.6.1.1.1"><img alt="[Uncaptioned image]" class="ltx_graphics ltx_img_square" height="100" id="Ax2.6.1.1.1.g1" src="extracted/5855233/bios/Dimos.jpg" width="100"/></td>
<td class="ltx_td" id="Ax2.6.1.1.2">
<span class="ltx_inline-block" id="Ax2.6.1.1.2.1">
<span class="ltx_p" id="Ax2.6.1.1.2.1.1"><span class="ltx_text ltx_font_bold" id="Ax2.6.1.1.2.1.1.1">Dimosthenis Karatzas</span>  is a professor at the Universitat Autónoma de Barcelona and associate director of the Computer Vision Centre, leading the Vision and Language research group. His research focuses on computer vision and machine learning, with an emphasis on robust reading systems and document image analysis. He received the 2013 IAPR/ICDAR Young Investigator Award. He has created two spin-off companies. He chaired the IAPR TC11 on Reading Systems from 2016 to 2021, he is an IEEE senior member, an ELLIS fellow and co-director of the ELLIS Unit Barcelona, and a member of the Artificial Intelligence Doctoral Academy (AIDA) Research and Industry Board. He launched the Robust Reading Competition portal, which serves more than 50,000 researchers worldwide.</span>
</span>
</td>
</tr>
</table>
</figure>
<div class="ltx_pagination ltx_role_newpage"></div>
</section>
</article>
</div>
<footer class="ltx_page_footer">
<div class="ltx_page_logo">Generated  on Sat Sep 14 18:13:36 2024 by <a class="ltx_LaTeXML_logo" href="http://dlmf.nist.gov/LaTeXML/"><span style="letter-spacing:-0.2em; margin-right:0.1em;">L<span class="ltx_font_smallcaps" style="position:relative; bottom:2.2pt;">a</span>T<span class="ltx_font_smallcaps" style="font-size:120%;position:relative; bottom:-0.2ex;">e</span></span><span style="font-size:90%; position:relative; bottom:-0.2ex;">XML</span><img alt="Mascot Sammy" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAsAAAAOCAYAAAD5YeaVAAAAAXNSR0IArs4c6QAAAAZiS0dEAP8A/wD/oL2nkwAAAAlwSFlzAAALEwAACxMBAJqcGAAAAAd0SU1FB9wKExQZLWTEaOUAAAAddEVYdENvbW1lbnQAQ3JlYXRlZCB3aXRoIFRoZSBHSU1Q72QlbgAAAdpJREFUKM9tkL+L2nAARz9fPZNCKFapUn8kyI0e4iRHSR1Kb8ng0lJw6FYHFwv2LwhOpcWxTjeUunYqOmqd6hEoRDhtDWdA8ApRYsSUCDHNt5ul13vz4w0vWCgUnnEc975arX6ORqN3VqtVZbfbTQC4uEHANM3jSqXymFI6yWazP2KxWAXAL9zCUa1Wy2tXVxheKA9YNoR8Pt+aTqe4FVVVvz05O6MBhqUIBGk8Hn8HAOVy+T+XLJfLS4ZhTiRJgqIoVBRFIoric47jPnmeB1mW/9rr9ZpSSn3Lsmir1fJZlqWlUonKsvwWwD8ymc/nXwVBeLjf7xEKhdBut9Hr9WgmkyGEkJwsy5eHG5vN5g0AKIoCAEgkEkin0wQAfN9/cXPdheu6P33fBwB4ngcAcByHJpPJl+fn54mD3Gg0NrquXxeLRQAAwzAYj8cwTZPwPH9/sVg8PXweDAauqqr2cDjEer1GJBLBZDJBs9mE4zjwfZ85lAGg2+06hmGgXq+j3+/DsixYlgVN03a9Xu8jgCNCyIegIAgx13Vfd7vdu+FweG8YRkjXdWy329+dTgeSJD3ieZ7RNO0VAXAPwDEAO5VKndi2fWrb9jWl9Esul6PZbDY9Go1OZ7PZ9z/lyuD3OozU2wAAAABJRU5ErkJggg=="/></a>
</div></footer>
</div>
</body>
</html>
